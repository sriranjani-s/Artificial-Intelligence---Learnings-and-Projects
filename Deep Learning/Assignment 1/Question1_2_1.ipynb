{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJFFAn71ZyAY"
   },
   "source": [
    "# R00182510 - Assignment1 PART A - Task 2 (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WSLCI4UupncQ",
    "outputId": "f6e4da66-2e99-4ec4-b8cf-f653d54b5572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "# To enable TF2\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y3JBeLDeppg5",
    "outputId": "ce88f390-811d-41db-b5c8-b863aafea6f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "random.seed(182510)\n",
    "\n",
    "# load and prepare the training and test data\n",
    "def load_Prepare_Data():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "    # load the training and test data    \n",
    "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "    # reshape the feature data\n",
    "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "    # noramlise feature data\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "\n",
    "    print( \"Shape of training features \", tr_x.shape)\n",
    "    print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "    # one hot encode the training labels and get the transpose\n",
    "    tr_y = np_utils.to_categorical(tr_y,10)\n",
    "    tr_y = tr_y.T\n",
    "    print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "    # one hot encode the test labels and get the transpose\n",
    "    te_y = np_utils.to_categorical(te_y,10)\n",
    "    te_y = te_y.T\n",
    "    print (\"Shape of testing labels \", te_y.shape)\n",
    "    \n",
    "    # Reshape the training data and test data so \n",
    "    # that the features becomes the rows of the matrix\n",
    "    tr_x = tr_x.T\n",
    "    te_x = te_x.T\n",
    "\n",
    "    print(\"Reshaped training data \", tr_x.shape)\n",
    "    print(\"Reshaped test data \",te_x.shape)\n",
    "    \n",
    "    return(tr_x, tr_y, te_x, te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xy06wmFTeK7F"
   },
   "outputs": [],
   "source": [
    "# push all training data through the hidden layer and the SoftMax layer\n",
    "def forward_pass(x, w_T1, b1, w_T2, b2):\n",
    "    \n",
    "    # Multiply each training example by the weights and add the bias for the 1st hidden layer\n",
    "    A1 = tf.matmul(w_T1, x) + b1\n",
    "    # ReLu activation\n",
    "    H1 = tf.nn.relu(A1)\n",
    "    \n",
    "    # Multiply each training example by the weights and add the bias for the SoftMax layer\n",
    "    A2 = tf.matmul(w_T2, H1) + b2\n",
    "    \n",
    "    #SoftMax activation\n",
    "    t = tf.math.exp(A2)\n",
    "    t_sum = tf.reduce_sum(t, 0)\n",
    "    t_sum = tf.reshape(t_sum,[1, -1])\n",
    "\n",
    "    y_pred_softmax = tf.divide(t, t_sum)\n",
    "\n",
    "    return y_pred_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxUFdp7WeQKZ"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_pred):\n",
    "\n",
    "   # Calculate the cross entropy error for all training data \n",
    "    cross_entropy_loss = -(tf.reduce_sum(tf.multiply(y, tf.math.log(y_pred)), 0))\n",
    "    \n",
    "    # Calculate cost which is the mean cross entropy error/ average loss across all training instances\n",
    "    cost = tf.reduce_mean(cross_entropy_loss)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHKpvgB3eSu9"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, y_pred_softmax):\n",
    "\n",
    "    # Calculate the predictions in the form of a boolean array, by considering only the class with the highest probability\n",
    "    # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
    "    predictions_bool = tf.equal(tf.argmax(y_pred_softmax, 0), tf.argmax(y, 0))\n",
    "    predictions_correct = tf.cast(predictions_bool, tf.float32)\n",
    "\n",
    "    # Finally, we just determine the mean value of the correct predictions\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "  \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aHd4zmAGeWOE",
    "outputId": "4c32aa7e-0c2c-4bcf-876b-43b1c2340fda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n",
      "Reshaped training data  (784, 60000)\n",
      "Reshaped test data  (784, 10000)\n",
      "Iteration  0 : Train Loss =  2.374438   Train Acc:  0.057733335   Validation Loss =  2.3740351   Validation Acc:  0.0583\n",
      "Iteration  1 : Train Loss =  2.0594566   Train Acc:  0.35171667   Validation Loss =  2.0603015   Validation Acc:  0.3485\n",
      "Iteration  2 : Train Loss =  1.824994   Train Acc:  0.5128   Validation Loss =  1.8272556   Validation Acc:  0.511\n",
      "Iteration  3 : Train Loss =  1.6307257   Train Acc:  0.59795   Validation Loss =  1.6345384   Validation Acc:  0.5935\n",
      "Iteration  4 : Train Loss =  1.4661934   Train Acc:  0.6369   Validation Loss =  1.471697   Validation Acc:  0.6332\n",
      "Iteration  5 : Train Loss =  1.327599   Train Acc:  0.65056664   Validation Loss =  1.3344256   Validation Acc:  0.643\n",
      "Iteration  6 : Train Loss =  1.210626   Train Acc:  0.6591333   Validation Loss =  1.2185895   Validation Acc:  0.6514\n",
      "Iteration  7 : Train Loss =  1.1152546   Train Acc:  0.6643   Validation Loss =  1.1241846   Validation Acc:  0.6516\n",
      "Iteration  8 : Train Loss =  1.0410588   Train Acc:  0.67003334   Validation Loss =  1.0508062   Validation Acc:  0.6612\n",
      "Iteration  9 : Train Loss =  0.98131955   Train Acc:  0.67983335   Validation Loss =  0.9920669   Validation Acc:  0.6714\n",
      "Iteration  10 : Train Loss =  0.9303355   Train Acc:  0.69191664   Validation Loss =  0.94246775   Validation Acc:  0.6807\n",
      "Iteration  11 : Train Loss =  0.88743544   Train Acc:  0.69991666   Validation Loss =  0.9013668   Validation Acc:  0.6892\n",
      "Iteration  12 : Train Loss =  0.8518681   Train Acc:  0.7072333   Validation Loss =  0.86763614   Validation Acc:  0.6954\n",
      "Iteration  13 : Train Loss =  0.82221895   Train Acc:  0.71423334   Validation Loss =  0.8395503   Validation Acc:  0.7037\n",
      "Iteration  14 : Train Loss =  0.7968583   Train Acc:  0.7230667   Validation Loss =  0.8154471   Validation Acc:  0.713\n",
      "Iteration  15 : Train Loss =  0.77471143   Train Acc:  0.73216665   Validation Loss =  0.79426277   Validation Acc:  0.7208\n",
      "Iteration  16 : Train Loss =  0.7545639   Train Acc:  0.73966664   Validation Loss =  0.7747307   Validation Acc:  0.7269\n",
      "Iteration  17 : Train Loss =  0.7360777   Train Acc:  0.74581665   Validation Loss =  0.75646913   Validation Acc:  0.7334\n",
      "Iteration  18 : Train Loss =  0.7195178   Train Acc:  0.75158334   Validation Loss =  0.73999065   Validation Acc:  0.7383\n",
      "Iteration  19 : Train Loss =  0.70452476   Train Acc:  0.75701666   Validation Loss =  0.7251053   Validation Acc:  0.7446\n",
      "Iteration  20 : Train Loss =  0.6905183   Train Acc:  0.76198334   Validation Loss =  0.7111799   Validation Acc:  0.7484\n",
      "Iteration  21 : Train Loss =  0.6773081   Train Acc:  0.7661667   Validation Loss =  0.6980492   Validation Acc:  0.7537\n",
      "Iteration  22 : Train Loss =  0.66493696   Train Acc:  0.77178335   Validation Loss =  0.68596894   Validation Acc:  0.7589\n",
      "Iteration  23 : Train Loss =  0.65309924   Train Acc:  0.77816665   Validation Loss =  0.674705   Validation Acc:  0.7654\n",
      "Iteration  24 : Train Loss =  0.6418128   Train Acc:  0.78321666   Validation Loss =  0.66397595   Validation Acc:  0.7717\n",
      "Iteration  25 : Train Loss =  0.6313345   Train Acc:  0.78783333   Validation Loss =  0.6539888   Validation Acc:  0.7754\n",
      "Iteration  26 : Train Loss =  0.62158334   Train Acc:  0.7917167   Validation Loss =  0.64473385   Validation Acc:  0.78\n",
      "Iteration  27 : Train Loss =  0.61215806   Train Acc:  0.79506665   Validation Loss =  0.635641   Validation Acc:  0.7833\n",
      "Iteration  28 : Train Loss =  0.6032349   Train Acc:  0.7981333   Validation Loss =  0.62690926   Validation Acc:  0.7862\n",
      "Iteration  29 : Train Loss =  0.59486556   Train Acc:  0.80075   Validation Loss =  0.61879736   Validation Acc:  0.7885\n",
      "Iteration  30 : Train Loss =  0.58701044   Train Acc:  0.8035167   Validation Loss =  0.611231   Validation Acc:  0.7927\n",
      "Iteration  31 : Train Loss =  0.57959855   Train Acc:  0.80656666   Validation Loss =  0.6040917   Validation Acc:  0.7952\n",
      "Iteration  32 : Train Loss =  0.57250834   Train Acc:  0.8085833   Validation Loss =  0.5972671   Validation Acc:  0.7981\n",
      "Iteration  33 : Train Loss =  0.5657621   Train Acc:  0.8105   Validation Loss =  0.5907235   Validation Acc:  0.8008\n",
      "Iteration  34 : Train Loss =  0.5593387   Train Acc:  0.81296664   Validation Loss =  0.58452666   Validation Acc:  0.802\n",
      "Iteration  35 : Train Loss =  0.5532617   Train Acc:  0.81493336   Validation Loss =  0.5788103   Validation Acc:  0.8031\n",
      "Iteration  36 : Train Loss =  0.547471   Train Acc:  0.8166   Validation Loss =  0.5733924   Validation Acc:  0.8047\n",
      "Iteration  37 : Train Loss =  0.54195863   Train Acc:  0.8184   Validation Loss =  0.5682756   Validation Acc:  0.805\n",
      "Iteration  38 : Train Loss =  0.53673106   Train Acc:  0.81995   Validation Loss =  0.5634137   Validation Acc:  0.8071\n",
      "Iteration  39 : Train Loss =  0.5317319   Train Acc:  0.82175   Validation Loss =  0.5585186   Validation Acc:  0.8087\n",
      "Iteration  40 : Train Loss =  0.5270194   Train Acc:  0.82306665   Validation Loss =  0.55393034   Validation Acc:  0.8103\n",
      "Iteration  41 : Train Loss =  0.52252513   Train Acc:  0.82458335   Validation Loss =  0.54966927   Validation Acc:  0.8115\n",
      "Iteration  42 : Train Loss =  0.5181281   Train Acc:  0.82565   Validation Loss =  0.5454869   Validation Acc:  0.814\n",
      "Iteration  43 : Train Loss =  0.51394016   Train Acc:  0.82678336   Validation Loss =  0.541667   Validation Acc:  0.8158\n",
      "Iteration  44 : Train Loss =  0.50997996   Train Acc:  0.82823336   Validation Loss =  0.537997   Validation Acc:  0.8168\n",
      "Iteration  45 : Train Loss =  0.50617033   Train Acc:  0.8289   Validation Loss =  0.53432286   Validation Acc:  0.8174\n",
      "Iteration  46 : Train Loss =  0.5025036   Train Acc:  0.8300833   Validation Loss =  0.5309051   Validation Acc:  0.8183\n",
      "Iteration  47 : Train Loss =  0.498985   Train Acc:  0.83133334   Validation Loss =  0.5276104   Validation Acc:  0.8204\n",
      "Iteration  48 : Train Loss =  0.49562678   Train Acc:  0.8322   Validation Loss =  0.52460754   Validation Acc:  0.8215\n",
      "Iteration  49 : Train Loss =  0.49239686   Train Acc:  0.83313334   Validation Loss =  0.5217235   Validation Acc:  0.8221\n",
      "Iteration  50 : Train Loss =  0.4892614   Train Acc:  0.83395   Validation Loss =  0.5189066   Validation Acc:  0.8227\n",
      "Iteration  51 : Train Loss =  0.48623165   Train Acc:  0.83488333   Validation Loss =  0.5161606   Validation Acc:  0.8242\n",
      "Iteration  52 : Train Loss =  0.4832871   Train Acc:  0.83575   Validation Loss =  0.5135291   Validation Acc:  0.8249\n",
      "Iteration  53 : Train Loss =  0.48042297   Train Acc:  0.83643335   Validation Loss =  0.51098865   Validation Acc:  0.8256\n",
      "Iteration  54 : Train Loss =  0.47763965   Train Acc:  0.83703333   Validation Loss =  0.5083738   Validation Acc:  0.8266\n",
      "Iteration  55 : Train Loss =  0.47492242   Train Acc:  0.8377   Validation Loss =  0.50593555   Validation Acc:  0.8279\n",
      "Iteration  56 : Train Loss =  0.47227767   Train Acc:  0.8387333   Validation Loss =  0.5033003   Validation Acc:  0.8292\n",
      "Iteration  57 : Train Loss =  0.46972388   Train Acc:  0.8398167   Validation Loss =  0.5010732   Validation Acc:  0.8288\n",
      "Iteration  58 : Train Loss =  0.46730828   Train Acc:  0.8402167   Validation Loss =  0.49857172   Validation Acc:  0.8299\n",
      "Iteration  59 : Train Loss =  0.4650722   Train Acc:  0.84075   Validation Loss =  0.49693534   Validation Acc:  0.829\n",
      "Iteration  60 : Train Loss =  0.46301395   Train Acc:  0.8415667   Validation Loss =  0.49460137   Validation Acc:  0.8299\n",
      "Iteration  61 : Train Loss =  0.4607855   Train Acc:  0.8426333   Validation Loss =  0.4932284   Validation Acc:  0.8297\n",
      "Iteration  62 : Train Loss =  0.45825192   Train Acc:  0.84326667   Validation Loss =  0.49034268   Validation Acc:  0.8313\n",
      "Iteration  63 : Train Loss =  0.45562023   Train Acc:  0.8443   Validation Loss =  0.48826528   Validation Acc:  0.8321\n",
      "Iteration  64 : Train Loss =  0.45354226   Train Acc:  0.84491664   Validation Loss =  0.48641947   Validation Acc:  0.833\n",
      "Iteration  65 : Train Loss =  0.45180997   Train Acc:  0.8452833   Validation Loss =  0.4845757   Validation Acc:  0.8321\n",
      "Iteration  66 : Train Loss =  0.44978535   Train Acc:  0.8461   Validation Loss =  0.48317656   Validation Acc:  0.8326\n",
      "Iteration  67 : Train Loss =  0.44751853   Train Acc:  0.84705   Validation Loss =  0.48071268   Validation Acc:  0.8332\n",
      "Iteration  68 : Train Loss =  0.44541976   Train Acc:  0.8473333   Validation Loss =  0.47888282   Validation Acc:  0.8345\n",
      "Iteration  69 : Train Loss =  0.44368315   Train Acc:  0.8481167   Validation Loss =  0.47740233   Validation Acc:  0.8347\n",
      "Iteration  70 : Train Loss =  0.44197193   Train Acc:  0.8485   Validation Loss =  0.4755167   Validation Acc:  0.8339\n",
      "Iteration  71 : Train Loss =  0.4399972   Train Acc:  0.84893334   Validation Loss =  0.47399873   Validation Acc:  0.8347\n",
      "Iteration  72 : Train Loss =  0.43801576   Train Acc:  0.84973335   Validation Loss =  0.47200185   Validation Acc:  0.836\n",
      "Iteration  73 : Train Loss =  0.43624386   Train Acc:  0.8502667   Validation Loss =  0.47043514   Validation Acc:  0.8366\n",
      "Iteration  74 : Train Loss =  0.43460444   Train Acc:  0.8507   Validation Loss =  0.46913534   Validation Acc:  0.8365\n",
      "Iteration  75 : Train Loss =  0.4329108   Train Acc:  0.85143334   Validation Loss =  0.4673963   Validation Acc:  0.8383\n",
      "Iteration  76 : Train Loss =  0.4311111   Train Acc:  0.85176665   Validation Loss =  0.46599892   Validation Acc:  0.8378\n",
      "Iteration  77 : Train Loss =  0.4293526   Train Acc:  0.85245   Validation Loss =  0.4642984   Validation Acc:  0.8382\n",
      "Iteration  78 : Train Loss =  0.42770272   Train Acc:  0.85295   Validation Loss =  0.46283814   Validation Acc:  0.8388\n",
      "Iteration  79 : Train Loss =  0.42611438   Train Acc:  0.8533   Validation Loss =  0.46152568   Validation Acc:  0.8389\n",
      "Iteration  80 : Train Loss =  0.42452058   Train Acc:  0.85391665   Validation Loss =  0.4599541   Validation Acc:  0.8409\n",
      "Iteration  81 : Train Loss =  0.42290086   Train Acc:  0.8545   Validation Loss =  0.45869276   Validation Acc:  0.8405\n",
      "Iteration  82 : Train Loss =  0.42129707   Train Acc:  0.85501665   Validation Loss =  0.45716152   Validation Acc:  0.8408\n",
      "Iteration  83 : Train Loss =  0.41971746   Train Acc:  0.8555167   Validation Loss =  0.45587733   Validation Acc:  0.8417\n",
      "Iteration  84 : Train Loss =  0.41816795   Train Acc:  0.8563833   Validation Loss =  0.45452377   Validation Acc:  0.8423\n",
      "Iteration  85 : Train Loss =  0.41664863   Train Acc:  0.85651666   Validation Loss =  0.4531715   Validation Acc:  0.8425\n",
      "Iteration  86 : Train Loss =  0.41516542   Train Acc:  0.8573833   Validation Loss =  0.45197338   Validation Acc:  0.8425\n",
      "Iteration  87 : Train Loss =  0.41371924   Train Acc:  0.8576667   Validation Loss =  0.4506215   Validation Acc:  0.8429\n",
      "Iteration  88 : Train Loss =  0.41229084   Train Acc:  0.8581833   Validation Loss =  0.44951504   Validation Acc:  0.8427\n",
      "Iteration  89 : Train Loss =  0.4108748   Train Acc:  0.85863334   Validation Loss =  0.44816905   Validation Acc:  0.8429\n",
      "Iteration  90 : Train Loss =  0.40945423   Train Acc:  0.85945   Validation Loss =  0.4470623   Validation Acc:  0.8436\n",
      "Iteration  91 : Train Loss =  0.40802553   Train Acc:  0.85965   Validation Loss =  0.4457067   Validation Acc:  0.844\n",
      "Iteration  92 : Train Loss =  0.4065989   Train Acc:  0.8603167   Validation Loss =  0.44456962   Validation Acc:  0.8444\n",
      "Iteration  93 : Train Loss =  0.40519592   Train Acc:  0.8605667   Validation Loss =  0.4432702   Validation Acc:  0.8448\n",
      "Iteration  94 : Train Loss =  0.4038246   Train Acc:  0.86123335   Validation Loss =  0.44217628   Validation Acc:  0.8456\n",
      "Iteration  95 : Train Loss =  0.402491   Train Acc:  0.86141664   Validation Loss =  0.44097894   Validation Acc:  0.8455\n",
      "Iteration  96 : Train Loss =  0.4011847   Train Acc:  0.86218333   Validation Loss =  0.4399358   Validation Acc:  0.8465\n",
      "Iteration  97 : Train Loss =  0.3998978   Train Acc:  0.86263335   Validation Loss =  0.43878427   Validation Acc:  0.8469\n",
      "Iteration  98 : Train Loss =  0.3986302   Train Acc:  0.8631   Validation Loss =  0.43779013   Validation Acc:  0.8472\n",
      "Iteration  99 : Train Loss =  0.39739218   Train Acc:  0.86331666   Validation Loss =  0.43666348   Validation Acc:  0.8482\n",
      "Iteration  100 : Train Loss =  0.39619887   Train Acc:  0.8635833   Validation Loss =  0.435783   Validation Acc:  0.8478\n",
      "Iteration  101 : Train Loss =  0.39511687   Train Acc:  0.86411667   Validation Loss =  0.43474516   Validation Acc:  0.8491\n",
      "Iteration  102 : Train Loss =  0.39414713   Train Acc:  0.86425   Validation Loss =  0.4341878   Validation Acc:  0.849\n",
      "Iteration  103 : Train Loss =  0.39352614   Train Acc:  0.86483335   Validation Loss =  0.43350458   Validation Acc:  0.8495\n",
      "Iteration  104 : Train Loss =  0.39274785   Train Acc:  0.8647   Validation Loss =  0.43328932   Validation Acc:  0.8488\n",
      "Iteration  105 : Train Loss =  0.392   Train Acc:  0.8648667   Validation Loss =  0.4323739   Validation Acc:  0.8502\n",
      "Iteration  106 : Train Loss =  0.38987648   Train Acc:  0.86548334   Validation Loss =  0.43074843   Validation Acc:  0.8502\n",
      "Iteration  107 : Train Loss =  0.38783306   Train Acc:  0.8666667   Validation Loss =  0.4286502   Validation Acc:  0.8514\n",
      "Iteration  108 : Train Loss =  0.3867301   Train Acc:  0.8670833   Validation Loss =  0.42773417   Validation Acc:  0.8517\n",
      "Iteration  109 : Train Loss =  0.38632962   Train Acc:  0.8671833   Validation Loss =  0.4277027   Validation Acc:  0.8514\n",
      "Iteration  110 : Train Loss =  0.3856648   Train Acc:  0.8672   Validation Loss =  0.4269703   Validation Acc:  0.8524\n",
      "Iteration  111 : Train Loss =  0.38386372   Train Acc:  0.86793333   Validation Loss =  0.42553535   Validation Acc:  0.8524\n",
      "Iteration  112 : Train Loss =  0.38231295   Train Acc:  0.86873335   Validation Loss =  0.42403907   Validation Acc:  0.8529\n",
      "Iteration  113 : Train Loss =  0.3815656   Train Acc:  0.86865   Validation Loss =  0.42341152   Validation Acc:  0.8541\n",
      "Iteration  114 : Train Loss =  0.38094595   Train Acc:  0.869   Validation Loss =  0.42313144   Validation Acc:  0.8534\n",
      "Iteration  115 : Train Loss =  0.37985873   Train Acc:  0.86925   Validation Loss =  0.4220383   Validation Acc:  0.8546\n",
      "Iteration  116 : Train Loss =  0.3783885   Train Acc:  0.87005   Validation Loss =  0.42082828   Validation Acc:  0.8547\n",
      "Iteration  117 : Train Loss =  0.3773485   Train Acc:  0.8706167   Validation Loss =  0.41996866   Validation Acc:  0.8555\n",
      "Iteration  118 : Train Loss =  0.37665495   Train Acc:  0.87046665   Validation Loss =  0.4193649   Validation Acc:  0.8547\n",
      "Iteration  119 : Train Loss =  0.37574196   Train Acc:  0.87088335   Validation Loss =  0.41877675   Validation Acc:  0.8558\n",
      "Iteration  120 : Train Loss =  0.37455136   Train Acc:  0.87123334   Validation Loss =  0.41762656   Validation Acc:  0.855\n",
      "Iteration  121 : Train Loss =  0.37342563   Train Acc:  0.8721333   Validation Loss =  0.41670343   Validation Acc:  0.8568\n",
      "Iteration  122 : Train Loss =  0.37261108   Train Acc:  0.87223333   Validation Loss =  0.4161195   Validation Acc:  0.8571\n",
      "Iteration  123 : Train Loss =  0.3718419   Train Acc:  0.87256664   Validation Loss =  0.4154212   Validation Acc:  0.8559\n",
      "Iteration  124 : Train Loss =  0.37083474   Train Acc:  0.8729   Validation Loss =  0.41469815   Validation Acc:  0.8575\n",
      "Iteration  125 : Train Loss =  0.3697035   Train Acc:  0.8731667   Validation Loss =  0.41364887   Validation Acc:  0.8561\n",
      "Iteration  126 : Train Loss =  0.3687351   Train Acc:  0.87376666   Validation Loss =  0.41287553   Validation Acc:  0.8573\n",
      "Iteration  127 : Train Loss =  0.36795294   Train Acc:  0.8739167   Validation Loss =  0.4123268   Validation Acc:  0.8588\n",
      "Iteration  128 : Train Loss =  0.367144   Train Acc:  0.8743   Validation Loss =  0.4116042   Validation Acc:  0.8576\n",
      "Iteration  129 : Train Loss =  0.36618358   Train Acc:  0.8742833   Validation Loss =  0.41093275   Validation Acc:  0.8588\n",
      "Iteration  130 : Train Loss =  0.3651733   Train Acc:  0.875   Validation Loss =  0.4100325   Validation Acc:  0.8582\n",
      "Iteration  131 : Train Loss =  0.36426425   Train Acc:  0.87516665   Validation Loss =  0.40933427   Validation Acc:  0.8589\n",
      "Iteration  132 : Train Loss =  0.36345887   Train Acc:  0.87493336   Validation Loss =  0.40873632   Validation Acc:  0.8593\n",
      "Iteration  133 : Train Loss =  0.36265424   Train Acc:  0.87593335   Validation Loss =  0.40804666   Validation Acc:  0.8585\n",
      "Iteration  134 : Train Loss =  0.3617614   Train Acc:  0.87558335   Validation Loss =  0.40738967   Validation Acc:  0.8594\n",
      "Iteration  135 : Train Loss =  0.3608334   Train Acc:  0.8764667   Validation Loss =  0.40658537   Validation Acc:  0.8595\n",
      "Iteration  136 : Train Loss =  0.35993555   Train Acc:  0.87645   Validation Loss =  0.4058697   Validation Acc:  0.8596\n",
      "Iteration  137 : Train Loss =  0.3591091   Train Acc:  0.87656665   Validation Loss =  0.40526173   Validation Acc:  0.8598\n",
      "Iteration  138 : Train Loss =  0.35833007   Train Acc:  0.87721664   Validation Loss =  0.40460268   Validation Acc:  0.8601\n",
      "Iteration  139 : Train Loss =  0.3575509   Train Acc:  0.8771167   Validation Loss =  0.40411508   Validation Acc:  0.8609\n",
      "Iteration  140 : Train Loss =  0.35676005   Train Acc:  0.87766665   Validation Loss =  0.40341172   Validation Acc:  0.8606\n",
      "Iteration  141 : Train Loss =  0.35596922   Train Acc:  0.8778333   Validation Loss =  0.4029375   Validation Acc:  0.8611\n",
      "Iteration  142 : Train Loss =  0.35525665   Train Acc:  0.87803334   Validation Loss =  0.40231234   Validation Acc:  0.8614\n",
      "Iteration  143 : Train Loss =  0.35466477   Train Acc:  0.87843335   Validation Loss =  0.40202433   Validation Acc:  0.8614\n",
      "Iteration  144 : Train Loss =  0.35423225   Train Acc:  0.8783   Validation Loss =  0.40168607   Validation Acc:  0.8615\n",
      "Iteration  145 : Train Loss =  0.35398042   Train Acc:  0.87843335   Validation Loss =  0.40174896   Validation Acc:  0.8617\n",
      "Iteration  146 : Train Loss =  0.35342494   Train Acc:  0.878   Validation Loss =  0.40123796   Validation Acc:  0.8612\n",
      "Iteration  147 : Train Loss =  0.352606   Train Acc:  0.8790333   Validation Loss =  0.4007335   Validation Acc:  0.8621\n",
      "Iteration  148 : Train Loss =  0.3510278   Train Acc:  0.87908334   Validation Loss =  0.39918754   Validation Acc:  0.8615\n",
      "Iteration  149 : Train Loss =  0.34960234   Train Acc:  0.8799667   Validation Loss =  0.39798117   Validation Acc:  0.8628\n",
      "Iteration  150 : Train Loss =  0.3487517   Train Acc:  0.88011664   Validation Loss =  0.39729413   Validation Acc:  0.8632\n",
      "Iteration  151 : Train Loss =  0.34842753   Train Acc:  0.88013333   Validation Loss =  0.39709952   Validation Acc:  0.863\n",
      "Iteration  152 : Train Loss =  0.34815937   Train Acc:  0.88061666   Validation Loss =  0.39711645   Validation Acc:  0.8627\n",
      "Iteration  153 : Train Loss =  0.3473697   Train Acc:  0.8803333   Validation Loss =  0.39637384   Validation Acc:  0.8632\n",
      "Iteration  154 : Train Loss =  0.34627116   Train Acc:  0.88088334   Validation Loss =  0.3955253   Validation Acc:  0.8634\n",
      "Iteration  155 : Train Loss =  0.34516913   Train Acc:  0.88135   Validation Loss =  0.39452657   Validation Acc:  0.864\n",
      "Iteration  156 : Train Loss =  0.34446025   Train Acc:  0.8814333   Validation Loss =  0.39395875   Validation Acc:  0.8641\n",
      "Iteration  157 : Train Loss =  0.34403512   Train Acc:  0.88145   Validation Loss =  0.39378643   Validation Acc:  0.8638\n",
      "Iteration  158 : Train Loss =  0.34353974   Train Acc:  0.8817   Validation Loss =  0.3933444   Validation Acc:  0.8648\n",
      "Iteration  159 : Train Loss =  0.3428091   Train Acc:  0.88198334   Validation Loss =  0.39290053   Validation Acc:  0.8641\n",
      "Iteration  160 : Train Loss =  0.3418568   Train Acc:  0.8824833   Validation Loss =  0.39201927   Validation Acc:  0.8647\n",
      "Iteration  161 : Train Loss =  0.3409959   Train Acc:  0.88236666   Validation Loss =  0.39134276   Validation Acc:  0.8649\n",
      "Iteration  162 : Train Loss =  0.34033763   Train Acc:  0.8829   Validation Loss =  0.39088556   Validation Acc:  0.8653\n",
      "Iteration  163 : Train Loss =  0.33981213   Train Acc:  0.8831   Validation Loss =  0.39042723   Validation Acc:  0.865\n",
      "Iteration  164 : Train Loss =  0.33924568   Train Acc:  0.88338333   Validation Loss =  0.39015776   Validation Acc:  0.8646\n",
      "Iteration  165 : Train Loss =  0.33854556   Train Acc:  0.88325   Validation Loss =  0.3894878   Validation Acc:  0.8655\n",
      "Iteration  166 : Train Loss =  0.33775976   Train Acc:  0.8839667   Validation Loss =  0.38896754   Validation Acc:  0.8649\n",
      "Iteration  167 : Train Loss =  0.33699003   Train Acc:  0.88406664   Validation Loss =  0.38830698   Validation Acc:  0.866\n",
      "Iteration  168 : Train Loss =  0.33632115   Train Acc:  0.8840167   Validation Loss =  0.38779384   Validation Acc:  0.8651\n",
      "Iteration  169 : Train Loss =  0.33573827   Train Acc:  0.8847333   Validation Loss =  0.38745433   Validation Acc:  0.8663\n",
      "Iteration  170 : Train Loss =  0.33517718   Train Acc:  0.88415   Validation Loss =  0.38694063   Validation Acc:  0.8666\n",
      "Iteration  171 : Train Loss =  0.3345617   Train Acc:  0.88535   Validation Loss =  0.3866405   Validation Acc:  0.8657\n",
      "Iteration  172 : Train Loss =  0.33389074   Train Acc:  0.8846833   Validation Loss =  0.38598028   Validation Acc:  0.8663\n",
      "Iteration  173 : Train Loss =  0.3331727   Train Acc:  0.88558334   Validation Loss =  0.3855467   Validation Acc:  0.8657\n",
      "Iteration  174 : Train Loss =  0.33246204   Train Acc:  0.8854   Validation Loss =  0.38490173   Validation Acc:  0.8657\n",
      "Iteration  175 : Train Loss =  0.3317892   Train Acc:  0.8857167   Validation Loss =  0.38443607   Validation Acc:  0.8661\n",
      "Iteration  176 : Train Loss =  0.3311634   Train Acc:  0.8861333   Validation Loss =  0.38398057   Validation Acc:  0.8664\n",
      "Iteration  177 : Train Loss =  0.3305708   Train Acc:  0.88615   Validation Loss =  0.38351372   Validation Acc:  0.8663\n",
      "Iteration  178 : Train Loss =  0.3299909   Train Acc:  0.88661665   Validation Loss =  0.38319382   Validation Acc:  0.8666\n",
      "Iteration  179 : Train Loss =  0.32940814   Train Acc:  0.88638335   Validation Loss =  0.3826817   Validation Acc:  0.8661\n",
      "Iteration  180 : Train Loss =  0.3288126   Train Acc:  0.8871667   Validation Loss =  0.38239154   Validation Acc:  0.8667\n",
      "Iteration  181 : Train Loss =  0.3282053   Train Acc:  0.88675   Validation Loss =  0.38182846   Validation Acc:  0.8664\n",
      "Iteration  182 : Train Loss =  0.32759187   Train Acc:  0.8872667   Validation Loss =  0.38151667   Validation Acc:  0.8669\n",
      "Iteration  183 : Train Loss =  0.32698107   Train Acc:  0.8871833   Validation Loss =  0.38095313   Validation Acc:  0.8666\n",
      "Iteration  184 : Train Loss =  0.32639477   Train Acc:  0.8878667   Validation Loss =  0.38065264   Validation Acc:  0.8672\n",
      "Iteration  185 : Train Loss =  0.32584244   Train Acc:  0.8877   Validation Loss =  0.3801728   Validation Acc:  0.8668\n",
      "Iteration  186 : Train Loss =  0.32540137   Train Acc:  0.88765   Validation Loss =  0.38000867   Validation Acc:  0.867\n",
      "Iteration  187 : Train Loss =  0.32510784   Train Acc:  0.8878667   Validation Loss =  0.37983316   Validation Acc:  0.8672\n",
      "Iteration  188 : Train Loss =  0.32525626   Train Acc:  0.88745   Validation Loss =  0.38024715   Validation Acc:  0.8671\n",
      "Iteration  189 : Train Loss =  0.32572845   Train Acc:  0.88736665   Validation Loss =  0.38091505   Validation Acc:  0.8667\n",
      "Iteration  190 : Train Loss =  0.32673648   Train Acc:  0.88655   Validation Loss =  0.38215172   Validation Acc:  0.8672\n",
      "Iteration  191 : Train Loss =  0.32588398   Train Acc:  0.88701665   Validation Loss =  0.3814852   Validation Acc:  0.8657\n",
      "Iteration  192 : Train Loss =  0.32363755   Train Acc:  0.88766664   Validation Loss =  0.37930512   Validation Acc:  0.8669\n",
      "Iteration  193 : Train Loss =  0.32119864   Train Acc:  0.88888335   Validation Loss =  0.3768894   Validation Acc:  0.8673\n",
      "Iteration  194 : Train Loss =  0.32112333   Train Acc:  0.88926667   Validation Loss =  0.37710273   Validation Acc:  0.8672\n",
      "Iteration  195 : Train Loss =  0.3219761   Train Acc:  0.88815   Validation Loss =  0.37803617   Validation Acc:  0.8681\n",
      "Iteration  196 : Train Loss =  0.32111934   Train Acc:  0.889   Validation Loss =  0.37746656   Validation Acc:  0.8687\n",
      "Iteration  197 : Train Loss =  0.31928736   Train Acc:  0.8896   Validation Loss =  0.37573785   Validation Acc:  0.8686\n",
      "Iteration  198 : Train Loss =  0.31868505   Train Acc:  0.8896   Validation Loss =  0.3751564   Validation Acc:  0.8687\n",
      "Iteration  199 : Train Loss =  0.3191099   Train Acc:  0.88983333   Validation Loss =  0.37606022   Validation Acc:  0.8679\n",
      "Iteration  200 : Train Loss =  0.31851244   Train Acc:  0.88953334   Validation Loss =  0.37536514   Validation Acc:  0.8693\n",
      "Iteration  201 : Train Loss =  0.31700483   Train Acc:  0.8904667   Validation Loss =  0.37416273   Validation Acc:  0.8684\n",
      "Iteration  202 : Train Loss =  0.3164119   Train Acc:  0.89058334   Validation Loss =  0.3737366   Validation Acc:  0.8683\n",
      "Iteration  203 : Train Loss =  0.3166017   Train Acc:  0.89033335   Validation Loss =  0.37397748   Validation Acc:  0.8694\n",
      "Iteration  204 : Train Loss =  0.31605813   Train Acc:  0.89068335   Validation Loss =  0.37381622   Validation Acc:  0.8688\n",
      "Iteration  205 : Train Loss =  0.3149053   Train Acc:  0.8908   Validation Loss =  0.37260556   Validation Acc:  0.8695\n",
      "Iteration  206 : Train Loss =  0.31437948   Train Acc:  0.89143336   Validation Loss =  0.37233043   Validation Acc:  0.8694\n",
      "Iteration  207 : Train Loss =  0.31433398   Train Acc:  0.89141667   Validation Loss =  0.37250766   Validation Acc:  0.8694\n",
      "Iteration  208 : Train Loss =  0.31377655   Train Acc:  0.89143336   Validation Loss =  0.37202176   Validation Acc:  0.8696\n",
      "Iteration  209 : Train Loss =  0.3128267   Train Acc:  0.89171666   Validation Loss =  0.37126854   Validation Acc:  0.869\n",
      "Iteration  210 : Train Loss =  0.312325   Train Acc:  0.8919167   Validation Loss =  0.37097067   Validation Acc:  0.8691\n",
      "Iteration  211 : Train Loss =  0.31214043   Train Acc:  0.89203334   Validation Loss =  0.3708616   Validation Acc:  0.8701\n",
      "Iteration  212 : Train Loss =  0.31160918   Train Acc:  0.89211667   Validation Loss =  0.37062135   Validation Acc:  0.8687\n",
      "Iteration  213 : Train Loss =  0.31083027   Train Acc:  0.8926   Validation Loss =  0.36993715   Validation Acc:  0.8694\n",
      "Iteration  214 : Train Loss =  0.31032977   Train Acc:  0.89246666   Validation Loss =  0.36957765   Validation Acc:  0.8697\n",
      "Iteration  215 : Train Loss =  0.31005442   Train Acc:  0.8925833   Validation Loss =  0.36958107   Validation Acc:  0.869\n",
      "Iteration  216 : Train Loss =  0.3095814   Train Acc:  0.89288336   Validation Loss =  0.36911413   Validation Acc:  0.8698\n",
      "Iteration  217 : Train Loss =  0.30890805   Train Acc:  0.8929667   Validation Loss =  0.36874187   Validation Acc:  0.8697\n",
      "Iteration  218 : Train Loss =  0.30838606   Train Acc:  0.8933   Validation Loss =  0.36829478   Validation Acc:  0.8698\n",
      "Iteration  219 : Train Loss =  0.30805162   Train Acc:  0.8936667   Validation Loss =  0.36815104   Validation Acc:  0.8699\n",
      "Iteration  220 : Train Loss =  0.30762896   Train Acc:  0.8933667   Validation Loss =  0.367898   Validation Acc:  0.8698\n",
      "Iteration  221 : Train Loss =  0.30706605   Train Acc:  0.8939833   Validation Loss =  0.36752623   Validation Acc:  0.8697\n",
      "Iteration  222 : Train Loss =  0.30655125   Train Acc:  0.89356667   Validation Loss =  0.36705995   Validation Acc:  0.8708\n",
      "Iteration  223 : Train Loss =  0.30620676   Train Acc:  0.8940167   Validation Loss =  0.3670912   Validation Acc:  0.8698\n",
      "Iteration  224 : Train Loss =  0.30592644   Train Acc:  0.8939   Validation Loss =  0.36668903   Validation Acc:  0.871\n",
      "Iteration  225 : Train Loss =  0.3055892   Train Acc:  0.8943667   Validation Loss =  0.36687687   Validation Acc:  0.8704\n",
      "Iteration  226 : Train Loss =  0.30538338   Train Acc:  0.89395   Validation Loss =  0.3664274   Validation Acc:  0.8706\n",
      "Iteration  227 : Train Loss =  0.30548516   Train Acc:  0.89433336   Validation Loss =  0.36718234   Validation Acc:  0.8704\n",
      "Iteration  228 : Train Loss =  0.3057003   Train Acc:  0.89346665   Validation Loss =  0.36703864   Validation Acc:  0.8706\n",
      "Iteration  229 : Train Loss =  0.30625668   Train Acc:  0.89415   Validation Loss =  0.36841887   Validation Acc:  0.8708\n",
      "Iteration  230 : Train Loss =  0.30563435   Train Acc:  0.89356667   Validation Loss =  0.3672351   Validation Acc:  0.8703\n",
      "Iteration  231 : Train Loss =  0.30461633   Train Acc:  0.8948   Validation Loss =  0.36707157   Validation Acc:  0.8709\n",
      "Iteration  232 : Train Loss =  0.30282426   Train Acc:  0.8947667   Validation Loss =  0.36480486   Validation Acc:  0.871\n",
      "Iteration  233 : Train Loss =  0.30152863   Train Acc:  0.89566666   Validation Loss =  0.3639629   Validation Acc:  0.8701\n",
      "Iteration  234 : Train Loss =  0.3012858   Train Acc:  0.89593333   Validation Loss =  0.3639218   Validation Acc:  0.8702\n",
      "Iteration  235 : Train Loss =  0.30163854   Train Acc:  0.89503336   Validation Loss =  0.3641024   Validation Acc:  0.8719\n",
      "Iteration  236 : Train Loss =  0.30173957   Train Acc:  0.8958667   Validation Loss =  0.3649017   Validation Acc:  0.8715\n",
      "Iteration  237 : Train Loss =  0.30090165   Train Acc:  0.89526665   Validation Loss =  0.36365753   Validation Acc:  0.8719\n",
      "Iteration  238 : Train Loss =  0.29968458   Train Acc:  0.8964667   Validation Loss =  0.36299366   Validation Acc:  0.8712\n",
      "Iteration  239 : Train Loss =  0.2988817   Train Acc:  0.89635   Validation Loss =  0.36220953   Validation Acc:  0.8706\n",
      "Iteration  240 : Train Loss =  0.29875177   Train Acc:  0.89633334   Validation Loss =  0.3621099   Validation Acc:  0.873\n",
      "Iteration  241 : Train Loss =  0.29879525   Train Acc:  0.8965333   Validation Loss =  0.36268216   Validation Acc:  0.8723\n",
      "Iteration  242 : Train Loss =  0.29845533   Train Acc:  0.89635   Validation Loss =  0.36208627   Validation Acc:  0.8727\n",
      "Iteration  243 : Train Loss =  0.2977622   Train Acc:  0.8971   Validation Loss =  0.3619068   Validation Acc:  0.8712\n",
      "Iteration  244 : Train Loss =  0.29695672   Train Acc:  0.89683336   Validation Loss =  0.3610589   Validation Acc:  0.872\n",
      "Iteration  245 : Train Loss =  0.29642755   Train Acc:  0.89741665   Validation Loss =  0.36066884   Validation Acc:  0.8723\n",
      "Iteration  246 : Train Loss =  0.29616556   Train Acc:  0.89751667   Validation Loss =  0.36072338   Validation Acc:  0.8721\n",
      "Iteration  247 : Train Loss =  0.29596454   Train Acc:  0.8971   Validation Loss =  0.36043927   Validation Acc:  0.8733\n",
      "Iteration  248 : Train Loss =  0.29565215   Train Acc:  0.89771664   Validation Loss =  0.3605511   Validation Acc:  0.8719\n",
      "Iteration  249 : Train Loss =  0.29507852   Train Acc:  0.89773333   Validation Loss =  0.359918   Validation Acc:  0.8722\n",
      "Iteration  250 : Train Loss =  0.29445806   Train Acc:  0.89816666   Validation Loss =  0.3595705   Validation Acc:  0.8726\n",
      "Iteration  251 : Train Loss =  0.2939267   Train Acc:  0.8982   Validation Loss =  0.35917726   Validation Acc:  0.8719\n",
      "Iteration  252 : Train Loss =  0.29357827   Train Acc:  0.8983   Validation Loss =  0.358936   Validation Acc:  0.8729\n",
      "Iteration  253 : Train Loss =  0.29333216   Train Acc:  0.89841664   Validation Loss =  0.3589897   Validation Acc:  0.8725\n",
      "Iteration  254 : Train Loss =  0.2930127   Train Acc:  0.8985   Validation Loss =  0.35866985   Validation Acc:  0.8727\n",
      "Iteration  255 : Train Loss =  0.29259148   Train Acc:  0.8986833   Validation Loss =  0.3585783   Validation Acc:  0.8722\n",
      "Iteration  256 : Train Loss =  0.2920567   Train Acc:  0.89895   Validation Loss =  0.35805053   Validation Acc:  0.8731\n",
      "Iteration  257 : Train Loss =  0.29154968   Train Acc:  0.89895   Validation Loss =  0.3578327   Validation Acc:  0.8727\n",
      "Iteration  258 : Train Loss =  0.29111686   Train Acc:  0.89923334   Validation Loss =  0.3574913   Validation Acc:  0.8735\n",
      "Iteration  259 : Train Loss =  0.2907513   Train Acc:  0.89931667   Validation Loss =  0.35729393   Validation Acc:  0.8731\n",
      "Iteration  260 : Train Loss =  0.29041338   Train Acc:  0.8994167   Validation Loss =  0.3571792   Validation Acc:  0.8733\n",
      "Iteration  261 : Train Loss =  0.29006293   Train Acc:  0.89955   Validation Loss =  0.35688093   Validation Acc:  0.8742\n",
      "Iteration  262 : Train Loss =  0.2896961   Train Acc:  0.8998167   Validation Loss =  0.35685393   Validation Acc:  0.8733\n",
      "Iteration  263 : Train Loss =  0.28929302   Train Acc:  0.8997333   Validation Loss =  0.35641596   Validation Acc:  0.8747\n",
      "Iteration  264 : Train Loss =  0.2888685   Train Acc:  0.9001167   Validation Loss =  0.3563522   Validation Acc:  0.8737\n",
      "Iteration  265 : Train Loss =  0.2884279   Train Acc:  0.9   Validation Loss =  0.3558979   Validation Acc:  0.875\n",
      "Iteration  266 : Train Loss =  0.28798756   Train Acc:  0.90008336   Validation Loss =  0.3557579   Validation Acc:  0.8741\n",
      "Iteration  267 : Train Loss =  0.28756312   Train Acc:  0.9002333   Validation Loss =  0.35541874   Validation Acc:  0.8745\n",
      "Iteration  268 : Train Loss =  0.28716013   Train Acc:  0.90031666   Validation Loss =  0.35522464   Validation Acc:  0.8745\n",
      "Iteration  269 : Train Loss =  0.28677496   Train Acc:  0.90045   Validation Loss =  0.35503033   Validation Acc:  0.8746\n",
      "Iteration  270 : Train Loss =  0.28640074   Train Acc:  0.9004667   Validation Loss =  0.35476544   Validation Acc:  0.8753\n",
      "Iteration  271 : Train Loss =  0.2860311   Train Acc:  0.90065   Validation Loss =  0.35466236   Validation Acc:  0.8752\n",
      "Iteration  272 : Train Loss =  0.28566423   Train Acc:  0.90101665   Validation Loss =  0.35433155   Validation Acc:  0.8753\n",
      "Iteration  273 : Train Loss =  0.28530514   Train Acc:  0.90095   Validation Loss =  0.35427785   Validation Acc:  0.8749\n",
      "Iteration  274 : Train Loss =  0.28495553   Train Acc:  0.9012167   Validation Loss =  0.3539399   Validation Acc:  0.8751\n",
      "Iteration  275 : Train Loss =  0.28463158   Train Acc:  0.90101665   Validation Loss =  0.35396683   Validation Acc:  0.8748\n",
      "Iteration  276 : Train Loss =  0.28432155   Train Acc:  0.9014   Validation Loss =  0.35361516   Validation Acc:  0.8756\n",
      "Iteration  277 : Train Loss =  0.28407234   Train Acc:  0.9012667   Validation Loss =  0.3537949   Validation Acc:  0.8746\n",
      "Iteration  278 : Train Loss =  0.28384197   Train Acc:  0.90148336   Validation Loss =  0.35343677   Validation Acc:  0.876\n",
      "Iteration  279 : Train Loss =  0.28374904   Train Acc:  0.90146667   Validation Loss =  0.3538907   Validation Acc:  0.8745\n",
      "Iteration  280 : Train Loss =  0.28365368   Train Acc:  0.9012333   Validation Loss =  0.35352135   Validation Acc:  0.8753\n",
      "Iteration  281 : Train Loss =  0.28380027   Train Acc:  0.90133333   Validation Loss =  0.35439435   Validation Acc:  0.8738\n",
      "Iteration  282 : Train Loss =  0.28370586   Train Acc:  0.90108335   Validation Loss =  0.353829   Validation Acc:  0.8755\n",
      "Iteration  283 : Train Loss =  0.28375873   Train Acc:  0.90103334   Validation Loss =  0.35476464   Validation Acc:  0.8745\n",
      "Iteration  284 : Train Loss =  0.28304785   Train Acc:  0.90133333   Validation Loss =  0.35347345   Validation Acc:  0.8759\n",
      "Iteration  285 : Train Loss =  0.2822149   Train Acc:  0.902   Validation Loss =  0.35345787   Validation Acc:  0.8743\n",
      "Iteration  286 : Train Loss =  0.28097448   Train Acc:  0.9022   Validation Loss =  0.35184598   Validation Acc:  0.8769\n",
      "Iteration  287 : Train Loss =  0.28004193   Train Acc:  0.90265   Validation Loss =  0.35137272   Validation Acc:  0.8758\n",
      "Iteration  288 : Train Loss =  0.27956152   Train Acc:  0.90255   Validation Loss =  0.35100973   Validation Acc:  0.8762\n",
      "Iteration  289 : Train Loss =  0.27948117   Train Acc:  0.90258336   Validation Loss =  0.35092476   Validation Acc:  0.8762\n",
      "Iteration  290 : Train Loss =  0.2795744   Train Acc:  0.90278333   Validation Loss =  0.35158074   Validation Acc:  0.8755\n",
      "Iteration  291 : Train Loss =  0.27949253   Train Acc:  0.9026667   Validation Loss =  0.35118017   Validation Acc:  0.8766\n",
      "Iteration  292 : Train Loss =  0.27924505   Train Acc:  0.90275   Validation Loss =  0.35163656   Validation Acc:  0.8749\n",
      "Iteration  293 : Train Loss =  0.27857444   Train Acc:  0.90313333   Validation Loss =  0.35061467   Validation Acc:  0.8773\n",
      "Iteration  294 : Train Loss =  0.27786654   Train Acc:  0.9030833   Validation Loss =  0.35045654   Validation Acc:  0.8758\n",
      "Iteration  295 : Train Loss =  0.27716413   Train Acc:  0.90326667   Validation Loss =  0.34967992   Validation Acc:  0.876\n",
      "Iteration  296 : Train Loss =  0.27667663   Train Acc:  0.90351665   Validation Loss =  0.3494151   Validation Acc:  0.8768\n",
      "Iteration  297 : Train Loss =  0.27638423   Train Acc:  0.90375   Validation Loss =  0.34940857   Validation Acc:  0.8777\n",
      "Iteration  298 : Train Loss =  0.27620268   Train Acc:  0.9036   Validation Loss =  0.3491549   Validation Acc:  0.8764\n",
      "Iteration  299 : Train Loss =  0.27603063   Train Acc:  0.9039   Validation Loss =  0.34949824   Validation Acc:  0.8767\n",
      "Iteration  300 : Train Loss =  0.27577347   Train Acc:  0.9038   Validation Loss =  0.34902203   Validation Acc:  0.8771\n",
      "Iteration  301 : Train Loss =  0.27545273   Train Acc:  0.90365   Validation Loss =  0.3492559   Validation Acc:  0.8763\n",
      "Iteration  302 : Train Loss =  0.27499533   Train Acc:  0.9042   Validation Loss =  0.34860936   Validation Acc:  0.8765\n",
      "Iteration  303 : Train Loss =  0.2745468   Train Acc:  0.90415   Validation Loss =  0.34860292   Validation Acc:  0.8765\n",
      "Iteration  304 : Train Loss =  0.27407002   Train Acc:  0.90468335   Validation Loss =  0.34811372   Validation Acc:  0.8764\n",
      "Iteration  305 : Train Loss =  0.27364844   Train Acc:  0.9044333   Validation Loss =  0.34792808   Validation Acc:  0.8772\n",
      "Iteration  306 : Train Loss =  0.27326754   Train Acc:  0.90506667   Validation Loss =  0.3477426   Validation Acc:  0.8767\n",
      "Iteration  307 : Train Loss =  0.27292517   Train Acc:  0.90493333   Validation Loss =  0.3474441   Validation Acc:  0.8763\n",
      "Iteration  308 : Train Loss =  0.27260196   Train Acc:  0.90528333   Validation Loss =  0.34749264   Validation Acc:  0.8773\n",
      "Iteration  309 : Train Loss =  0.2722913   Train Acc:  0.9051   Validation Loss =  0.34709722   Validation Acc:  0.8771\n",
      "Iteration  310 : Train Loss =  0.27196434   Train Acc:  0.90545   Validation Loss =  0.34724998   Validation Acc:  0.8783\n",
      "Iteration  311 : Train Loss =  0.27164197   Train Acc:  0.90528333   Validation Loss =  0.3467835   Validation Acc:  0.8771\n",
      "Iteration  312 : Train Loss =  0.27130195   Train Acc:  0.90561664   Validation Loss =  0.3469624   Validation Acc:  0.8786\n",
      "Iteration  313 : Train Loss =  0.27096775   Train Acc:  0.9055   Validation Loss =  0.34646904   Validation Acc:  0.8772\n",
      "Iteration  314 : Train Loss =  0.2706436   Train Acc:  0.90545   Validation Loss =  0.34666118   Validation Acc:  0.8784\n",
      "Iteration  315 : Train Loss =  0.27032575   Train Acc:  0.90575   Validation Loss =  0.34619778   Validation Acc:  0.8769\n",
      "Iteration  316 : Train Loss =  0.27006456   Train Acc:  0.90543336   Validation Loss =  0.34643614   Validation Acc:  0.8777\n",
      "Iteration  317 : Train Loss =  0.26981142   Train Acc:  0.90601665   Validation Loss =  0.34604353   Validation Acc:  0.8768\n",
      "Iteration  318 : Train Loss =  0.26968423   Train Acc:  0.9055667   Validation Loss =  0.3464037   Validation Acc:  0.8768\n",
      "Iteration  319 : Train Loss =  0.2695398   Train Acc:  0.90595   Validation Loss =  0.34613517   Validation Acc:  0.8777\n",
      "Iteration  320 : Train Loss =  0.26960877   Train Acc:  0.9055833   Validation Loss =  0.3466995   Validation Acc:  0.8774\n",
      "Iteration  321 : Train Loss =  0.26951647   Train Acc:  0.90596664   Validation Loss =  0.3464819   Validation Acc:  0.8778\n",
      "Iteration  322 : Train Loss =  0.26965672   Train Acc:  0.9055833   Validation Loss =  0.3471221   Validation Acc:  0.8774\n",
      "Iteration  323 : Train Loss =  0.26925215   Train Acc:  0.90625   Validation Loss =  0.34660724   Validation Acc:  0.8778\n",
      "Iteration  324 : Train Loss =  0.26887172   Train Acc:  0.90566665   Validation Loss =  0.34667173   Validation Acc:  0.878\n",
      "Iteration  325 : Train Loss =  0.2678617   Train Acc:  0.9065833   Validation Loss =  0.34560952   Validation Acc:  0.8786\n",
      "Iteration  326 : Train Loss =  0.26690736   Train Acc:  0.9065833   Validation Loss =  0.34496588   Validation Acc:  0.8785\n",
      "Iteration  327 : Train Loss =  0.26607576   Train Acc:  0.90721667   Validation Loss =  0.34422776   Validation Acc:  0.8782\n",
      "Iteration  328 : Train Loss =  0.2656164   Train Acc:  0.90725   Validation Loss =  0.34394604   Validation Acc:  0.8781\n",
      "Iteration  329 : Train Loss =  0.2654901   Train Acc:  0.9073   Validation Loss =  0.34408668   Validation Acc:  0.8787\n",
      "Iteration  330 : Train Loss =  0.2655101   Train Acc:  0.90713334   Validation Loss =  0.34415233   Validation Acc:  0.8775\n",
      "Iteration  331 : Train Loss =  0.26556593   Train Acc:  0.9069167   Validation Loss =  0.3446171   Validation Acc:  0.8785\n",
      "Iteration  332 : Train Loss =  0.26535094   Train Acc:  0.9073167   Validation Loss =  0.34432086   Validation Acc:  0.8788\n",
      "Iteration  333 : Train Loss =  0.2650788   Train Acc:  0.90715   Validation Loss =  0.34454775   Validation Acc:  0.8781\n",
      "Iteration  334 : Train Loss =  0.2644945   Train Acc:  0.90755   Validation Loss =  0.34376356   Validation Acc:  0.8777\n",
      "Iteration  335 : Train Loss =  0.2640155   Train Acc:  0.90753335   Validation Loss =  0.34385487   Validation Acc:  0.879\n",
      "Iteration  336 : Train Loss =  0.2636197   Train Acc:  0.9083667   Validation Loss =  0.34318036   Validation Acc:  0.8782\n",
      "Iteration  337 : Train Loss =  0.26346186   Train Acc:  0.9083667   Validation Loss =  0.3436878   Validation Acc:  0.8789\n",
      "Iteration  338 : Train Loss =  0.26361415   Train Acc:  0.9080167   Validation Loss =  0.34348136   Validation Acc:  0.8785\n",
      "Iteration  339 : Train Loss =  0.26378074   Train Acc:  0.9079667   Validation Loss =  0.34447387   Validation Acc:  0.8788\n",
      "Iteration  340 : Train Loss =  0.26404393   Train Acc:  0.9076833   Validation Loss =  0.3442113   Validation Acc:  0.8786\n",
      "Iteration  341 : Train Loss =  0.26361585   Train Acc:  0.90805   Validation Loss =  0.34474027   Validation Acc:  0.8793\n",
      "Iteration  342 : Train Loss =  0.2629014   Train Acc:  0.90815   Validation Loss =  0.34344414   Validation Acc:  0.8786\n",
      "Iteration  343 : Train Loss =  0.26169205   Train Acc:  0.9086667   Validation Loss =  0.34303287   Validation Acc:  0.8794\n",
      "Iteration  344 : Train Loss =  0.2608683   Train Acc:  0.90895   Validation Loss =  0.34193268   Validation Acc:  0.8778\n",
      "Iteration  345 : Train Loss =  0.26062578   Train Acc:  0.9089   Validation Loss =  0.3420839   Validation Acc:  0.8795\n",
      "Iteration  346 : Train Loss =  0.26068708   Train Acc:  0.90868336   Validation Loss =  0.34231922   Validation Acc:  0.8792\n",
      "Iteration  347 : Train Loss =  0.26061708   Train Acc:  0.90886664   Validation Loss =  0.34228212   Validation Acc:  0.8796\n",
      "Iteration  348 : Train Loss =  0.26012847   Train Acc:  0.90895   Validation Loss =  0.3422018   Validation Acc:  0.8798\n",
      "Iteration  349 : Train Loss =  0.2593848   Train Acc:  0.90926665   Validation Loss =  0.34137285   Validation Acc:  0.8795\n",
      "Iteration  350 : Train Loss =  0.25875598   Train Acc:  0.9094333   Validation Loss =  0.34112695   Validation Acc:  0.8796\n",
      "Iteration  351 : Train Loss =  0.25849777   Train Acc:  0.90991664   Validation Loss =  0.3409215   Validation Acc:  0.8787\n",
      "Iteration  352 : Train Loss =  0.25852   Train Acc:  0.9094   Validation Loss =  0.34121487   Validation Acc:  0.8793\n",
      "Iteration  353 : Train Loss =  0.2585059   Train Acc:  0.90966666   Validation Loss =  0.34134412   Validation Acc:  0.8796\n",
      "Iteration  354 : Train Loss =  0.25829   Train Acc:  0.9094167   Validation Loss =  0.34136665   Validation Acc:  0.8797\n",
      "Iteration  355 : Train Loss =  0.25780398   Train Acc:  0.9099   Validation Loss =  0.3409351   Validation Acc:  0.8791\n",
      "Iteration  356 : Train Loss =  0.25735506   Train Acc:  0.9098667   Validation Loss =  0.34086272   Validation Acc:  0.8794\n",
      "Iteration  357 : Train Loss =  0.2570462   Train Acc:  0.90991664   Validation Loss =  0.34039214   Validation Acc:  0.879\n",
      "Iteration  358 : Train Loss =  0.25698274   Train Acc:  0.91   Validation Loss =  0.34094438   Validation Acc:  0.8798\n",
      "Iteration  359 : Train Loss =  0.25705266   Train Acc:  0.91005   Validation Loss =  0.3406133   Validation Acc:  0.8797\n",
      "Iteration  360 : Train Loss =  0.25694346   Train Acc:  0.91031665   Validation Loss =  0.34132648   Validation Acc:  0.8794\n",
      "Iteration  361 : Train Loss =  0.25674292   Train Acc:  0.91015   Validation Loss =  0.3406122   Validation Acc:  0.8797\n",
      "Iteration  362 : Train Loss =  0.2562968   Train Acc:  0.91026664   Validation Loss =  0.34101707   Validation Acc:  0.879\n",
      "Iteration  363 : Train Loss =  0.25585994   Train Acc:  0.9104   Validation Loss =  0.3401633   Validation Acc:  0.8784\n",
      "Iteration  364 : Train Loss =  0.25562122   Train Acc:  0.91045   Validation Loss =  0.3406197   Validation Acc:  0.879\n",
      "Iteration  365 : Train Loss =  0.25526902   Train Acc:  0.91075   Validation Loss =  0.340067   Validation Acc:  0.8787\n",
      "Iteration  366 : Train Loss =  0.25498158   Train Acc:  0.9106167   Validation Loss =  0.34024823   Validation Acc:  0.8798\n",
      "Iteration  367 : Train Loss =  0.2543818   Train Acc:  0.91105   Validation Loss =  0.33960682   Validation Acc:  0.8787\n",
      "Iteration  368 : Train Loss =  0.2537633   Train Acc:  0.9109333   Validation Loss =  0.33932135   Validation Acc:  0.8802\n",
      "Iteration  369 : Train Loss =  0.25317508   Train Acc:  0.91186666   Validation Loss =  0.33877122   Validation Acc:  0.8796\n",
      "Iteration  370 : Train Loss =  0.25277954   Train Acc:  0.91181666   Validation Loss =  0.33866102   Validation Acc:  0.8805\n",
      "Iteration  371 : Train Loss =  0.25254506   Train Acc:  0.9116167   Validation Loss =  0.3384868   Validation Acc:  0.8807\n",
      "Iteration  372 : Train Loss =  0.2523701   Train Acc:  0.9123   Validation Loss =  0.3385594   Validation Acc:  0.8796\n",
      "Iteration  373 : Train Loss =  0.25216073   Train Acc:  0.91176665   Validation Loss =  0.33848614   Validation Acc:  0.8801\n",
      "Iteration  374 : Train Loss =  0.25188145   Train Acc:  0.91241664   Validation Loss =  0.33837852   Validation Acc:  0.8798\n",
      "Iteration  375 : Train Loss =  0.25158685   Train Acc:  0.91183335   Validation Loss =  0.3383652   Validation Acc:  0.8806\n",
      "Iteration  376 : Train Loss =  0.25131986   Train Acc:  0.9124   Validation Loss =  0.3380869   Validation Acc:  0.8794\n",
      "Iteration  377 : Train Loss =  0.25118077   Train Acc:  0.91225   Validation Loss =  0.3384234   Validation Acc:  0.8804\n",
      "Iteration  378 : Train Loss =  0.25115004   Train Acc:  0.91206664   Validation Loss =  0.33815053   Validation Acc:  0.8796\n",
      "Iteration  379 : Train Loss =  0.25124297   Train Acc:  0.91225   Validation Loss =  0.33897856   Validation Acc:  0.88\n",
      "Iteration  380 : Train Loss =  0.25142977   Train Acc:  0.9119833   Validation Loss =  0.33867052   Validation Acc:  0.88\n",
      "Iteration  381 : Train Loss =  0.25161895   Train Acc:  0.9120333   Validation Loss =  0.339819   Validation Acc:  0.8803\n",
      "Iteration  382 : Train Loss =  0.25172254   Train Acc:  0.91178334   Validation Loss =  0.3392458   Validation Acc:  0.8796\n",
      "Iteration  383 : Train Loss =  0.2517096   Train Acc:  0.91185   Validation Loss =  0.3402956   Validation Acc:  0.88\n",
      "Iteration  384 : Train Loss =  0.25108466   Train Acc:  0.9119667   Validation Loss =  0.33899584   Validation Acc:  0.8794\n",
      "Iteration  385 : Train Loss =  0.25046897   Train Acc:  0.91218334   Validation Loss =  0.33925527   Validation Acc:  0.8803\n",
      "Iteration  386 : Train Loss =  0.24934347   Train Acc:  0.91261667   Validation Loss =  0.33778408   Validation Acc:  0.8802\n",
      "Iteration  387 : Train Loss =  0.24853909   Train Acc:  0.91315   Validation Loss =  0.33740607   Validation Acc:  0.881\n",
      "Iteration  388 : Train Loss =  0.24796578   Train Acc:  0.9136   Validation Loss =  0.33700475   Validation Acc:  0.8803\n",
      "Iteration  389 : Train Loss =  0.24771383   Train Acc:  0.9137333   Validation Loss =  0.33672822   Validation Acc:  0.8799\n",
      "Iteration  390 : Train Loss =  0.24767914   Train Acc:  0.91361666   Validation Loss =  0.33725858   Validation Acc:  0.8811\n",
      "Iteration  391 : Train Loss =  0.24774618   Train Acc:  0.91333336   Validation Loss =  0.33702943   Validation Acc:  0.8807\n",
      "Iteration  392 : Train Loss =  0.24785905   Train Acc:  0.91323334   Validation Loss =  0.3378604   Validation Acc:  0.8807\n",
      "Iteration  393 : Train Loss =  0.24764046   Train Acc:  0.91323334   Validation Loss =  0.3372783   Validation Acc:  0.8802\n",
      "Iteration  394 : Train Loss =  0.24740483   Train Acc:  0.91335   Validation Loss =  0.3376988   Validation Acc:  0.8811\n",
      "Iteration  395 : Train Loss =  0.2467162   Train Acc:  0.91368335   Validation Loss =  0.3367848   Validation Acc:  0.8809\n",
      "Iteration  396 : Train Loss =  0.24607062   Train Acc:  0.9141333   Validation Loss =  0.33659026   Validation Acc:  0.8807\n",
      "Iteration  397 : Train Loss =  0.24542956   Train Acc:  0.9141833   Validation Loss =  0.3359826   Validation Acc:  0.8807\n",
      "Iteration  398 : Train Loss =  0.24499723   Train Acc:  0.9145   Validation Loss =  0.33574063   Validation Acc:  0.88\n",
      "Iteration  399 : Train Loss =  0.24477975   Train Acc:  0.91428334   Validation Loss =  0.33580795   Validation Acc:  0.881\n",
      "Iteration  400 : Train Loss =  0.24470788   Train Acc:  0.91445   Validation Loss =  0.3357235   Validation Acc:  0.88\n",
      "Iteration  401 : Train Loss =  0.24470824   Train Acc:  0.9144833   Validation Loss =  0.3361886   Validation Acc:  0.8809\n",
      "Iteration  402 : Train Loss =  0.24460915   Train Acc:  0.9145833   Validation Loss =  0.33593786   Validation Acc:  0.8807\n",
      "Iteration  403 : Train Loss =  0.24449313   Train Acc:  0.91438335   Validation Loss =  0.33636874   Validation Acc:  0.8813\n",
      "Iteration  404 : Train Loss =  0.2441432   Train Acc:  0.9148   Validation Loss =  0.3358048   Validation Acc:  0.8808\n",
      "Iteration  405 : Train Loss =  0.24378906   Train Acc:  0.9148167   Validation Loss =  0.33599672   Validation Acc:  0.8815\n",
      "Iteration  406 : Train Loss =  0.24330547   Train Acc:  0.9151833   Validation Loss =  0.335326   Validation Acc:  0.8802\n",
      "Iteration  407 : Train Loss =  0.24287301   Train Acc:  0.91531664   Validation Loss =  0.33538485   Validation Acc:  0.8814\n",
      "Iteration  408 : Train Loss =  0.24248299   Train Acc:  0.9157   Validation Loss =  0.33490336   Validation Acc:  0.8803\n",
      "Iteration  409 : Train Loss =  0.24216464   Train Acc:  0.9155   Validation Loss =  0.33498836   Validation Acc:  0.8816\n",
      "Iteration  410 : Train Loss =  0.24190733   Train Acc:  0.9156833   Validation Loss =  0.33475512   Validation Acc:  0.8808\n",
      "Iteration  411 : Train Loss =  0.24168132   Train Acc:  0.91575   Validation Loss =  0.3348407   Validation Acc:  0.8809\n",
      "Iteration  412 : Train Loss =  0.24146596   Train Acc:  0.9159167   Validation Loss =  0.33475173   Validation Acc:  0.8809\n",
      "Iteration  413 : Train Loss =  0.24124268   Train Acc:  0.91581666   Validation Loss =  0.3347337   Validation Acc:  0.8811\n",
      "Iteration  414 : Train Loss =  0.24100943   Train Acc:  0.9158667   Validation Loss =  0.33471444   Validation Acc:  0.8807\n",
      "Iteration  415 : Train Loss =  0.2407519   Train Acc:  0.91605   Validation Loss =  0.33458617   Validation Acc:  0.8814\n",
      "Iteration  416 : Train Loss =  0.24049827   Train Acc:  0.91613334   Validation Loss =  0.3346156   Validation Acc:  0.8813\n",
      "Iteration  417 : Train Loss =  0.24022858   Train Acc:  0.9163   Validation Loss =  0.3343798   Validation Acc:  0.8814\n",
      "Iteration  418 : Train Loss =  0.23999268   Train Acc:  0.9160333   Validation Loss =  0.33451268   Validation Acc:  0.8819\n",
      "Iteration  419 : Train Loss =  0.2397652   Train Acc:  0.91665   Validation Loss =  0.33421358   Validation Acc:  0.8808\n",
      "Iteration  420 : Train Loss =  0.23961739   Train Acc:  0.91623336   Validation Loss =  0.3345698   Validation Acc:  0.8822\n",
      "Iteration  421 : Train Loss =  0.2395332   Train Acc:  0.9169667   Validation Loss =  0.33426288   Validation Acc:  0.8811\n",
      "Iteration  422 : Train Loss =  0.23960741   Train Acc:  0.91623336   Validation Loss =  0.33505085   Validation Acc:  0.8819\n",
      "Iteration  423 : Train Loss =  0.23987454   Train Acc:  0.91646665   Validation Loss =  0.33484498   Validation Acc:  0.8806\n",
      "Iteration  424 : Train Loss =  0.24032153   Train Acc:  0.9159667   Validation Loss =  0.33630183   Validation Acc:  0.882\n",
      "Iteration  425 : Train Loss =  0.24105434   Train Acc:  0.9157   Validation Loss =  0.33621776   Validation Acc:  0.8803\n",
      "Iteration  426 : Train Loss =  0.24134623   Train Acc:  0.9152   Validation Loss =  0.33782825   Validation Acc:  0.8808\n",
      "Iteration  427 : Train Loss =  0.24134375   Train Acc:  0.91533333   Validation Loss =  0.33679825   Validation Acc:  0.8804\n",
      "Iteration  428 : Train Loss =  0.23984975   Train Acc:  0.9159167   Validation Loss =  0.33657274   Validation Acc:  0.8821\n",
      "Iteration  429 : Train Loss =  0.23809034   Train Acc:  0.9173167   Validation Loss =  0.33412856   Validation Acc:  0.8812\n",
      "Iteration  430 : Train Loss =  0.236767   Train Acc:  0.9173   Validation Loss =  0.33341816   Validation Acc:  0.8827\n",
      "Iteration  431 : Train Loss =  0.23660381   Train Acc:  0.9173333   Validation Loss =  0.3334734   Validation Acc:  0.8828\n",
      "Iteration  432 : Train Loss =  0.23725693   Train Acc:  0.91761667   Validation Loss =  0.33388072   Validation Acc:  0.8816\n",
      "Iteration  433 : Train Loss =  0.23775029   Train Acc:  0.91676664   Validation Loss =  0.3352851   Validation Acc:  0.8826\n",
      "Iteration  434 : Train Loss =  0.23779337   Train Acc:  0.91725   Validation Loss =  0.33464387   Validation Acc:  0.8816\n",
      "Iteration  435 : Train Loss =  0.23682462   Train Acc:  0.91711664   Validation Loss =  0.33465847   Validation Acc:  0.8825\n",
      "Iteration  436 : Train Loss =  0.23572998   Train Acc:  0.91838336   Validation Loss =  0.333121   Validation Acc:  0.8817\n",
      "Iteration  437 : Train Loss =  0.23504528   Train Acc:  0.91785   Validation Loss =  0.33292052   Validation Acc:  0.8824\n",
      "Iteration  438 : Train Loss =  0.23500748   Train Acc:  0.9181833   Validation Loss =  0.33311358   Validation Acc:  0.883\n",
      "Iteration  439 : Train Loss =  0.23532723   Train Acc:  0.91805   Validation Loss =  0.33330265   Validation Acc:  0.882\n",
      "Iteration  440 : Train Loss =  0.23541626   Train Acc:  0.91805   Validation Loss =  0.33405   Validation Acc:  0.8832\n",
      "Iteration  441 : Train Loss =  0.23515251   Train Acc:  0.9181   Validation Loss =  0.33342943   Validation Acc:  0.8826\n",
      "Iteration  442 : Train Loss =  0.2343948   Train Acc:  0.91845   Validation Loss =  0.33329204   Validation Acc:  0.8833\n",
      "Iteration  443 : Train Loss =  0.23368159   Train Acc:  0.9185   Validation Loss =  0.33248246   Validation Acc:  0.8829\n",
      "Iteration  444 : Train Loss =  0.23328765   Train Acc:  0.9192   Validation Loss =  0.33229297   Validation Acc:  0.8827\n",
      "Iteration  445 : Train Loss =  0.23325923   Train Acc:  0.9184167   Validation Loss =  0.33267418   Validation Acc:  0.883\n",
      "Iteration  446 : Train Loss =  0.23335423   Train Acc:  0.91938335   Validation Loss =  0.33252928   Validation Acc:  0.8822\n",
      "Iteration  447 : Train Loss =  0.23328851   Train Acc:  0.91831666   Validation Loss =  0.33317482   Validation Acc:  0.8829\n",
      "Iteration  448 : Train Loss =  0.23299426   Train Acc:  0.91965   Validation Loss =  0.33251417   Validation Acc:  0.882\n",
      "Iteration  449 : Train Loss =  0.23254281   Train Acc:  0.91896665   Validation Loss =  0.33271813   Validation Acc:  0.883\n",
      "Iteration  450 : Train Loss =  0.23213197   Train Acc:  0.91966665   Validation Loss =  0.33213022   Validation Acc:  0.8822\n",
      "Iteration  451 : Train Loss =  0.23196533   Train Acc:  0.9188667   Validation Loss =  0.33234823   Validation Acc:  0.882\n",
      "Iteration  452 : Train Loss =  0.23199175   Train Acc:  0.9194   Validation Loss =  0.3324918   Validation Acc:  0.8831\n",
      "Iteration  453 : Train Loss =  0.23217529   Train Acc:  0.91855   Validation Loss =  0.33282545   Validation Acc:  0.8817\n",
      "Iteration  454 : Train Loss =  0.23234493   Train Acc:  0.91935   Validation Loss =  0.3332674   Validation Acc:  0.8834\n",
      "Iteration  455 : Train Loss =  0.23245902   Train Acc:  0.91831666   Validation Loss =  0.3335008   Validation Acc:  0.8817\n",
      "Iteration  456 : Train Loss =  0.23237985   Train Acc:  0.9191833   Validation Loss =  0.33353627   Validation Acc:  0.8831\n",
      "Iteration  457 : Train Loss =  0.23238021   Train Acc:  0.9185   Validation Loss =  0.33389398   Validation Acc:  0.882\n",
      "Iteration  458 : Train Loss =  0.2319909   Train Acc:  0.91906667   Validation Loss =  0.33327   Validation Acc:  0.8844\n",
      "Iteration  459 : Train Loss =  0.23195773   Train Acc:  0.9185167   Validation Loss =  0.33396393   Validation Acc:  0.8822\n",
      "Iteration  460 : Train Loss =  0.23128143   Train Acc:  0.91935   Validation Loss =  0.33276552   Validation Acc:  0.8829\n",
      "Iteration  461 : Train Loss =  0.23070146   Train Acc:  0.9194   Validation Loss =  0.33307764   Validation Acc:  0.8833\n",
      "Iteration  462 : Train Loss =  0.22980246   Train Acc:  0.9205833   Validation Loss =  0.33170938   Validation Acc:  0.8831\n",
      "Iteration  463 : Train Loss =  0.22898819   Train Acc:  0.9202167   Validation Loss =  0.3315716   Validation Acc:  0.8843\n",
      "Iteration  464 : Train Loss =  0.22849263   Train Acc:  0.9205   Validation Loss =  0.33103228   Validation Acc:  0.883\n",
      "Iteration  465 : Train Loss =  0.22838001   Train Acc:  0.9209833   Validation Loss =  0.33109468   Validation Acc:  0.8838\n",
      "Iteration  466 : Train Loss =  0.22856644   Train Acc:  0.92018336   Validation Loss =  0.33174017   Validation Acc:  0.8834\n",
      "Iteration  467 : Train Loss =  0.22875933   Train Acc:  0.9209   Validation Loss =  0.33161485   Validation Acc:  0.8831\n",
      "Iteration  468 : Train Loss =  0.2289367   Train Acc:  0.92018336   Validation Loss =  0.3325626   Validation Acc:  0.8839\n",
      "Iteration  469 : Train Loss =  0.22866216   Train Acc:  0.92073333   Validation Loss =  0.3317797   Validation Acc:  0.883\n",
      "Iteration  470 : Train Loss =  0.22829351   Train Acc:  0.92038333   Validation Loss =  0.33220923   Validation Acc:  0.8835\n",
      "Iteration  471 : Train Loss =  0.22758527   Train Acc:  0.92115   Validation Loss =  0.33116326   Validation Acc:  0.884\n",
      "Iteration  472 : Train Loss =  0.22699666   Train Acc:  0.9209   Validation Loss =  0.33109713   Validation Acc:  0.8837\n",
      "Iteration  473 : Train Loss =  0.22651611   Train Acc:  0.9219   Validation Loss =  0.33065522   Validation Acc:  0.8838\n",
      "Iteration  474 : Train Loss =  0.22622655   Train Acc:  0.9216167   Validation Loss =  0.33049357   Validation Acc:  0.8825\n",
      "Iteration  475 : Train Loss =  0.22605927   Train Acc:  0.92188334   Validation Loss =  0.33069977   Validation Acc:  0.8846\n",
      "Iteration  476 : Train Loss =  0.22595216   Train Acc:  0.92218333   Validation Loss =  0.33042368   Validation Acc:  0.8832\n",
      "Iteration  477 : Train Loss =  0.2258341   Train Acc:  0.9216667   Validation Loss =  0.33086908   Validation Acc:  0.8832\n",
      "Iteration  478 : Train Loss =  0.22567445   Train Acc:  0.92225   Validation Loss =  0.33044595   Validation Acc:  0.8831\n",
      "Iteration  479 : Train Loss =  0.22551614   Train Acc:  0.9216833   Validation Loss =  0.33088246   Validation Acc:  0.8836\n",
      "Iteration  480 : Train Loss =  0.22529414   Train Acc:  0.9223   Validation Loss =  0.33049443   Validation Acc:  0.8841\n",
      "Iteration  481 : Train Loss =  0.22514081   Train Acc:  0.92145   Validation Loss =  0.3308344   Validation Acc:  0.8828\n",
      "Iteration  482 : Train Loss =  0.22493434   Train Acc:  0.9223833   Validation Loss =  0.33061484   Validation Acc:  0.885\n",
      "Iteration  483 : Train Loss =  0.22476731   Train Acc:  0.92145   Validation Loss =  0.33075732   Validation Acc:  0.8834\n",
      "Iteration  484 : Train Loss =  0.22453228   Train Acc:  0.9225   Validation Loss =  0.33064067   Validation Acc:  0.885\n",
      "Iteration  485 : Train Loss =  0.22426957   Train Acc:  0.92175   Validation Loss =  0.33053857   Validation Acc:  0.8838\n",
      "Iteration  486 : Train Loss =  0.22393985   Train Acc:  0.92288333   Validation Loss =  0.33041924   Validation Acc:  0.8847\n",
      "Iteration  487 : Train Loss =  0.2235785   Train Acc:  0.92211664   Validation Loss =  0.3301516   Validation Acc:  0.8839\n",
      "Iteration  488 : Train Loss =  0.22319616   Train Acc:  0.92323333   Validation Loss =  0.3300181   Validation Acc:  0.8844\n",
      "Iteration  489 : Train Loss =  0.22283126   Train Acc:  0.92275   Validation Loss =  0.32976675   Validation Acc:  0.8841\n",
      "Iteration  490 : Train Loss =  0.22250131   Train Acc:  0.9233   Validation Loss =  0.32967597   Validation Acc:  0.8846\n",
      "Iteration  491 : Train Loss =  0.22221562   Train Acc:  0.9231333   Validation Loss =  0.32954058   Validation Acc:  0.8835\n",
      "Iteration  492 : Train Loss =  0.22197068   Train Acc:  0.9235   Validation Loss =  0.3294777   Validation Acc:  0.8837\n",
      "Iteration  493 : Train Loss =  0.22175887   Train Acc:  0.9234833   Validation Loss =  0.32945058   Validation Acc:  0.8842\n",
      "Iteration  494 : Train Loss =  0.22157136   Train Acc:  0.9234333   Validation Loss =  0.32941124   Validation Acc:  0.8833\n",
      "Iteration  495 : Train Loss =  0.22140366   Train Acc:  0.9238333   Validation Loss =  0.32946005   Validation Acc:  0.8845\n",
      "Iteration  496 : Train Loss =  0.22125609   Train Acc:  0.92333335   Validation Loss =  0.3294292   Validation Acc:  0.884\n",
      "Iteration  497 : Train Loss =  0.22112699   Train Acc:  0.9237667   Validation Loss =  0.32954204   Validation Acc:  0.8848\n",
      "Iteration  498 : Train Loss =  0.221027   Train Acc:  0.9231667   Validation Loss =  0.32956165   Validation Acc:  0.8847\n",
      "Iteration  499 : Train Loss =  0.22095872   Train Acc:  0.9237667   Validation Loss =  0.32971826   Validation Acc:  0.885\n",
      "Iteration  500 : Train Loss =  0.22094366   Train Acc:  0.9227167   Validation Loss =  0.32984978   Validation Acc:  0.8841\n",
      "Iteration  501 : Train Loss =  0.22097327   Train Acc:  0.92371666   Validation Loss =  0.33005142   Validation Acc:  0.8845\n",
      "Iteration  502 : Train Loss =  0.22110292   Train Acc:  0.9225   Validation Loss =  0.33041957   Validation Acc:  0.8834\n",
      "Iteration  503 : Train Loss =  0.22122447   Train Acc:  0.9234167   Validation Loss =  0.33057043   Validation Acc:  0.8857\n",
      "Iteration  504 : Train Loss =  0.22153592   Train Acc:  0.9225   Validation Loss =  0.33132267   Validation Acc:  0.8826\n",
      "Iteration  505 : Train Loss =  0.2215778   Train Acc:  0.9229   Validation Loss =  0.33112314   Validation Acc:  0.8858\n",
      "Iteration  506 : Train Loss =  0.22206765   Train Acc:  0.9220833   Validation Loss =  0.33239502   Validation Acc:  0.8825\n",
      "Iteration  507 : Train Loss =  0.2218344   Train Acc:  0.92223334   Validation Loss =  0.33148834   Validation Acc:  0.8848\n",
      "Iteration  508 : Train Loss =  0.22241065   Train Acc:  0.922   Validation Loss =  0.33329645   Validation Acc:  0.8829\n",
      "Iteration  509 : Train Loss =  0.22231524   Train Acc:  0.92155   Validation Loss =  0.3320917   Validation Acc:  0.8845\n",
      "Iteration  510 : Train Loss =  0.22217976   Train Acc:  0.9224167   Validation Loss =  0.3335248   Validation Acc:  0.8829\n",
      "Iteration  511 : Train Loss =  0.22185464   Train Acc:  0.92256665   Validation Loss =  0.33202422   Validation Acc:  0.8841\n",
      "Iteration  512 : Train Loss =  0.22007838   Train Acc:  0.9235   Validation Loss =  0.33156568   Validation Acc:  0.8845\n",
      "Iteration  513 : Train Loss =  0.21865176   Train Acc:  0.92355   Validation Loss =  0.32967696   Validation Acc:  0.8848\n",
      "Iteration  514 : Train Loss =  0.21793272   Train Acc:  0.92466664   Validation Loss =  0.3293208   Validation Acc:  0.8852\n",
      "Iteration  515 : Train Loss =  0.21839258   Train Acc:  0.9239333   Validation Loss =  0.33039808   Validation Acc:  0.8841\n",
      "Iteration  516 : Train Loss =  0.21918823   Train Acc:  0.9234   Validation Loss =  0.33044428   Validation Acc:  0.8851\n",
      "Iteration  517 : Train Loss =  0.21958785   Train Acc:  0.9234167   Validation Loss =  0.33212066   Validation Acc:  0.8837\n",
      "Iteration  518 : Train Loss =  0.21916585   Train Acc:  0.92355   Validation Loss =  0.33060727   Validation Acc:  0.8855\n",
      "Iteration  519 : Train Loss =  0.21752985   Train Acc:  0.92465   Validation Loss =  0.3301043   Validation Acc:  0.8839\n",
      "Iteration  520 : Train Loss =  0.21616295   Train Acc:  0.92525   Validation Loss =  0.3283423   Validation Acc:  0.8839\n",
      "Iteration  521 : Train Loss =  0.21576901   Train Acc:  0.9256833   Validation Loss =  0.328247   Validation Acc:  0.8844\n",
      "Iteration  522 : Train Loss =  0.21632786   Train Acc:  0.9249333   Validation Loss =  0.3294479   Validation Acc:  0.8836\n",
      "Iteration  523 : Train Loss =  0.21701145   Train Acc:  0.92455   Validation Loss =  0.32952926   Validation Acc:  0.8856\n",
      "Iteration  524 : Train Loss =  0.21691777   Train Acc:  0.9247   Validation Loss =  0.33051458   Validation Acc:  0.8836\n",
      "Iteration  525 : Train Loss =  0.21613818   Train Acc:  0.92516667   Validation Loss =  0.3290043   Validation Acc:  0.8852\n",
      "Iteration  526 : Train Loss =  0.21509847   Train Acc:  0.92541665   Validation Loss =  0.32872206   Validation Acc:  0.8844\n",
      "Iteration  527 : Train Loss =  0.21448822   Train Acc:  0.92635   Validation Loss =  0.32799813   Validation Acc:  0.8851\n",
      "Iteration  528 : Train Loss =  0.21448246   Train Acc:  0.92578334   Validation Loss =  0.32806915   Validation Acc:  0.8842\n",
      "Iteration  529 : Train Loss =  0.21468659   Train Acc:  0.92578334   Validation Loss =  0.32890806   Validation Acc:  0.885\n",
      "Iteration  530 : Train Loss =  0.21473636   Train Acc:  0.9257333   Validation Loss =  0.32854706   Validation Acc:  0.8853\n",
      "Iteration  531 : Train Loss =  0.21431248   Train Acc:  0.92571664   Validation Loss =  0.3289537   Validation Acc:  0.8838\n",
      "Iteration  532 : Train Loss =  0.21375363   Train Acc:  0.92635   Validation Loss =  0.32806876   Validation Acc:  0.8853\n",
      "Iteration  533 : Train Loss =  0.21332924   Train Acc:  0.9260333   Validation Loss =  0.32812038   Validation Acc:  0.8845\n",
      "Iteration  534 : Train Loss =  0.21316664   Train Acc:  0.9266833   Validation Loss =  0.32801944   Validation Acc:  0.8859\n",
      "Iteration  535 : Train Loss =  0.2131734   Train Acc:  0.92585   Validation Loss =  0.32803237   Validation Acc:  0.8846\n",
      "Iteration  536 : Train Loss =  0.21310629   Train Acc:  0.9266   Validation Loss =  0.32842654   Validation Acc:  0.8854\n",
      "Iteration  537 : Train Loss =  0.21287937   Train Acc:  0.92615   Validation Loss =  0.32801852   Validation Acc:  0.885\n",
      "Iteration  538 : Train Loss =  0.21245332   Train Acc:  0.9268   Validation Loss =  0.32812625   Validation Acc:  0.8859\n",
      "Iteration  539 : Train Loss =  0.21202295   Train Acc:  0.92688334   Validation Loss =  0.32761922   Validation Acc:  0.8847\n",
      "Iteration  540 : Train Loss =  0.21169315   Train Acc:  0.9271167   Validation Loss =  0.3275899   Validation Acc:  0.8844\n",
      "Iteration  541 : Train Loss =  0.2115138   Train Acc:  0.92716664   Validation Loss =  0.32758883   Validation Acc:  0.8849\n",
      "Iteration  542 : Train Loss =  0.21143015   Train Acc:  0.92716664   Validation Loss =  0.32752323   Validation Acc:  0.8849\n",
      "Iteration  543 : Train Loss =  0.21134496   Train Acc:  0.9273667   Validation Loss =  0.3278315   Validation Acc:  0.8859\n",
      "Iteration  544 : Train Loss =  0.21120387   Train Acc:  0.92723334   Validation Loss =  0.32759923   Validation Acc:  0.8848\n",
      "Iteration  545 : Train Loss =  0.21097134   Train Acc:  0.92758334   Validation Loss =  0.32779223   Validation Acc:  0.8856\n",
      "Iteration  546 : Train Loss =  0.21069336   Train Acc:  0.92715   Validation Loss =  0.3274958   Validation Acc:  0.8847\n",
      "Iteration  547 : Train Loss =  0.21041253   Train Acc:  0.9279   Validation Loss =  0.32748798   Validation Acc:  0.8855\n",
      "Iteration  548 : Train Loss =  0.21018155   Train Acc:  0.92726666   Validation Loss =  0.32744396   Validation Acc:  0.8847\n",
      "Iteration  549 : Train Loss =  0.2100198   Train Acc:  0.928   Validation Loss =  0.3273297   Validation Acc:  0.8854\n",
      "Iteration  550 : Train Loss =  0.20994596   Train Acc:  0.9271167   Validation Loss =  0.327707   Validation Acc:  0.8842\n",
      "Iteration  551 : Train Loss =  0.20991792   Train Acc:  0.92775   Validation Loss =  0.32748473   Validation Acc:  0.8852\n",
      "Iteration  552 : Train Loss =  0.20998137   Train Acc:  0.92691666   Validation Loss =  0.3282152   Validation Acc:  0.8843\n",
      "Iteration  553 : Train Loss =  0.21002536   Train Acc:  0.92733335   Validation Loss =  0.3278728   Validation Acc:  0.8857\n",
      "Iteration  554 : Train Loss =  0.21023771   Train Acc:  0.9269   Validation Loss =  0.3289196   Validation Acc:  0.8843\n",
      "Iteration  555 : Train Loss =  0.21031173   Train Acc:  0.92718333   Validation Loss =  0.32848886   Validation Acc:  0.8865\n",
      "Iteration  556 : Train Loss =  0.21072492   Train Acc:  0.92648333   Validation Loss =  0.32981646   Validation Acc:  0.8832\n",
      "Iteration  557 : Train Loss =  0.21068723   Train Acc:  0.9270167   Validation Loss =  0.3292558   Validation Acc:  0.8862\n",
      "Iteration  558 : Train Loss =  0.21101993   Train Acc:  0.92623335   Validation Loss =  0.33044103   Validation Acc:  0.8836\n",
      "Iteration  559 : Train Loss =  0.21047926   Train Acc:  0.9271   Validation Loss =  0.32948065   Validation Acc:  0.8867\n",
      "Iteration  560 : Train Loss =  0.21002068   Train Acc:  0.9265   Validation Loss =  0.32963192   Validation Acc:  0.8838\n",
      "Iteration  561 : Train Loss =  0.20894502   Train Acc:  0.9285333   Validation Loss =  0.32841003   Validation Acc:  0.8864\n",
      "Iteration  562 : Train Loss =  0.2079831   Train Acc:  0.92751664   Validation Loss =  0.32771215   Validation Acc:  0.8847\n",
      "Iteration  563 : Train Loss =  0.20726222   Train Acc:  0.92885   Validation Loss =  0.32721943   Validation Acc:  0.8856\n",
      "Iteration  564 : Train Loss =  0.206974   Train Acc:  0.9285833   Validation Loss =  0.32688662   Validation Acc:  0.8853\n",
      "Iteration  565 : Train Loss =  0.207068   Train Acc:  0.9281333   Validation Loss =  0.3275297   Validation Acc:  0.8844\n",
      "Iteration  566 : Train Loss =  0.20734648   Train Acc:  0.9284   Validation Loss =  0.32753614   Validation Acc:  0.8863\n",
      "Iteration  567 : Train Loss =  0.20778985   Train Acc:  0.92761666   Validation Loss =  0.32870468   Validation Acc:  0.8839\n",
      "Iteration  568 : Train Loss =  0.20782194   Train Acc:  0.9284667   Validation Loss =  0.32831547   Validation Acc:  0.8862\n",
      "Iteration  569 : Train Loss =  0.20785892   Train Acc:  0.92731667   Validation Loss =  0.32911524   Validation Acc:  0.8838\n",
      "Iteration  570 : Train Loss =  0.2071799   Train Acc:  0.92876667   Validation Loss =  0.3280164   Validation Acc:  0.8862\n",
      "Iteration  571 : Train Loss =  0.20650977   Train Acc:  0.9278167   Validation Loss =  0.3279969   Validation Acc:  0.8842\n",
      "Iteration  572 : Train Loss =  0.20567822   Train Acc:  0.92915   Validation Loss =  0.32692704   Validation Acc:  0.8859\n",
      "Iteration  573 : Train Loss =  0.20511286   Train Acc:  0.92906666   Validation Loss =  0.3267904   Validation Acc:  0.8848\n",
      "Iteration  574 : Train Loss =  0.20484887   Train Acc:  0.9293   Validation Loss =  0.32659712   Validation Acc:  0.8848\n",
      "Iteration  575 : Train Loss =  0.2048343   Train Acc:  0.92988336   Validation Loss =  0.3267719   Validation Acc:  0.8856\n",
      "Iteration  576 : Train Loss =  0.20493865   Train Acc:  0.92873335   Validation Loss =  0.3271651   Validation Acc:  0.8852\n",
      "Iteration  577 : Train Loss =  0.20498998   Train Acc:  0.92965   Validation Loss =  0.3271789   Validation Acc:  0.8862\n",
      "Iteration  578 : Train Loss =  0.20498788   Train Acc:  0.9282   Validation Loss =  0.32761797   Validation Acc:  0.8838\n",
      "Iteration  579 : Train Loss =  0.20474778   Train Acc:  0.92966664   Validation Loss =  0.32719165   Validation Acc:  0.8865\n",
      "Iteration  580 : Train Loss =  0.20447835   Train Acc:  0.92841667   Validation Loss =  0.32746893   Validation Acc:  0.8842\n",
      "Iteration  581 : Train Loss =  0.20405583   Train Acc:  0.92953336   Validation Loss =  0.32679456   Validation Acc:  0.8866\n",
      "Iteration  582 : Train Loss =  0.20372273   Train Acc:  0.9293   Validation Loss =  0.32707053   Validation Acc:  0.8848\n",
      "Iteration  583 : Train Loss =  0.20345722   Train Acc:  0.9295667   Validation Loss =  0.32650122   Validation Acc:  0.8863\n",
      "Iteration  584 : Train Loss =  0.20334233   Train Acc:  0.92988336   Validation Loss =  0.32703412   Validation Acc:  0.8854\n",
      "Iteration  585 : Train Loss =  0.20341599   Train Acc:  0.9293333   Validation Loss =  0.3267341   Validation Acc:  0.8859\n",
      "Iteration  586 : Train Loss =  0.20353702   Train Acc:  0.9299333   Validation Loss =  0.32763588   Validation Acc:  0.8853\n",
      "Iteration  587 : Train Loss =  0.20385651   Train Acc:  0.92925   Validation Loss =  0.32743818   Validation Acc:  0.8864\n",
      "Iteration  588 : Train Loss =  0.20395887   Train Acc:  0.9298   Validation Loss =  0.32849234   Validation Acc:  0.8852\n",
      "Iteration  589 : Train Loss =  0.20418793   Train Acc:  0.92873335   Validation Loss =  0.3280219   Validation Acc:  0.886\n",
      "Iteration  590 : Train Loss =  0.20378035   Train Acc:  0.93006665   Validation Loss =  0.32869735   Validation Acc:  0.8853\n",
      "Iteration  591 : Train Loss =  0.20339555   Train Acc:  0.9291667   Validation Loss =  0.32754636   Validation Acc:  0.8867\n",
      "Iteration  592 : Train Loss =  0.20250902   Train Acc:  0.9303167   Validation Loss =  0.32767406   Validation Acc:  0.8847\n",
      "Iteration  593 : Train Loss =  0.20185204   Train Acc:  0.93021667   Validation Loss =  0.32646623   Validation Acc:  0.8864\n",
      "Iteration  594 : Train Loss =  0.20145683   Train Acc:  0.9300333   Validation Loss =  0.32682043   Validation Acc:  0.8846\n",
      "Iteration  595 : Train Loss =  0.20134324   Train Acc:  0.9310167   Validation Loss =  0.32652304   Validation Acc:  0.8864\n",
      "Iteration  596 : Train Loss =  0.2014502   Train Acc:  0.92955   Validation Loss =  0.32704663   Validation Acc:  0.8854\n",
      "Iteration  597 : Train Loss =  0.2014862   Train Acc:  0.9314833   Validation Loss =  0.32716414   Validation Acc:  0.8865\n",
      "Iteration  598 : Train Loss =  0.20135951   Train Acc:  0.9296167   Validation Loss =  0.3271928   Validation Acc:  0.8854\n",
      "Iteration  599 : Train Loss =  0.20097458   Train Acc:  0.93163335   Validation Loss =  0.32703897   Validation Acc:  0.8868\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    num_Iterations = 600\n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    validation_accuracy = []\n",
    "    validation_loss = []\n",
    "\n",
    "    # load and prepare the training and test data\n",
    "    tr_x, tr_y, te_x, te_y = load_Prepare_Data()\n",
    "    \n",
    "    # To avoid problems related to type conversion, all data is converted to  float32 data types\n",
    "    tr_x = tf.cast(tr_x, tf.float32)\n",
    "    te_x = tf.cast(te_x, tf.float32)\n",
    "    tr_y = tf.cast(tr_y, tf.float32)\n",
    "    te_y = tf.cast(te_y, tf.float32)\n",
    "    \n",
    "    #Initialize the values of the weights and bias for the 1st hidden layer\n",
    "    w1 = tf.Variable(tf.random.normal([ 300, tr_x.shape[0]], mean=0.0, stddev=0.05))\n",
    "    b1 = tf.Variable(tf.zeros([300, 1]))\n",
    "    #Initialize the values of the weights and bias for the SoftMax layer\n",
    "    w2 = tf.Variable(tf.random.normal([ tr_y.shape[0], 300], mean=0.0, stddev=0.05))\n",
    "    b2 = tf.Variable(tf.zeros([tr_y.shape[0], 1]))\n",
    "    \n",
    "    # Iterate the training loop\n",
    "    for i in range(num_Iterations):\n",
    "        \n",
    "        # Create an instance of Gradient Tape to monitor the forward pass to calcualte the gradients based on the training data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = forward_pass(tr_x, w1, b1, w2, b2)\n",
    "            currentLoss = cross_entropy(tr_y, y_pred)\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        gradients = tape.gradient(currentLoss, [w1, b1, w2, b2])\n",
    "        # Determine the prediction accuracy for the training data\n",
    "        accuracy = calculate_accuracy(tr_y, y_pred)\n",
    "\n",
    "        train_accuracy.append(accuracy)\n",
    "        train_loss.append(currentLoss)\n",
    "\n",
    "        # Calculate forward pass, loss and accuracy for the valdation data\n",
    "        te_y_pred = forward_pass(te_x, w1, b1, w2, b2) \n",
    "        te_currentLoss = cross_entropy(te_y, te_y_pred)\n",
    "        te_accuracy = calculate_accuracy(te_y, te_y_pred) \n",
    "        validation_accuracy.append(te_accuracy)\n",
    "        validation_loss.append(te_currentLoss)\n",
    "\n",
    "        print (\"Iteration \", i, \": Train Loss = \",currentLoss.numpy(), \"  Train Acc: \", accuracy.numpy(), \"  Validation Loss = \", te_currentLoss.numpy(), \"  Validation Acc: \", te_accuracy.numpy())\n",
    "        # Update the trainable parameters using Adam Optimizer\n",
    "        adam_optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2]))\n",
    "        \n",
    "    # Plot the training and the validation accuracy and loss\n",
    "    plt.plot(train_accuracy, label=\"Train Acc\")\n",
    "    plt.plot(train_loss, label=\"Train Loss\")\n",
    "    plt.plot(validation_accuracy, label=\"Validation Acc\")\n",
    "    plt.plot(validation_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"No. of Iterations\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show();\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Question1_2_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
