{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RhHMm04wZhYC"
   },
   "source": [
    "# R00182510 - Assignment1 PART A - Task 2 (ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WSLCI4UupncQ",
    "outputId": "1ddf1bb2-bd0e-4ad1-ea01-3c5dc12bdfdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "# To enable TF2\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y3JBeLDeppg5",
    "outputId": "c86c2067-d22b-4aae-9d6c-9eefbf82ee9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "random.seed(182510)\n",
    "\n",
    "# load and prepare the training and test data\n",
    "def load_Prepare_Data():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "    # load the training and test data    \n",
    "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "    # reshape the feature data\n",
    "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "    # noramlise feature data\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "\n",
    "    print( \"Shape of training features \", tr_x.shape)\n",
    "    print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "    # one hot encode the training labels and get the transpose\n",
    "    tr_y = np_utils.to_categorical(tr_y,10)\n",
    "    tr_y = tr_y.T\n",
    "    print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "    # one hot encode the test labels and get the transpose\n",
    "    te_y = np_utils.to_categorical(te_y,10)\n",
    "    te_y = te_y.T\n",
    "    print (\"Shape of testing labels \", te_y.shape)\n",
    "    \n",
    "    # Reshape the training data and test data so \n",
    "    # that the features becomes the rows of the matrix\n",
    "    tr_x = tr_x.T\n",
    "    te_x = te_x.T\n",
    "\n",
    "    print(\"Reshaped training data \", tr_x.shape)\n",
    "    print(\"Reshaped test data \",te_x.shape)\n",
    "    \n",
    "    return(tr_x, tr_y, te_x, te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xy06wmFTeK7F"
   },
   "outputs": [],
   "source": [
    "# push all training data through the hidden layers and the SoftMax layer\n",
    "def forward_pass(x, w_T1, b1, w_T2, b2, w_T3, b3):\n",
    "\n",
    "    # Multiply each training example by the weights and add the bias for the 1st hidden layer\n",
    "    A1 = tf.matmul(w_T1, x) + b1\n",
    "    H1 = tf.nn.relu(A1)\n",
    "    \n",
    "    # Multiply each training example by the weights and add the bias for the 2nd hidden layer\n",
    "    A2 = tf.matmul(w_T2, H1) + b2\n",
    "    H2 = tf.nn.relu(A2)\n",
    "\n",
    "    #SoftMax activation\n",
    "    A3 = tf.matmul(w_T3, H2) + b3\n",
    "    t = tf.math.exp(A3)\n",
    "    t_sum = tf.reduce_sum(t, 0)\n",
    "    t_sum = tf.reshape(t_sum,[1, -1])\n",
    "\n",
    "    y_pred_softmax = tf.divide(t, t_sum)\n",
    "\n",
    "    return y_pred_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxUFdp7WeQKZ"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_pred):\n",
    "\n",
    "   # Calculate the cross entropy error for all training data \n",
    "    cross_entropy_loss = -(tf.reduce_sum(tf.multiply(y, tf.math.log(y_pred)), 0))\n",
    "    \n",
    "    # Calculate cost which is the mean cross entropy error/ average loss across all training instances\n",
    "    cost = tf.reduce_mean(cross_entropy_loss)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHKpvgB3eSu9"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, y_pred_softmax):\n",
    "\n",
    "    # Calculate the predictions in the form of a boolean array, by considering only the class with the highest probability\n",
    "    # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
    "    predictions_bool = tf.equal(tf.argmax(y_pred_softmax, 0), tf.argmax(y, 0))\n",
    "    predictions_correct = tf.cast(predictions_bool, tf.float32)\n",
    "\n",
    "    # Finally, we just determine the mean value of the correct predictions\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "  \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aHd4zmAGeWOE",
    "outputId": "bba040d8-98b0-4309-a111-beb347857bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n",
      "Reshaped training data  (784, 60000)\n",
      "Reshaped test data  (784, 10000)\n",
      "Iteration  0 : Train Loss =  2.3147078   Train Acc:  0.102633335   Validation Loss =  2.3144174   Validation Acc:  0.1024\n",
      "Iteration  1 : Train Loss =  2.1998842   Train Acc:  0.31701666   Validation Loss =  2.2005155   Validation Acc:  0.3189\n",
      "Iteration  2 : Train Loss =  2.1081498   Train Acc:  0.48698333   Validation Loss =  2.1096928   Validation Acc:  0.4839\n",
      "Iteration  3 : Train Loss =  2.0082135   Train Acc:  0.5226667   Validation Loss =  2.0106113   Validation Acc:  0.5219\n",
      "Iteration  4 : Train Loss =  1.8994812   Train Acc:  0.54466665   Validation Loss =  1.9029397   Validation Acc:  0.5442\n",
      "Iteration  5 : Train Loss =  1.785344   Train Acc:  0.5717833   Validation Loss =  1.789693   Validation Acc:  0.5694\n",
      "Iteration  6 : Train Loss =  1.670363   Train Acc:  0.59818333   Validation Loss =  1.6754575   Validation Acc:  0.5956\n",
      "Iteration  7 : Train Loss =  1.5575815   Train Acc:  0.61833334   Validation Loss =  1.5634059   Validation Acc:  0.6162\n",
      "Iteration  8 : Train Loss =  1.4465829   Train Acc:  0.64751667   Validation Loss =  1.4530797   Validation Acc:  0.6433\n",
      "Iteration  9 : Train Loss =  1.340068   Train Acc:  0.66833335   Validation Loss =  1.3471023   Validation Acc:  0.6597\n",
      "Iteration  10 : Train Loss =  1.241613   Train Acc:  0.67433333   Validation Loss =  1.2492373   Validation Acc:  0.6631\n",
      "Iteration  11 : Train Loss =  1.1521466   Train Acc:  0.6741167   Validation Loss =  1.1604806   Validation Acc:  0.6623\n",
      "Iteration  12 : Train Loss =  1.0730871   Train Acc:  0.67971665   Validation Loss =  1.0821775   Validation Acc:  0.665\n",
      "Iteration  13 : Train Loss =  1.0066729   Train Acc:  0.6846167   Validation Loss =  1.0164413   Validation Acc:  0.6722\n",
      "Iteration  14 : Train Loss =  0.9512556   Train Acc:  0.68755   Validation Loss =  0.9615073   Validation Acc:  0.6776\n",
      "Iteration  15 : Train Loss =  0.904737   Train Acc:  0.68988335   Validation Loss =  0.9155096   Validation Acc:  0.6792\n",
      "Iteration  16 : Train Loss =  0.86652577   Train Acc:  0.69133335   Validation Loss =  0.87813103   Validation Acc:  0.6801\n",
      "Iteration  17 : Train Loss =  0.83400923   Train Acc:  0.6989   Validation Loss =  0.84681445   Validation Acc:  0.6863\n",
      "Iteration  18 : Train Loss =  0.80715287   Train Acc:  0.70911664   Validation Loss =  0.8214375   Validation Acc:  0.6962\n",
      "Iteration  19 : Train Loss =  0.78427213   Train Acc:  0.7163333   Validation Loss =  0.80010366   Validation Acc:  0.7001\n",
      "Iteration  20 : Train Loss =  0.76485544   Train Acc:  0.7201167   Validation Loss =  0.78185606   Validation Acc:  0.7046\n",
      "Iteration  21 : Train Loss =  0.7465703   Train Acc:  0.7259333   Validation Loss =  0.76426864   Validation Acc:  0.7095\n",
      "Iteration  22 : Train Loss =  0.73097825   Train Acc:  0.73053336   Validation Loss =  0.7493188   Validation Acc:  0.7152\n",
      "Iteration  23 : Train Loss =  0.7158421   Train Acc:  0.73406667   Validation Loss =  0.7346295   Validation Acc:  0.7194\n",
      "Iteration  24 : Train Loss =  0.70177376   Train Acc:  0.74036664   Validation Loss =  0.7213074   Validation Acc:  0.7269\n",
      "Iteration  25 : Train Loss =  0.6876742   Train Acc:  0.74916667   Validation Loss =  0.7085552   Validation Acc:  0.7361\n",
      "Iteration  26 : Train Loss =  0.6745889   Train Acc:  0.75511664   Validation Loss =  0.69675416   Validation Acc:  0.7433\n",
      "Iteration  27 : Train Loss =  0.66197306   Train Acc:  0.75871664   Validation Loss =  0.6845189   Validation Acc:  0.7468\n",
      "Iteration  28 : Train Loss =  0.64918494   Train Acc:  0.76555   Validation Loss =  0.6716777   Validation Acc:  0.7551\n",
      "Iteration  29 : Train Loss =  0.63777673   Train Acc:  0.77071667   Validation Loss =  0.66012526   Validation Acc:  0.7594\n",
      "Iteration  30 : Train Loss =  0.6265944   Train Acc:  0.77501667   Validation Loss =  0.64884806   Validation Acc:  0.7639\n",
      "Iteration  31 : Train Loss =  0.6157983   Train Acc:  0.78125   Validation Loss =  0.6388599   Validation Acc:  0.77\n",
      "Iteration  32 : Train Loss =  0.60659766   Train Acc:  0.785   Validation Loss =  0.62988377   Validation Acc:  0.7749\n",
      "Iteration  33 : Train Loss =  0.5997434   Train Acc:  0.7880167   Validation Loss =  0.62346315   Validation Acc:  0.7745\n",
      "Iteration  34 : Train Loss =  0.59114987   Train Acc:  0.78931665   Validation Loss =  0.6145504   Validation Acc:  0.7804\n",
      "Iteration  35 : Train Loss =  0.57981646   Train Acc:  0.79745   Validation Loss =  0.604225   Validation Acc:  0.7873\n",
      "Iteration  36 : Train Loss =  0.57231027   Train Acc:  0.8004   Validation Loss =  0.5960791   Validation Acc:  0.7883\n",
      "Iteration  37 : Train Loss =  0.56580186   Train Acc:  0.8003833   Validation Loss =  0.5897637   Validation Acc:  0.7908\n",
      "Iteration  38 : Train Loss =  0.55807513   Train Acc:  0.8050167   Validation Loss =  0.58322316   Validation Acc:  0.7946\n",
      "Iteration  39 : Train Loss =  0.55183184   Train Acc:  0.8078333   Validation Loss =  0.57620144   Validation Acc:  0.7961\n",
      "Iteration  40 : Train Loss =  0.54528546   Train Acc:  0.81005   Validation Loss =  0.5702158   Validation Acc:  0.7988\n",
      "Iteration  41 : Train Loss =  0.53999025   Train Acc:  0.81305   Validation Loss =  0.56599414   Validation Acc:  0.801\n",
      "Iteration  42 : Train Loss =  0.5348445   Train Acc:  0.8154167   Validation Loss =  0.5599542   Validation Acc:  0.803\n",
      "Iteration  43 : Train Loss =  0.5283212   Train Acc:  0.81771666   Validation Loss =  0.5544541   Validation Acc:  0.8041\n",
      "Iteration  44 : Train Loss =  0.5244967   Train Acc:  0.81875   Validation Loss =  0.5516177   Validation Acc:  0.8069\n",
      "Iteration  45 : Train Loss =  0.51978177   Train Acc:  0.8208167   Validation Loss =  0.545641   Validation Acc:  0.8089\n",
      "Iteration  46 : Train Loss =  0.51357454   Train Acc:  0.8232   Validation Loss =  0.5405558   Validation Acc:  0.8111\n",
      "Iteration  47 : Train Loss =  0.5101612   Train Acc:  0.82383335   Validation Loss =  0.5381274   Validation Acc:  0.8127\n",
      "Iteration  48 : Train Loss =  0.50590765   Train Acc:  0.8259   Validation Loss =  0.5327011   Validation Acc:  0.8131\n",
      "Iteration  49 : Train Loss =  0.5003493   Train Acc:  0.82843333   Validation Loss =  0.52844346   Validation Acc:  0.8155\n",
      "Iteration  50 : Train Loss =  0.49682784   Train Acc:  0.82785   Validation Loss =  0.52570635   Validation Acc:  0.8164\n",
      "Iteration  51 : Train Loss =  0.4929629   Train Acc:  0.8297333   Validation Loss =  0.5208424   Validation Acc:  0.8186\n",
      "Iteration  52 : Train Loss =  0.48880067   Train Acc:  0.83215   Validation Loss =  0.5177883   Validation Acc:  0.8199\n",
      "Iteration  53 : Train Loss =  0.4851625   Train Acc:  0.83195   Validation Loss =  0.5144859   Validation Acc:  0.8201\n",
      "Iteration  54 : Train Loss =  0.48118833   Train Acc:  0.8336667   Validation Loss =  0.5104208   Validation Acc:  0.8218\n",
      "Iteration  55 : Train Loss =  0.47778794   Train Acc:  0.8349   Validation Loss =  0.50761384   Validation Acc:  0.8234\n",
      "Iteration  56 : Train Loss =  0.47515613   Train Acc:  0.83466667   Validation Loss =  0.50494564   Validation Acc:  0.8242\n",
      "Iteration  57 : Train Loss =  0.4718833   Train Acc:  0.83738333   Validation Loss =  0.5024117   Validation Acc:  0.8265\n",
      "Iteration  58 : Train Loss =  0.46841523   Train Acc:  0.8379667   Validation Loss =  0.49862626   Validation Acc:  0.8268\n",
      "Iteration  59 : Train Loss =  0.4657581   Train Acc:  0.83861667   Validation Loss =  0.4968498   Validation Acc:  0.8273\n",
      "Iteration  60 : Train Loss =  0.46280527   Train Acc:  0.83998334   Validation Loss =  0.4935806   Validation Acc:  0.8292\n",
      "Iteration  61 : Train Loss =  0.4597832   Train Acc:  0.84113336   Validation Loss =  0.49106875   Validation Acc:  0.8295\n",
      "Iteration  62 : Train Loss =  0.45732513   Train Acc:  0.8416167   Validation Loss =  0.48881796   Validation Acc:  0.8301\n",
      "Iteration  63 : Train Loss =  0.45495936   Train Acc:  0.84256667   Validation Loss =  0.48623618   Validation Acc:  0.8314\n",
      "Iteration  64 : Train Loss =  0.4524002   Train Acc:  0.8435   Validation Loss =  0.48452792   Validation Acc:  0.8313\n",
      "Iteration  65 : Train Loss =  0.45019966   Train Acc:  0.8441833   Validation Loss =  0.48184872   Validation Acc:  0.8318\n",
      "Iteration  66 : Train Loss =  0.4485099   Train Acc:  0.84436667   Validation Loss =  0.48098212   Validation Acc:  0.8325\n",
      "Iteration  67 : Train Loss =  0.44716048   Train Acc:  0.8447667   Validation Loss =  0.4788429   Validation Acc:  0.8331\n",
      "Iteration  68 : Train Loss =  0.44604152   Train Acc:  0.8444   Validation Loss =  0.47949347   Validation Acc:  0.8326\n",
      "Iteration  69 : Train Loss =  0.44611734   Train Acc:  0.8445167   Validation Loss =  0.47763985   Validation Acc:  0.8325\n",
      "Iteration  70 : Train Loss =  0.44215137   Train Acc:  0.84536666   Validation Loss =  0.475929   Validation Acc:  0.8326\n",
      "Iteration  71 : Train Loss =  0.4378458   Train Acc:  0.8481   Validation Loss =  0.47031713   Validation Acc:  0.8354\n",
      "Iteration  72 : Train Loss =  0.43509504   Train Acc:  0.84893334   Validation Loss =  0.46800226   Validation Acc:  0.8367\n",
      "Iteration  73 : Train Loss =  0.43473753   Train Acc:  0.8484333   Validation Loss =  0.46863273   Validation Acc:  0.8355\n",
      "Iteration  74 : Train Loss =  0.43434167   Train Acc:  0.84891665   Validation Loss =  0.46706215   Validation Acc:  0.8366\n",
      "Iteration  75 : Train Loss =  0.43119228   Train Acc:  0.84975   Validation Loss =  0.46540868   Validation Acc:  0.837\n",
      "Iteration  76 : Train Loss =  0.4283083   Train Acc:  0.8516667   Validation Loss =  0.4621123   Validation Acc:  0.8385\n",
      "Iteration  77 : Train Loss =  0.42656785   Train Acc:  0.8520667   Validation Loss =  0.46030223   Validation Acc:  0.8379\n",
      "Iteration  78 : Train Loss =  0.42558908   Train Acc:  0.85155   Validation Loss =  0.46045673   Validation Acc:  0.8392\n",
      "Iteration  79 : Train Loss =  0.42459643   Train Acc:  0.85226667   Validation Loss =  0.45878926   Validation Acc:  0.838\n",
      "Iteration  80 : Train Loss =  0.42229635   Train Acc:  0.85331666   Validation Loss =  0.45727423   Validation Acc:  0.8398\n",
      "Iteration  81 : Train Loss =  0.41966313   Train Acc:  0.8538667   Validation Loss =  0.45458937   Validation Acc:  0.8394\n",
      "Iteration  82 : Train Loss =  0.41809702   Train Acc:  0.85466665   Validation Loss =  0.45311508   Validation Acc:  0.8393\n",
      "Iteration  83 : Train Loss =  0.41738635   Train Acc:  0.8544833   Validation Loss =  0.45290282   Validation Acc:  0.8413\n",
      "Iteration  84 : Train Loss =  0.41621783   Train Acc:  0.85501665   Validation Loss =  0.45133448   Validation Acc:  0.84\n",
      "Iteration  85 : Train Loss =  0.41405618   Train Acc:  0.8551667   Validation Loss =  0.44992134   Validation Acc:  0.8414\n",
      "Iteration  86 : Train Loss =  0.41204134   Train Acc:  0.85658336   Validation Loss =  0.44755197   Validation Acc:  0.8418\n",
      "Iteration  87 : Train Loss =  0.4105152   Train Acc:  0.8566333   Validation Loss =  0.44643262   Validation Acc:  0.8419\n",
      "Iteration  88 : Train Loss =  0.4091404   Train Acc:  0.8570667   Validation Loss =  0.44526407   Validation Acc:  0.8435\n",
      "Iteration  89 : Train Loss =  0.40775442   Train Acc:  0.8577667   Validation Loss =  0.44386488   Validation Acc:  0.8439\n",
      "Iteration  90 : Train Loss =  0.4066881   Train Acc:  0.8577833   Validation Loss =  0.44346943   Validation Acc:  0.8447\n",
      "Iteration  91 : Train Loss =  0.40590298   Train Acc:  0.85836667   Validation Loss =  0.44224375   Validation Acc:  0.844\n",
      "Iteration  92 : Train Loss =  0.40444654   Train Acc:  0.85821664   Validation Loss =  0.4418731   Validation Acc:  0.8443\n",
      "Iteration  93 : Train Loss =  0.40267864   Train Acc:  0.85955   Validation Loss =  0.4395904   Validation Acc:  0.8447\n",
      "Iteration  94 : Train Loss =  0.40067542   Train Acc:  0.8597   Validation Loss =  0.43824422   Validation Acc:  0.8463\n",
      "Iteration  95 : Train Loss =  0.39905354   Train Acc:  0.8608   Validation Loss =  0.43664315   Validation Acc:  0.8461\n",
      "Iteration  96 : Train Loss =  0.39772618   Train Acc:  0.86111665   Validation Loss =  0.4354711   Validation Acc:  0.8472\n",
      "Iteration  97 : Train Loss =  0.39654988   Train Acc:  0.86135   Validation Loss =  0.43471226   Validation Acc:  0.8477\n",
      "Iteration  98 : Train Loss =  0.39550787   Train Acc:  0.8621167   Validation Loss =  0.43353242   Validation Acc:  0.8471\n",
      "Iteration  99 : Train Loss =  0.39445665   Train Acc:  0.8613167   Validation Loss =  0.4331687   Validation Acc:  0.8484\n",
      "Iteration  100 : Train Loss =  0.39347625   Train Acc:  0.86263335   Validation Loss =  0.43198878   Validation Acc:  0.8473\n",
      "Iteration  101 : Train Loss =  0.392165   Train Acc:  0.8624333   Validation Loss =  0.4313252   Validation Acc:  0.8488\n",
      "Iteration  102 : Train Loss =  0.39080104   Train Acc:  0.86338335   Validation Loss =  0.4298146   Validation Acc:  0.8485\n",
      "Iteration  103 : Train Loss =  0.38919753   Train Acc:  0.86326665   Validation Loss =  0.4288253   Validation Acc:  0.85\n",
      "Iteration  104 : Train Loss =  0.38777965   Train Acc:  0.8645833   Validation Loss =  0.42723104   Validation Acc:  0.8489\n",
      "Iteration  105 : Train Loss =  0.3863676   Train Acc:  0.86455   Validation Loss =  0.42638916   Validation Acc:  0.85\n",
      "Iteration  106 : Train Loss =  0.38504907   Train Acc:  0.86546665   Validation Loss =  0.4249852   Validation Acc:  0.8503\n",
      "Iteration  107 : Train Loss =  0.38371417   Train Acc:  0.86555   Validation Loss =  0.42408004   Validation Acc:  0.8513\n",
      "Iteration  108 : Train Loss =  0.3823717   Train Acc:  0.86635   Validation Loss =  0.4227746   Validation Acc:  0.8516\n",
      "Iteration  109 : Train Loss =  0.3810565   Train Acc:  0.86663336   Validation Loss =  0.42181602   Validation Acc:  0.8517\n",
      "Iteration  110 : Train Loss =  0.37979227   Train Acc:  0.8673   Validation Loss =  0.42067975   Validation Acc:  0.8516\n",
      "Iteration  111 : Train Loss =  0.3785797   Train Acc:  0.86745   Validation Loss =  0.41973603   Validation Acc:  0.8522\n",
      "Iteration  112 : Train Loss =  0.37740764   Train Acc:  0.86768335   Validation Loss =  0.41877705   Validation Acc:  0.8524\n",
      "Iteration  113 : Train Loss =  0.37627298   Train Acc:  0.8681   Validation Loss =  0.41788632   Validation Acc:  0.8531\n",
      "Iteration  114 : Train Loss =  0.37516263   Train Acc:  0.8688667   Validation Loss =  0.41695863   Validation Acc:  0.8528\n",
      "Iteration  115 : Train Loss =  0.3741382   Train Acc:  0.869   Validation Loss =  0.41620332   Validation Acc:  0.8536\n",
      "Iteration  116 : Train Loss =  0.37335482   Train Acc:  0.86988336   Validation Loss =  0.41551554   Validation Acc:  0.8528\n",
      "Iteration  117 : Train Loss =  0.37318537   Train Acc:  0.8695167   Validation Loss =  0.41581845   Validation Acc:  0.8541\n",
      "Iteration  118 : Train Loss =  0.37501445   Train Acc:  0.86973333   Validation Loss =  0.4175747   Validation Acc:  0.8519\n",
      "Iteration  119 : Train Loss =  0.37850818   Train Acc:  0.86661667   Validation Loss =  0.42210478   Validation Acc:  0.8508\n",
      "Iteration  120 : Train Loss =  0.38758075   Train Acc:  0.86495   Validation Loss =  0.4306164   Validation Acc:  0.8486\n",
      "Iteration  121 : Train Loss =  0.37450486   Train Acc:  0.86873335   Validation Loss =  0.41833496   Validation Acc:  0.8534\n",
      "Iteration  122 : Train Loss =  0.36714426   Train Acc:  0.8723   Validation Loss =  0.41032538   Validation Acc:  0.856\n",
      "Iteration  123 : Train Loss =  0.36962447   Train Acc:  0.8714   Validation Loss =  0.41300768   Validation Acc:  0.8525\n",
      "Iteration  124 : Train Loss =  0.37105778   Train Acc:  0.8692833   Validation Loss =  0.41538194   Validation Acc:  0.8555\n",
      "Iteration  125 : Train Loss =  0.36638144   Train Acc:  0.87266666   Validation Loss =  0.4102436   Validation Acc:  0.854\n",
      "Iteration  126 : Train Loss =  0.36303318   Train Acc:  0.87343335   Validation Loss =  0.40717173   Validation Acc:  0.8557\n",
      "Iteration  127 : Train Loss =  0.3666871   Train Acc:  0.87116665   Validation Loss =  0.41158506   Validation Acc:  0.8578\n",
      "Iteration  128 : Train Loss =  0.3654611   Train Acc:  0.8724333   Validation Loss =  0.4101795   Validation Acc:  0.854\n",
      "Iteration  129 : Train Loss =  0.35947913   Train Acc:  0.8750833   Validation Loss =  0.40444097   Validation Acc:  0.8567\n",
      "Iteration  130 : Train Loss =  0.36183834   Train Acc:  0.87355   Validation Loss =  0.40723616   Validation Acc:  0.859\n",
      "Iteration  131 : Train Loss =  0.36293265   Train Acc:  0.87368333   Validation Loss =  0.40830278   Validation Acc:  0.8544\n",
      "Iteration  132 : Train Loss =  0.35696852   Train Acc:  0.87565   Validation Loss =  0.40262786   Validation Acc:  0.8581\n",
      "Iteration  133 : Train Loss =  0.3576759   Train Acc:  0.87556666   Validation Loss =  0.4035263   Validation Acc:  0.8591\n",
      "Iteration  134 : Train Loss =  0.35960183   Train Acc:  0.8742833   Validation Loss =  0.40552565   Validation Acc:  0.8556\n",
      "Iteration  135 : Train Loss =  0.354584   Train Acc:  0.87665   Validation Loss =  0.40091562   Validation Acc:  0.8584\n",
      "Iteration  136 : Train Loss =  0.3540898   Train Acc:  0.87665   Validation Loss =  0.40047544   Validation Acc:  0.8589\n",
      "Iteration  137 : Train Loss =  0.35601544   Train Acc:  0.87583333   Validation Loss =  0.40247467   Validation Acc:  0.8564\n",
      "Iteration  138 : Train Loss =  0.35211775   Train Acc:  0.87715   Validation Loss =  0.3990814   Validation Acc:  0.8595\n",
      "Iteration  139 : Train Loss =  0.3507759   Train Acc:  0.87768334   Validation Loss =  0.3977112   Validation Acc:  0.8598\n",
      "Iteration  140 : Train Loss =  0.35219452   Train Acc:  0.8770667   Validation Loss =  0.39910963   Validation Acc:  0.8574\n",
      "Iteration  141 : Train Loss =  0.34957242   Train Acc:  0.8779167   Validation Loss =  0.3970321   Validation Acc:  0.8599\n",
      "Iteration  142 : Train Loss =  0.34775177   Train Acc:  0.8785167   Validation Loss =  0.39526662   Validation Acc:  0.861\n",
      "Iteration  143 : Train Loss =  0.34851176   Train Acc:  0.87815   Validation Loss =  0.39599013   Validation Acc:  0.8587\n",
      "Iteration  144 : Train Loss =  0.34705552   Train Acc:  0.8786333   Validation Loss =  0.39505503   Validation Acc:  0.8612\n",
      "Iteration  145 : Train Loss =  0.3450634   Train Acc:  0.87948334   Validation Loss =  0.39321408   Validation Acc:  0.8601\n",
      "Iteration  146 : Train Loss =  0.3449987   Train Acc:  0.8799   Validation Loss =  0.39316577   Validation Acc:  0.86\n",
      "Iteration  147 : Train Loss =  0.34441555   Train Acc:  0.87946665   Validation Loss =  0.39299712   Validation Acc:  0.8621\n",
      "Iteration  148 : Train Loss =  0.34269348   Train Acc:  0.88045   Validation Loss =  0.3914416   Validation Acc:  0.8607\n",
      "Iteration  149 : Train Loss =  0.34177864   Train Acc:  0.88065   Validation Loss =  0.39061344   Validation Acc:  0.8607\n",
      "Iteration  150 : Train Loss =  0.34159285   Train Acc:  0.88061666   Validation Loss =  0.39076966   Validation Acc:  0.8622\n",
      "Iteration  151 : Train Loss =  0.34051204   Train Acc:  0.8807167   Validation Loss =  0.38988876   Validation Acc:  0.861\n",
      "Iteration  152 : Train Loss =  0.33905587   Train Acc:  0.88135   Validation Loss =  0.38863042   Validation Acc:  0.8614\n",
      "Iteration  153 : Train Loss =  0.3385546   Train Acc:  0.8818   Validation Loss =  0.3883542   Validation Acc:  0.8634\n",
      "Iteration  154 : Train Loss =  0.3380721   Train Acc:  0.8814   Validation Loss =  0.38810208   Validation Acc:  0.8618\n",
      "Iteration  155 : Train Loss =  0.33678558   Train Acc:  0.8824   Validation Loss =  0.38708994   Validation Acc:  0.8636\n",
      "Iteration  156 : Train Loss =  0.33568624   Train Acc:  0.88236666   Validation Loss =  0.38604805   Validation Acc:  0.8625\n",
      "Iteration  157 : Train Loss =  0.3351544   Train Acc:  0.88255   Validation Loss =  0.3858281   Validation Acc:  0.8631\n",
      "Iteration  158 : Train Loss =  0.33448938   Train Acc:  0.88341665   Validation Loss =  0.38548177   Validation Acc:  0.8646\n",
      "Iteration  159 : Train Loss =  0.3334761   Train Acc:  0.88353336   Validation Loss =  0.38453954   Validation Acc:  0.8637\n",
      "Iteration  160 : Train Loss =  0.33262256   Train Acc:  0.8833167   Validation Loss =  0.3839717   Validation Acc:  0.8637\n",
      "Iteration  161 : Train Loss =  0.33260697   Train Acc:  0.88371664   Validation Loss =  0.3844163   Validation Acc:  0.8634\n",
      "Iteration  162 : Train Loss =  0.33415914   Train Acc:  0.8825   Validation Loss =  0.38581017   Validation Acc:  0.8623\n",
      "Iteration  163 : Train Loss =  0.3376245   Train Acc:  0.8811833   Validation Loss =  0.3902554   Validation Acc:  0.8619\n",
      "Iteration  164 : Train Loss =  0.3456156   Train Acc:  0.8767833   Validation Loss =  0.39800972   Validation Acc:  0.8599\n",
      "Iteration  165 : Train Loss =  0.3426532   Train Acc:  0.87923336   Validation Loss =  0.3957367   Validation Acc:  0.8608\n",
      "Iteration  166 : Train Loss =  0.3310683   Train Acc:  0.88346666   Validation Loss =  0.38371876   Validation Acc:  0.865\n",
      "Iteration  167 : Train Loss =  0.3288594   Train Acc:  0.8839   Validation Loss =  0.38155934   Validation Acc:  0.8647\n",
      "Iteration  168 : Train Loss =  0.33397943   Train Acc:  0.8831   Validation Loss =  0.3873295   Validation Acc:  0.8632\n",
      "Iteration  169 : Train Loss =  0.3314002   Train Acc:  0.88318336   Validation Loss =  0.38470373   Validation Acc:  0.8636\n",
      "Iteration  170 : Train Loss =  0.326181   Train Acc:  0.88528335   Validation Loss =  0.37972137   Validation Acc:  0.8651\n",
      "Iteration  171 : Train Loss =  0.3288694   Train Acc:  0.88493335   Validation Loss =  0.3828563   Validation Acc:  0.8642\n",
      "Iteration  172 : Train Loss =  0.32911217   Train Acc:  0.88351667   Validation Loss =  0.3829233   Validation Acc:  0.8645\n",
      "Iteration  173 : Train Loss =  0.32500026   Train Acc:  0.8857333   Validation Loss =  0.37926808   Validation Acc:  0.867\n",
      "Iteration  174 : Train Loss =  0.326173   Train Acc:  0.8854667   Validation Loss =  0.38091603   Validation Acc:  0.8657\n",
      "Iteration  175 : Train Loss =  0.32638523   Train Acc:  0.88481665   Validation Loss =  0.3806219   Validation Acc:  0.865\n",
      "Iteration  176 : Train Loss =  0.32285458   Train Acc:  0.88691664   Validation Loss =  0.37768734   Validation Acc:  0.867\n",
      "Iteration  177 : Train Loss =  0.32401553   Train Acc:  0.8858   Validation Loss =  0.37944916   Validation Acc:  0.8666\n",
      "Iteration  178 : Train Loss =  0.32375678   Train Acc:  0.8857167   Validation Loss =  0.37859875   Validation Acc:  0.8649\n",
      "Iteration  179 : Train Loss =  0.3199473   Train Acc:  0.88803333   Validation Loss =  0.3753231   Validation Acc:  0.868\n",
      "Iteration  180 : Train Loss =  0.32158244   Train Acc:  0.88631666   Validation Loss =  0.37765417   Validation Acc:  0.8669\n",
      "Iteration  181 : Train Loss =  0.3211122   Train Acc:  0.8863   Validation Loss =  0.37671402   Validation Acc:  0.865\n",
      "Iteration  182 : Train Loss =  0.31728128   Train Acc:  0.88893336   Validation Loss =  0.37331972   Validation Acc:  0.8684\n",
      "Iteration  183 : Train Loss =  0.31885645   Train Acc:  0.88738334   Validation Loss =  0.37558207   Validation Acc:  0.8674\n",
      "Iteration  184 : Train Loss =  0.31870908   Train Acc:  0.8871833   Validation Loss =  0.37505862   Validation Acc:  0.8668\n",
      "Iteration  185 : Train Loss =  0.3151413   Train Acc:  0.8893667   Validation Loss =  0.37177858   Validation Acc:  0.8682\n",
      "Iteration  186 : Train Loss =  0.31619868   Train Acc:  0.88878334   Validation Loss =  0.37344962   Validation Acc:  0.8691\n",
      "Iteration  187 : Train Loss =  0.31622806   Train Acc:  0.88746667   Validation Loss =  0.3732595   Validation Acc:  0.868\n",
      "Iteration  188 : Train Loss =  0.31338483   Train Acc:  0.88995   Validation Loss =  0.3706385   Validation Acc:  0.8688\n",
      "Iteration  189 : Train Loss =  0.31385377   Train Acc:  0.88991666   Validation Loss =  0.3716497   Validation Acc:  0.8692\n",
      "Iteration  190 : Train Loss =  0.3137933   Train Acc:  0.8885667   Validation Loss =  0.37148878   Validation Acc:  0.8689\n",
      "Iteration  191 : Train Loss =  0.3114315   Train Acc:  0.89061666   Validation Loss =  0.36939946   Validation Acc:  0.8694\n",
      "Iteration  192 : Train Loss =  0.3118078   Train Acc:  0.89075   Validation Loss =  0.37023228   Validation Acc:  0.8696\n",
      "Iteration  193 : Train Loss =  0.31172702   Train Acc:  0.88951665   Validation Loss =  0.37011275   Validation Acc:  0.8688\n",
      "Iteration  194 : Train Loss =  0.30948073   Train Acc:  0.89108336   Validation Loss =  0.36821085   Validation Acc:  0.8702\n",
      "Iteration  195 : Train Loss =  0.3095149   Train Acc:  0.8912333   Validation Loss =  0.36856982   Validation Acc:  0.8696\n",
      "Iteration  196 : Train Loss =  0.30967242   Train Acc:  0.8903667   Validation Loss =  0.36872736   Validation Acc:  0.8689\n",
      "Iteration  197 : Train Loss =  0.30780786   Train Acc:  0.8914833   Validation Loss =  0.36728075   Validation Acc:  0.8702\n",
      "Iteration  198 : Train Loss =  0.3073317   Train Acc:  0.89196664   Validation Loss =  0.3669602   Validation Acc:  0.8695\n",
      "Iteration  199 : Train Loss =  0.30740488   Train Acc:  0.8912   Validation Loss =  0.367083   Validation Acc:  0.87\n",
      "Iteration  200 : Train Loss =  0.30603462   Train Acc:  0.8923   Validation Loss =  0.36622941   Validation Acc:  0.8707\n",
      "Iteration  201 : Train Loss =  0.30521792   Train Acc:  0.8925667   Validation Loss =  0.3654482   Validation Acc:  0.8707\n",
      "Iteration  202 : Train Loss =  0.30524877   Train Acc:  0.89155   Validation Loss =  0.36560008   Validation Acc:  0.8709\n",
      "Iteration  203 : Train Loss =  0.3044149   Train Acc:  0.89266664   Validation Loss =  0.3653299   Validation Acc:  0.8715\n",
      "Iteration  204 : Train Loss =  0.30330923   Train Acc:  0.89273334   Validation Loss =  0.36416483   Validation Acc:  0.8718\n",
      "Iteration  205 : Train Loss =  0.3029643   Train Acc:  0.89245   Validation Loss =  0.36402076   Validation Acc:  0.8717\n",
      "Iteration  206 : Train Loss =  0.30254844   Train Acc:  0.89335   Validation Loss =  0.3641636   Validation Acc:  0.8718\n",
      "Iteration  207 : Train Loss =  0.30160612   Train Acc:  0.8929833   Validation Loss =  0.36313006   Validation Acc:  0.8714\n",
      "Iteration  208 : Train Loss =  0.3008833   Train Acc:  0.8936   Validation Loss =  0.3627611   Validation Acc:  0.872\n",
      "Iteration  209 : Train Loss =  0.30059928   Train Acc:  0.89388335   Validation Loss =  0.36292514   Validation Acc:  0.872\n",
      "Iteration  210 : Train Loss =  0.30005378   Train Acc:  0.89346665   Validation Loss =  0.36229813   Validation Acc:  0.873\n",
      "Iteration  211 : Train Loss =  0.2991793   Train Acc:  0.89451665   Validation Loss =  0.36190704   Validation Acc:  0.8723\n",
      "Iteration  212 : Train Loss =  0.29861876   Train Acc:  0.89475   Validation Loss =  0.3615788   Validation Acc:  0.8735\n",
      "Iteration  213 : Train Loss =  0.29827604   Train Acc:  0.8943667   Validation Loss =  0.36124474   Validation Acc:  0.8732\n",
      "Iteration  214 : Train Loss =  0.29769698   Train Acc:  0.8949   Validation Loss =  0.36118978   Validation Acc:  0.873\n",
      "Iteration  215 : Train Loss =  0.29703072   Train Acc:  0.89503336   Validation Loss =  0.36065325   Validation Acc:  0.8725\n",
      "Iteration  216 : Train Loss =  0.2966168   Train Acc:  0.89463335   Validation Loss =  0.36041793   Validation Acc:  0.8717\n",
      "Iteration  217 : Train Loss =  0.29657024   Train Acc:  0.8954167   Validation Loss =  0.36087942   Validation Acc:  0.8728\n",
      "Iteration  218 : Train Loss =  0.29660773   Train Acc:  0.8945   Validation Loss =  0.3609376   Validation Acc:  0.8718\n",
      "Iteration  219 : Train Loss =  0.29725018   Train Acc:  0.89538336   Validation Loss =  0.3620568   Validation Acc:  0.8728\n",
      "Iteration  220 : Train Loss =  0.29781312   Train Acc:  0.89378333   Validation Loss =  0.36274666   Validation Acc:  0.8702\n",
      "Iteration  221 : Train Loss =  0.30014154   Train Acc:  0.89345   Validation Loss =  0.36549965   Validation Acc:  0.8717\n",
      "Iteration  222 : Train Loss =  0.29893574   Train Acc:  0.89308333   Validation Loss =  0.3644063   Validation Acc:  0.8688\n",
      "Iteration  223 : Train Loss =  0.2980288   Train Acc:  0.89433336   Validation Loss =  0.36376727   Validation Acc:  0.8709\n",
      "Iteration  224 : Train Loss =  0.29365927   Train Acc:  0.89523333   Validation Loss =  0.35933867   Validation Acc:  0.8723\n",
      "Iteration  225 : Train Loss =  0.29131058   Train Acc:  0.8972833   Validation Loss =  0.35722208   Validation Acc:  0.8745\n",
      "Iteration  226 : Train Loss =  0.29132187   Train Acc:  0.8969167   Validation Loss =  0.35738575   Validation Acc:  0.874\n",
      "Iteration  227 : Train Loss =  0.29241636   Train Acc:  0.8957667   Validation Loss =  0.35887554   Validation Acc:  0.8726\n",
      "Iteration  228 : Train Loss =  0.29345378   Train Acc:  0.89641666   Validation Loss =  0.36028305   Validation Acc:  0.8731\n",
      "Iteration  229 : Train Loss =  0.29181865   Train Acc:  0.8961   Validation Loss =  0.35866272   Validation Acc:  0.8725\n",
      "Iteration  230 : Train Loss =  0.2900419   Train Acc:  0.89736664   Validation Loss =  0.3574384   Validation Acc:  0.8759\n",
      "Iteration  231 : Train Loss =  0.28834546   Train Acc:  0.89745   Validation Loss =  0.35557818   Validation Acc:  0.8743\n",
      "Iteration  232 : Train Loss =  0.2877788   Train Acc:  0.8975833   Validation Loss =  0.35551387   Validation Acc:  0.8749\n",
      "Iteration  233 : Train Loss =  0.28816146   Train Acc:  0.89788336   Validation Loss =  0.35626435   Validation Acc:  0.8761\n",
      "Iteration  234 : Train Loss =  0.28841645   Train Acc:  0.8973167   Validation Loss =  0.35654107   Validation Acc:  0.8735\n",
      "Iteration  235 : Train Loss =  0.2884023   Train Acc:  0.89776665   Validation Loss =  0.35722762   Validation Acc:  0.8758\n",
      "Iteration  236 : Train Loss =  0.2870405   Train Acc:  0.8975667   Validation Loss =  0.35567626   Validation Acc:  0.874\n",
      "Iteration  237 : Train Loss =  0.28561062   Train Acc:  0.89888334   Validation Loss =  0.35472825   Validation Acc:  0.8762\n",
      "Iteration  238 : Train Loss =  0.2844966   Train Acc:  0.8986167   Validation Loss =  0.35367018   Validation Acc:  0.8752\n",
      "Iteration  239 : Train Loss =  0.28408533   Train Acc:  0.8987   Validation Loss =  0.3534676   Validation Acc:  0.8753\n",
      "Iteration  240 : Train Loss =  0.28414693   Train Acc:  0.8993667   Validation Loss =  0.35397604   Validation Acc:  0.8759\n",
      "Iteration  241 : Train Loss =  0.28417003   Train Acc:  0.89888334   Validation Loss =  0.3539636   Validation Acc:  0.875\n",
      "Iteration  242 : Train Loss =  0.28405312   Train Acc:  0.89918333   Validation Loss =  0.3544665   Validation Acc:  0.876\n",
      "Iteration  243 : Train Loss =  0.28332442   Train Acc:  0.8990833   Validation Loss =  0.3536298   Validation Acc:  0.8749\n",
      "Iteration  244 : Train Loss =  0.28256628   Train Acc:  0.90008336   Validation Loss =  0.3533631   Validation Acc:  0.8759\n",
      "Iteration  245 : Train Loss =  0.28157842   Train Acc:  0.90005   Validation Loss =  0.3524726   Validation Acc:  0.8751\n",
      "Iteration  246 : Train Loss =  0.28077447   Train Acc:  0.9005333   Validation Loss =  0.35190928   Validation Acc:  0.8769\n",
      "Iteration  247 : Train Loss =  0.28012824   Train Acc:  0.90066665   Validation Loss =  0.3516405   Validation Acc:  0.8761\n",
      "Iteration  248 : Train Loss =  0.2796499   Train Acc:  0.90085   Validation Loss =  0.35127085   Validation Acc:  0.8756\n",
      "Iteration  249 : Train Loss =  0.2792844   Train Acc:  0.9008333   Validation Loss =  0.3513542   Validation Acc:  0.8772\n",
      "Iteration  250 : Train Loss =  0.27899018   Train Acc:  0.9004667   Validation Loss =  0.35116205   Validation Acc:  0.8759\n",
      "Iteration  251 : Train Loss =  0.2788008   Train Acc:  0.90111667   Validation Loss =  0.35140094   Validation Acc:  0.878\n",
      "Iteration  252 : Train Loss =  0.27860945   Train Acc:  0.9009333   Validation Loss =  0.35131088   Validation Acc:  0.8755\n",
      "Iteration  253 : Train Loss =  0.2786879   Train Acc:  0.9012   Validation Loss =  0.3518363   Validation Acc:  0.8776\n",
      "Iteration  254 : Train Loss =  0.2785827   Train Acc:  0.90085   Validation Loss =  0.35182774   Validation Acc:  0.875\n",
      "Iteration  255 : Train Loss =  0.279014   Train Acc:  0.90071666   Validation Loss =  0.35272154   Validation Acc:  0.8772\n",
      "Iteration  256 : Train Loss =  0.27877125   Train Acc:  0.9009   Validation Loss =  0.35258883   Validation Acc:  0.8747\n",
      "Iteration  257 : Train Loss =  0.27918717   Train Acc:  0.9005333   Validation Loss =  0.35344264   Validation Acc:  0.8764\n",
      "Iteration  258 : Train Loss =  0.27808183   Train Acc:  0.9012833   Validation Loss =  0.35240698   Validation Acc:  0.8744\n",
      "Iteration  259 : Train Loss =  0.27736622   Train Acc:  0.9011833   Validation Loss =  0.35206836   Validation Acc:  0.8773\n",
      "Iteration  260 : Train Loss =  0.27552167   Train Acc:  0.9023333   Validation Loss =  0.3502195   Validation Acc:  0.8758\n",
      "Iteration  261 : Train Loss =  0.27411634   Train Acc:  0.9029   Validation Loss =  0.3491636   Validation Acc:  0.8793\n",
      "Iteration  262 : Train Loss =  0.2730543   Train Acc:  0.9030167   Validation Loss =  0.34819242   Validation Acc:  0.8774\n",
      "Iteration  263 : Train Loss =  0.27257186   Train Acc:  0.9034   Validation Loss =  0.348087   Validation Acc:  0.8772\n",
      "Iteration  264 : Train Loss =  0.27253318   Train Acc:  0.90315   Validation Loss =  0.3483042   Validation Acc:  0.8789\n",
      "Iteration  265 : Train Loss =  0.27268288   Train Acc:  0.90358335   Validation Loss =  0.34883857   Validation Acc:  0.8771\n",
      "Iteration  266 : Train Loss =  0.27302635   Train Acc:  0.90328336   Validation Loss =  0.34945884   Validation Acc:  0.879\n",
      "Iteration  267 : Train Loss =  0.27299425   Train Acc:  0.9030667   Validation Loss =  0.34979874   Validation Acc:  0.8765\n",
      "Iteration  268 : Train Loss =  0.2732055   Train Acc:  0.90316665   Validation Loss =  0.3502511   Validation Acc:  0.8785\n",
      "Iteration  269 : Train Loss =  0.27250713   Train Acc:  0.90326667   Validation Loss =  0.34984848   Validation Acc:  0.8766\n",
      "Iteration  270 : Train Loss =  0.27207524   Train Acc:  0.90353334   Validation Loss =  0.34966376   Validation Acc:  0.8788\n",
      "Iteration  271 : Train Loss =  0.27076676   Train Acc:  0.90396667   Validation Loss =  0.34853882   Validation Acc:  0.8774\n",
      "Iteration  272 : Train Loss =  0.26973367   Train Acc:  0.90455   Validation Loss =  0.34778532   Validation Acc:  0.88\n",
      "Iteration  273 : Train Loss =  0.26860797   Train Acc:  0.90508336   Validation Loss =  0.34674323   Validation Acc:  0.8782\n",
      "Iteration  274 : Train Loss =  0.2678922   Train Acc:  0.90485   Validation Loss =  0.34644863   Validation Acc:  0.8788\n",
      "Iteration  275 : Train Loss =  0.2676392   Train Acc:  0.90508336   Validation Loss =  0.3461294   Validation Acc:  0.8789\n",
      "Iteration  276 : Train Loss =  0.26798394   Train Acc:  0.9048667   Validation Loss =  0.34726235   Validation Acc:  0.8786\n",
      "Iteration  277 : Train Loss =  0.26958758   Train Acc:  0.9040167   Validation Loss =  0.34839183   Validation Acc:  0.8779\n",
      "Iteration  278 : Train Loss =  0.27225742   Train Acc:  0.90201664   Validation Loss =  0.35261866   Validation Acc:  0.877\n",
      "Iteration  279 : Train Loss =  0.27830237   Train Acc:  0.89961666   Validation Loss =  0.35743618   Validation Acc:  0.8736\n",
      "Iteration  280 : Train Loss =  0.27749988   Train Acc:  0.89925   Validation Loss =  0.35879308   Validation Acc:  0.8745\n",
      "Iteration  281 : Train Loss =  0.27474654   Train Acc:  0.90095   Validation Loss =  0.35426113   Validation Acc:  0.8747\n",
      "Iteration  282 : Train Loss =  0.26653138   Train Acc:  0.90503335   Validation Loss =  0.34702787   Validation Acc:  0.88\n",
      "Iteration  283 : Train Loss =  0.2680236   Train Acc:  0.90433335   Validation Loss =  0.3489756   Validation Acc:  0.8763\n",
      "Iteration  284 : Train Loss =  0.27292177   Train Acc:  0.90175   Validation Loss =  0.35333657   Validation Acc:  0.875\n",
      "Iteration  285 : Train Loss =  0.26825273   Train Acc:  0.9043667   Validation Loss =  0.34975156   Validation Acc:  0.8768\n",
      "Iteration  286 : Train Loss =  0.26338813   Train Acc:  0.9067   Validation Loss =  0.34481803   Validation Acc:  0.8805\n",
      "Iteration  287 : Train Loss =  0.26588422   Train Acc:  0.9051667   Validation Loss =  0.34702492   Validation Acc:  0.8782\n",
      "Iteration  288 : Train Loss =  0.26653707   Train Acc:  0.90473336   Validation Loss =  0.3488914   Validation Acc:  0.8787\n",
      "Iteration  289 : Train Loss =  0.26291972   Train Acc:  0.90625   Validation Loss =  0.34485167   Validation Acc:  0.879\n",
      "Iteration  290 : Train Loss =  0.2614654   Train Acc:  0.9069833   Validation Loss =  0.34349552   Validation Acc:  0.879\n",
      "Iteration  291 : Train Loss =  0.26377577   Train Acc:  0.90571666   Validation Loss =  0.346804   Validation Acc:  0.8788\n",
      "Iteration  292 : Train Loss =  0.26407728   Train Acc:  0.9055167   Validation Loss =  0.34663087   Validation Acc:  0.8785\n",
      "Iteration  293 : Train Loss =  0.26070607   Train Acc:  0.90716666   Validation Loss =  0.3439741   Validation Acc:  0.8803\n",
      "Iteration  294 : Train Loss =  0.2607779   Train Acc:  0.9077167   Validation Loss =  0.34426257   Validation Acc:  0.8779\n",
      "Iteration  295 : Train Loss =  0.26249394   Train Acc:  0.9063333   Validation Loss =  0.34593475   Validation Acc:  0.8788\n",
      "Iteration  296 : Train Loss =  0.26097476   Train Acc:  0.9073667   Validation Loss =  0.34513876   Validation Acc:  0.8776\n",
      "Iteration  297 : Train Loss =  0.25894278   Train Acc:  0.90795   Validation Loss =  0.34291327   Validation Acc:  0.8812\n",
      "Iteration  298 : Train Loss =  0.2592456   Train Acc:  0.90825   Validation Loss =  0.34333795   Validation Acc:  0.8796\n",
      "Iteration  299 : Train Loss =  0.2597414   Train Acc:  0.9076833   Validation Loss =  0.34459957   Validation Acc:  0.8803\n",
      "Iteration  300 : Train Loss =  0.25878876   Train Acc:  0.90823334   Validation Loss =  0.34311503   Validation Acc:  0.8804\n",
      "Iteration  301 : Train Loss =  0.2573536   Train Acc:  0.9087333   Validation Loss =  0.34245956   Validation Acc:  0.8814\n",
      "Iteration  302 : Train Loss =  0.2573801   Train Acc:  0.90898335   Validation Loss =  0.34260997   Validation Acc:  0.8791\n",
      "Iteration  303 : Train Loss =  0.257845   Train Acc:  0.9081333   Validation Loss =  0.34322807   Validation Acc:  0.8812\n",
      "Iteration  304 : Train Loss =  0.2571418   Train Acc:  0.9086667   Validation Loss =  0.34324828   Validation Acc:  0.8772\n",
      "Iteration  305 : Train Loss =  0.25631887   Train Acc:  0.90865   Validation Loss =  0.34236637   Validation Acc:  0.8811\n",
      "Iteration  306 : Train Loss =  0.25635383   Train Acc:  0.9094   Validation Loss =  0.34264588   Validation Acc:  0.8789\n",
      "Iteration  307 : Train Loss =  0.25703454   Train Acc:  0.90816665   Validation Loss =  0.34407592   Validation Acc:  0.8805\n",
      "Iteration  308 : Train Loss =  0.25724006   Train Acc:  0.90868336   Validation Loss =  0.3436192   Validation Acc:  0.8788\n",
      "Iteration  309 : Train Loss =  0.25738227   Train Acc:  0.90811664   Validation Loss =  0.34483567   Validation Acc:  0.8793\n",
      "Iteration  310 : Train Loss =  0.25707528   Train Acc:  0.9086667   Validation Loss =  0.34421128   Validation Acc:  0.8781\n",
      "Iteration  311 : Train Loss =  0.2583839   Train Acc:  0.9077333   Validation Loss =  0.3458373   Validation Acc:  0.8793\n",
      "Iteration  312 : Train Loss =  0.25715056   Train Acc:  0.90793335   Validation Loss =  0.3450924   Validation Acc:  0.8764\n",
      "Iteration  313 : Train Loss =  0.25553682   Train Acc:  0.90868336   Validation Loss =  0.34348544   Validation Acc:  0.8811\n",
      "Iteration  314 : Train Loss =  0.25306848   Train Acc:  0.91015   Validation Loss =  0.34103435   Validation Acc:  0.8802\n",
      "Iteration  315 : Train Loss =  0.2519352   Train Acc:  0.91065   Validation Loss =  0.3406151   Validation Acc:  0.8808\n",
      "Iteration  316 : Train Loss =  0.25129265   Train Acc:  0.9107   Validation Loss =  0.3396168   Validation Acc:  0.8809\n",
      "Iteration  317 : Train Loss =  0.25078303   Train Acc:  0.9116   Validation Loss =  0.33974788   Validation Acc:  0.8797\n",
      "Iteration  318 : Train Loss =  0.25084457   Train Acc:  0.91076666   Validation Loss =  0.34012616   Validation Acc:  0.8819\n",
      "Iteration  319 : Train Loss =  0.2513615   Train Acc:  0.91111666   Validation Loss =  0.34046862   Validation Acc:  0.8794\n",
      "Iteration  320 : Train Loss =  0.25213447   Train Acc:  0.90975   Validation Loss =  0.34224737   Validation Acc:  0.8801\n",
      "Iteration  321 : Train Loss =  0.2519974   Train Acc:  0.91036665   Validation Loss =  0.34175664   Validation Acc:  0.8792\n",
      "Iteration  322 : Train Loss =  0.2517553   Train Acc:  0.91006666   Validation Loss =  0.34227505   Validation Acc:  0.8812\n",
      "Iteration  323 : Train Loss =  0.25061825   Train Acc:  0.9112   Validation Loss =  0.34115744   Validation Acc:  0.8795\n",
      "Iteration  324 : Train Loss =  0.24990436   Train Acc:  0.91103333   Validation Loss =  0.3407794   Validation Acc:  0.8818\n",
      "Iteration  325 : Train Loss =  0.24867949   Train Acc:  0.9116667   Validation Loss =  0.3397501   Validation Acc:  0.8795\n",
      "Iteration  326 : Train Loss =  0.24755788   Train Acc:  0.91213334   Validation Loss =  0.33889064   Validation Acc:  0.8823\n",
      "Iteration  327 : Train Loss =  0.24659294   Train Acc:  0.91251665   Validation Loss =  0.33804184   Validation Acc:  0.8796\n",
      "Iteration  328 : Train Loss =  0.24615824   Train Acc:  0.9131167   Validation Loss =  0.337895   Validation Acc:  0.8802\n",
      "Iteration  329 : Train Loss =  0.24616507   Train Acc:  0.91255   Validation Loss =  0.33819368   Validation Acc:  0.8815\n",
      "Iteration  330 : Train Loss =  0.24624716   Train Acc:  0.91291666   Validation Loss =  0.33853853   Validation Acc:  0.8799\n",
      "Iteration  331 : Train Loss =  0.24634407   Train Acc:  0.9127833   Validation Loss =  0.33905908   Validation Acc:  0.8822\n",
      "Iteration  332 : Train Loss =  0.24631093   Train Acc:  0.91263336   Validation Loss =  0.339208   Validation Acc:  0.8796\n",
      "Iteration  333 : Train Loss =  0.24679466   Train Acc:  0.912   Validation Loss =  0.3402338   Validation Acc:  0.882\n",
      "Iteration  334 : Train Loss =  0.24697043   Train Acc:  0.91221666   Validation Loss =  0.3404334   Validation Acc:  0.8799\n",
      "Iteration  335 : Train Loss =  0.24762012   Train Acc:  0.9112333   Validation Loss =  0.34180805   Validation Acc:  0.881\n",
      "Iteration  336 : Train Loss =  0.24706449   Train Acc:  0.91205   Validation Loss =  0.34095907   Validation Acc:  0.8797\n",
      "Iteration  337 : Train Loss =  0.24683139   Train Acc:  0.91145   Validation Loss =  0.34156698   Validation Acc:  0.8812\n",
      "Iteration  338 : Train Loss =  0.24513291   Train Acc:  0.91288334   Validation Loss =  0.33954838   Validation Acc:  0.8806\n",
      "Iteration  339 : Train Loss =  0.24397343   Train Acc:  0.91325   Validation Loss =  0.33888835   Validation Acc:  0.8822\n",
      "Iteration  340 : Train Loss =  0.24264473   Train Acc:  0.91436666   Validation Loss =  0.3377478   Validation Acc:  0.8802\n",
      "Iteration  341 : Train Loss =  0.2417402   Train Acc:  0.91396666   Validation Loss =  0.33693457   Validation Acc:  0.8822\n",
      "Iteration  342 : Train Loss =  0.24113902   Train Acc:  0.91445   Validation Loss =  0.33687794   Validation Acc:  0.8805\n",
      "Iteration  343 : Train Loss =  0.24086915   Train Acc:  0.91463333   Validation Loss =  0.33669016   Validation Acc:  0.8814\n",
      "Iteration  344 : Train Loss =  0.24092242   Train Acc:  0.9142333   Validation Loss =  0.33722472   Validation Acc:  0.8824\n",
      "Iteration  345 : Train Loss =  0.24115579   Train Acc:  0.91461664   Validation Loss =  0.33766893   Validation Acc:  0.8803\n",
      "Iteration  346 : Train Loss =  0.24176265   Train Acc:  0.91395   Validation Loss =  0.3387077   Validation Acc:  0.882\n",
      "Iteration  347 : Train Loss =  0.24201085   Train Acc:  0.9140667   Validation Loss =  0.33916992   Validation Acc:  0.8801\n",
      "Iteration  348 : Train Loss =  0.24288046   Train Acc:  0.9130833   Validation Loss =  0.3405722   Validation Acc:  0.8813\n",
      "Iteration  349 : Train Loss =  0.24245808   Train Acc:  0.9134667   Validation Loss =  0.3401799   Validation Acc:  0.8797\n",
      "Iteration  350 : Train Loss =  0.24281383   Train Acc:  0.91316664   Validation Loss =  0.3411787   Validation Acc:  0.8812\n",
      "Iteration  351 : Train Loss =  0.24127588   Train Acc:  0.9142   Validation Loss =  0.33941206   Validation Acc:  0.8803\n",
      "Iteration  352 : Train Loss =  0.24032074   Train Acc:  0.91436666   Validation Loss =  0.33909965   Validation Acc:  0.8822\n",
      "Iteration  353 : Train Loss =  0.23866735   Train Acc:  0.91548336   Validation Loss =  0.33720097   Validation Acc:  0.8814\n",
      "Iteration  354 : Train Loss =  0.23744804   Train Acc:  0.9155833   Validation Loss =  0.33655748   Validation Acc:  0.8822\n",
      "Iteration  355 : Train Loss =  0.23662913   Train Acc:  0.9159667   Validation Loss =  0.33573055   Validation Acc:  0.8814\n",
      "Iteration  356 : Train Loss =  0.23622824   Train Acc:  0.9166167   Validation Loss =  0.33578348   Validation Acc:  0.8804\n",
      "Iteration  357 : Train Loss =  0.23620111   Train Acc:  0.91611665   Validation Loss =  0.33603257   Validation Acc:  0.8823\n",
      "Iteration  358 : Train Loss =  0.23636228   Train Acc:  0.9167167   Validation Loss =  0.33647987   Validation Acc:  0.8813\n",
      "Iteration  359 : Train Loss =  0.23679566   Train Acc:  0.91578335   Validation Loss =  0.3374099   Validation Acc:  0.8821\n",
      "Iteration  360 : Train Loss =  0.23697011   Train Acc:  0.9159667   Validation Loss =  0.3376069   Validation Acc:  0.8813\n",
      "Iteration  361 : Train Loss =  0.23762456   Train Acc:  0.91548336   Validation Loss =  0.3390588   Validation Acc:  0.8808\n",
      "Iteration  362 : Train Loss =  0.23747233   Train Acc:  0.9152333   Validation Loss =  0.3385515   Validation Acc:  0.8814\n",
      "Iteration  363 : Train Loss =  0.2378014   Train Acc:  0.91501665   Validation Loss =  0.33988297   Validation Acc:  0.8813\n",
      "Iteration  364 : Train Loss =  0.2367737   Train Acc:  0.91548336   Validation Loss =  0.33829284   Validation Acc:  0.8812\n",
      "Iteration  365 : Train Loss =  0.23580258   Train Acc:  0.91625   Validation Loss =  0.33822936   Validation Acc:  0.8818\n",
      "Iteration  366 : Train Loss =  0.23422581   Train Acc:  0.91685   Validation Loss =  0.33628392   Validation Acc:  0.882\n",
      "Iteration  367 : Train Loss =  0.23289011   Train Acc:  0.91725   Validation Loss =  0.33558795   Validation Acc:  0.8815\n",
      "Iteration  368 : Train Loss =  0.23198424   Train Acc:  0.91791666   Validation Loss =  0.33475688   Validation Acc:  0.8821\n",
      "Iteration  369 : Train Loss =  0.23158403   Train Acc:  0.9180667   Validation Loss =  0.3346973   Validation Acc:  0.8815\n",
      "Iteration  370 : Train Loss =  0.23161548   Train Acc:  0.9180167   Validation Loss =  0.3351506   Validation Acc:  0.8815\n",
      "Iteration  371 : Train Loss =  0.23187475   Train Acc:  0.91791666   Validation Loss =  0.33549428   Validation Acc:  0.882\n",
      "Iteration  372 : Train Loss =  0.23244727   Train Acc:  0.91758335   Validation Loss =  0.33674276   Validation Acc:  0.8815\n",
      "Iteration  373 : Train Loss =  0.23280297   Train Acc:  0.91751665   Validation Loss =  0.3370352   Validation Acc:  0.8816\n",
      "Iteration  374 : Train Loss =  0.2336996   Train Acc:  0.9166833   Validation Loss =  0.3387727   Validation Acc:  0.8813\n",
      "Iteration  375 : Train Loss =  0.23357692   Train Acc:  0.9168   Validation Loss =  0.33843824   Validation Acc:  0.8813\n",
      "Iteration  376 : Train Loss =  0.23414677   Train Acc:  0.9163   Validation Loss =  0.3398313   Validation Acc:  0.881\n",
      "Iteration  377 : Train Loss =  0.23274326   Train Acc:  0.91718334   Validation Loss =  0.3381486   Validation Acc:  0.8813\n",
      "Iteration  378 : Train Loss =  0.23191471   Train Acc:  0.9173833   Validation Loss =  0.3378689   Validation Acc:  0.8822\n",
      "Iteration  379 : Train Loss =  0.2301321   Train Acc:  0.91865   Validation Loss =  0.33609083   Validation Acc:  0.8814\n",
      "Iteration  380 : Train Loss =  0.22922008   Train Acc:  0.91863334   Validation Loss =  0.33530182   Validation Acc:  0.8824\n",
      "Iteration  381 : Train Loss =  0.22915176   Train Acc:  0.91896665   Validation Loss =  0.3359017   Validation Acc:  0.8803\n",
      "Iteration  382 : Train Loss =  0.23040536   Train Acc:  0.9175   Validation Loss =  0.33671162   Validation Acc:  0.8809\n",
      "Iteration  383 : Train Loss =  0.23216319   Train Acc:  0.9173833   Validation Loss =  0.33987945   Validation Acc:  0.8793\n",
      "Iteration  384 : Train Loss =  0.23620193   Train Acc:  0.9145833   Validation Loss =  0.34285834   Validation Acc:  0.8788\n",
      "Iteration  385 : Train Loss =  0.23567113   Train Acc:  0.9155333   Validation Loss =  0.34423864   Validation Acc:  0.8782\n",
      "Iteration  386 : Train Loss =  0.23520726   Train Acc:  0.91473335   Validation Loss =  0.3423393   Validation Acc:  0.8791\n",
      "Iteration  387 : Train Loss =  0.22885297   Train Acc:  0.9191167   Validation Loss =  0.33733466   Validation Acc:  0.8791\n",
      "Iteration  388 : Train Loss =  0.2259663   Train Acc:  0.92015   Validation Loss =  0.33415392   Validation Acc:  0.8825\n",
      "Iteration  389 : Train Loss =  0.22781317   Train Acc:  0.91893333   Validation Loss =  0.33602855   Validation Acc:  0.8828\n",
      "Iteration  390 : Train Loss =  0.22990274   Train Acc:  0.9180833   Validation Loss =  0.33953682   Validation Acc:  0.881\n",
      "Iteration  391 : Train Loss =  0.22941469   Train Acc:  0.91751665   Validation Loss =  0.33804452   Validation Acc:  0.881\n",
      "Iteration  392 : Train Loss =  0.22571133   Train Acc:  0.92018336   Validation Loss =  0.33547026   Validation Acc:  0.8818\n",
      "Iteration  393 : Train Loss =  0.22422849   Train Acc:  0.9209   Validation Loss =  0.33392242   Validation Acc:  0.8829\n",
      "Iteration  394 : Train Loss =  0.22560744   Train Acc:  0.91945   Validation Loss =  0.3353703   Validation Acc:  0.8812\n",
      "Iteration  395 : Train Loss =  0.2263678   Train Acc:  0.9202   Validation Loss =  0.33717987   Validation Acc:  0.8801\n",
      "Iteration  396 : Train Loss =  0.22551171   Train Acc:  0.9195833   Validation Loss =  0.33576572   Validation Acc:  0.8817\n",
      "Iteration  397 : Train Loss =  0.22310114   Train Acc:  0.92153335   Validation Loss =  0.3343095   Validation Acc:  0.881\n",
      "Iteration  398 : Train Loss =  0.22225553   Train Acc:  0.9216667   Validation Loss =  0.33344322   Validation Acc:  0.8825\n",
      "Iteration  399 : Train Loss =  0.22305156   Train Acc:  0.9206667   Validation Loss =  0.33435944   Validation Acc:  0.8824\n",
      "Iteration  400 : Train Loss =  0.2234832   Train Acc:  0.9213833   Validation Loss =  0.335617   Validation Acc:  0.881\n",
      "Iteration  401 : Train Loss =  0.22301677   Train Acc:  0.92055   Validation Loss =  0.33480713   Validation Acc:  0.8827\n",
      "Iteration  402 : Train Loss =  0.22145306   Train Acc:  0.92218333   Validation Loss =  0.33403063   Validation Acc:  0.8807\n",
      "Iteration  403 : Train Loss =  0.22048025   Train Acc:  0.92223334   Validation Loss =  0.33294764   Validation Acc:  0.8824\n",
      "Iteration  404 : Train Loss =  0.22049886   Train Acc:  0.92191666   Validation Loss =  0.33329487   Validation Acc:  0.8828\n",
      "Iteration  405 : Train Loss =  0.22084613   Train Acc:  0.92245   Validation Loss =  0.33422318   Validation Acc:  0.8813\n",
      "Iteration  406 : Train Loss =  0.22086829   Train Acc:  0.9217   Validation Loss =  0.33415103   Validation Acc:  0.8821\n",
      "Iteration  407 : Train Loss =  0.2200573   Train Acc:  0.9228167   Validation Loss =  0.33401904   Validation Acc:  0.8805\n",
      "Iteration  408 : Train Loss =  0.21921748   Train Acc:  0.92225   Validation Loss =  0.33308002   Validation Acc:  0.8822\n",
      "Iteration  409 : Train Loss =  0.21881478   Train Acc:  0.92266667   Validation Loss =  0.33313608   Validation Acc:  0.8832\n",
      "Iteration  410 : Train Loss =  0.21915172   Train Acc:  0.9227833   Validation Loss =  0.33380952   Validation Acc:  0.8824\n",
      "Iteration  411 : Train Loss =  0.2201811   Train Acc:  0.92205   Validation Loss =  0.3349623   Validation Acc:  0.8832\n",
      "Iteration  412 : Train Loss =  0.22266647   Train Acc:  0.9209167   Validation Loss =  0.33815327   Validation Acc:  0.8805\n",
      "Iteration  413 : Train Loss =  0.22628914   Train Acc:  0.9183667   Validation Loss =  0.34182903   Validation Acc:  0.8798\n",
      "Iteration  414 : Train Loss =  0.23737377   Train Acc:  0.91345   Validation Loss =  0.3539288   Validation Acc:  0.8774\n",
      "Iteration  415 : Train Loss =  0.24012235   Train Acc:  0.91188335   Validation Loss =  0.35706532   Validation Acc:  0.872\n",
      "Iteration  416 : Train Loss =  0.2482278   Train Acc:  0.90828335   Validation Loss =  0.36520317   Validation Acc:  0.8718\n",
      "Iteration  417 : Train Loss =  0.23079565   Train Acc:  0.9163167   Validation Loss =  0.34800547   Validation Acc:  0.8782\n",
      "Iteration  418 : Train Loss =  0.22010061   Train Acc:  0.92095   Validation Loss =  0.33571213   Validation Acc:  0.8839\n",
      "Iteration  419 : Train Loss =  0.22462277   Train Acc:  0.91973335   Validation Loss =  0.34141365   Validation Acc:  0.8808\n",
      "Iteration  420 : Train Loss =  0.22793575   Train Acc:  0.91745   Validation Loss =  0.34469903   Validation Acc:  0.8784\n",
      "Iteration  421 : Train Loss =  0.22363307   Train Acc:  0.91955   Validation Loss =  0.33991674   Validation Acc:  0.8804\n",
      "Iteration  422 : Train Loss =  0.21650295   Train Acc:  0.9235167   Validation Loss =  0.33344737   Validation Acc:  0.8819\n",
      "Iteration  423 : Train Loss =  0.22060224   Train Acc:  0.9211   Validation Loss =  0.33705357   Validation Acc:  0.8824\n",
      "Iteration  424 : Train Loss =  0.2266456   Train Acc:  0.9181333   Validation Loss =  0.34467635   Validation Acc:  0.879\n",
      "Iteration  425 : Train Loss =  0.21804887   Train Acc:  0.92211664   Validation Loss =  0.3361624   Validation Acc:  0.8816\n",
      "Iteration  426 : Train Loss =  0.21506566   Train Acc:  0.9236   Validation Loss =  0.3330589   Validation Acc:  0.8828\n",
      "Iteration  427 : Train Loss =  0.22093105   Train Acc:  0.9210333   Validation Loss =  0.34010857   Validation Acc:  0.8817\n",
      "Iteration  428 : Train Loss =  0.21919264   Train Acc:  0.9213167   Validation Loss =  0.3383636   Validation Acc:  0.8824\n",
      "Iteration  429 : Train Loss =  0.21420462   Train Acc:  0.92473334   Validation Loss =  0.3337517   Validation Acc:  0.8838\n",
      "Iteration  430 : Train Loss =  0.2139966   Train Acc:  0.92446667   Validation Loss =  0.33393037   Validation Acc:  0.8843\n",
      "Iteration  431 : Train Loss =  0.21659327   Train Acc:  0.92301667   Validation Loss =  0.3366542   Validation Acc:  0.8828\n",
      "Iteration  432 : Train Loss =  0.21613991   Train Acc:  0.9237667   Validation Loss =  0.33651865   Validation Acc:  0.8831\n",
      "Iteration  433 : Train Loss =  0.21272677   Train Acc:  0.92503333   Validation Loss =  0.33304667   Validation Acc:  0.8834\n",
      "Iteration  434 : Train Loss =  0.2130393   Train Acc:  0.9246   Validation Loss =  0.33380115   Validation Acc:  0.8822\n",
      "Iteration  435 : Train Loss =  0.21473184   Train Acc:  0.92408335   Validation Loss =  0.3360545   Validation Acc:  0.8832\n",
      "Iteration  436 : Train Loss =  0.21315764   Train Acc:  0.9243   Validation Loss =  0.33443773   Validation Acc:  0.8821\n",
      "Iteration  437 : Train Loss =  0.21125156   Train Acc:  0.9256833   Validation Loss =  0.33293414   Validation Acc:  0.884\n",
      "Iteration  438 : Train Loss =  0.21137626   Train Acc:  0.92516667   Validation Loss =  0.33347437   Validation Acc:  0.8846\n",
      "Iteration  439 : Train Loss =  0.21232688   Train Acc:  0.92505   Validation Loss =  0.3346445   Validation Acc:  0.8814\n",
      "Iteration  440 : Train Loss =  0.21177928   Train Acc:  0.9253167   Validation Loss =  0.3344768   Validation Acc:  0.8838\n",
      "Iteration  441 : Train Loss =  0.21011354   Train Acc:  0.9256833   Validation Loss =  0.33308482   Validation Acc:  0.8828\n",
      "Iteration  442 : Train Loss =  0.2097151   Train Acc:  0.9261   Validation Loss =  0.3327698   Validation Acc:  0.8832\n",
      "Iteration  443 : Train Loss =  0.21022855   Train Acc:  0.9256667   Validation Loss =  0.33389118   Validation Acc:  0.8838\n",
      "Iteration  444 : Train Loss =  0.21008575   Train Acc:  0.92581666   Validation Loss =  0.33389327   Validation Acc:  0.8832\n",
      "Iteration  445 : Train Loss =  0.20927295   Train Acc:  0.92648333   Validation Loss =  0.33309764   Validation Acc:  0.8848\n",
      "Iteration  446 : Train Loss =  0.20853543   Train Acc:  0.9267333   Validation Loss =  0.33296493   Validation Acc:  0.8841\n",
      "Iteration  447 : Train Loss =  0.20832618   Train Acc:  0.9264333   Validation Loss =  0.33259103   Validation Acc:  0.8838\n",
      "Iteration  448 : Train Loss =  0.20837961   Train Acc:  0.9267667   Validation Loss =  0.3332271   Validation Acc:  0.8851\n",
      "Iteration  449 : Train Loss =  0.20809849   Train Acc:  0.92621666   Validation Loss =  0.33330417   Validation Acc:  0.8832\n",
      "Iteration  450 : Train Loss =  0.2075081   Train Acc:  0.92695   Validation Loss =  0.3327097   Validation Acc:  0.8852\n",
      "Iteration  451 : Train Loss =  0.20700762   Train Acc:  0.92725   Validation Loss =  0.33282334   Validation Acc:  0.8847\n",
      "Iteration  452 : Train Loss =  0.20680198   Train Acc:  0.9269   Validation Loss =  0.3326634   Validation Acc:  0.8839\n",
      "Iteration  453 : Train Loss =  0.206675   Train Acc:  0.9270333   Validation Loss =  0.3329653   Validation Acc:  0.8858\n",
      "Iteration  454 : Train Loss =  0.20642799   Train Acc:  0.92716664   Validation Loss =  0.33290783   Validation Acc:  0.8844\n",
      "Iteration  455 : Train Loss =  0.20605728   Train Acc:  0.92768335   Validation Loss =  0.33276117   Validation Acc:  0.8858\n",
      "Iteration  456 : Train Loss =  0.20556407   Train Acc:  0.92775   Validation Loss =  0.33271897   Validation Acc:  0.8851\n",
      "Iteration  457 : Train Loss =  0.20515727   Train Acc:  0.9277667   Validation Loss =  0.33249077   Validation Acc:  0.8847\n",
      "Iteration  458 : Train Loss =  0.20494995   Train Acc:  0.92801666   Validation Loss =  0.33273607   Validation Acc:  0.8856\n",
      "Iteration  459 : Train Loss =  0.20482679   Train Acc:  0.92751664   Validation Loss =  0.3328325   Validation Acc:  0.8845\n",
      "Iteration  460 : Train Loss =  0.20461059   Train Acc:  0.92796665   Validation Loss =  0.33285108   Validation Acc:  0.8855\n",
      "Iteration  461 : Train Loss =  0.20423834   Train Acc:  0.9278833   Validation Loss =  0.33285025   Validation Acc:  0.8846\n",
      "Iteration  462 : Train Loss =  0.20379844   Train Acc:  0.9283   Validation Loss =  0.33252528   Validation Acc:  0.885\n",
      "Iteration  463 : Train Loss =  0.20338824   Train Acc:  0.92873335   Validation Loss =  0.33258855   Validation Acc:  0.8853\n",
      "Iteration  464 : Train Loss =  0.20310986   Train Acc:  0.92868334   Validation Loss =  0.3326001   Validation Acc:  0.885\n",
      "Iteration  465 : Train Loss =  0.20294401   Train Acc:  0.92855   Validation Loss =  0.3327087   Validation Acc:  0.8855\n",
      "Iteration  466 : Train Loss =  0.20276995   Train Acc:  0.92873335   Validation Loss =  0.33295363   Validation Acc:  0.8849\n",
      "Iteration  467 : Train Loss =  0.2025113   Train Acc:  0.9288667   Validation Loss =  0.33283636   Validation Acc:  0.8849\n",
      "Iteration  468 : Train Loss =  0.20217708   Train Acc:  0.92905   Validation Loss =  0.33287364   Validation Acc:  0.8851\n",
      "Iteration  469 : Train Loss =  0.20181587   Train Acc:  0.929   Validation Loss =  0.33281335   Validation Acc:  0.8855\n",
      "Iteration  470 : Train Loss =  0.20147529   Train Acc:  0.92913336   Validation Loss =  0.3327763   Validation Acc:  0.8853\n",
      "Iteration  471 : Train Loss =  0.20116687   Train Acc:  0.92931664   Validation Loss =  0.3328748   Validation Acc:  0.8854\n",
      "Iteration  472 : Train Loss =  0.2008968   Train Acc:  0.92945   Validation Loss =  0.33279884   Validation Acc:  0.8854\n",
      "Iteration  473 : Train Loss =  0.20066941   Train Acc:  0.92948335   Validation Loss =  0.33287817   Validation Acc:  0.8847\n",
      "Iteration  474 : Train Loss =  0.20050263   Train Acc:  0.92983335   Validation Loss =  0.33302942   Validation Acc:  0.8849\n",
      "Iteration  475 : Train Loss =  0.20051904   Train Acc:  0.92945   Validation Loss =  0.33327687   Validation Acc:  0.8844\n",
      "Iteration  476 : Train Loss =  0.20098522   Train Acc:  0.93011665   Validation Loss =  0.33434647   Validation Acc:  0.8835\n",
      "Iteration  477 : Train Loss =  0.20279966   Train Acc:  0.92768335   Validation Loss =  0.33610854   Validation Acc:  0.8841\n",
      "Iteration  478 : Train Loss =  0.20692684   Train Acc:  0.9272   Validation Loss =  0.34121987   Validation Acc:  0.8818\n",
      "Iteration  479 : Train Loss =  0.21779528   Train Acc:  0.9206   Validation Loss =  0.3509187   Validation Acc:  0.8792\n",
      "Iteration  480 : Train Loss =  0.22268848   Train Acc:  0.91906667   Validation Loss =  0.35814282   Validation Acc:  0.877\n",
      "Iteration  481 : Train Loss =  0.22851898   Train Acc:  0.91691667   Validation Loss =  0.3621368   Validation Acc:  0.8743\n",
      "Iteration  482 : Train Loss =  0.20691222   Train Acc:  0.92665   Validation Loss =  0.34147656   Validation Acc:  0.8823\n",
      "Iteration  483 : Train Loss =  0.20239905   Train Acc:  0.92843336   Validation Loss =  0.33660662   Validation Acc:  0.8835\n",
      "Iteration  484 : Train Loss =  0.21454951   Train Acc:  0.92211664   Validation Loss =  0.3487336   Validation Acc:  0.8793\n",
      "Iteration  485 : Train Loss =  0.20680371   Train Acc:  0.92695   Validation Loss =  0.34224474   Validation Acc:  0.8825\n",
      "Iteration  486 : Train Loss =  0.1989924   Train Acc:  0.9299333   Validation Loss =  0.33377904   Validation Acc:  0.8856\n",
      "Iteration  487 : Train Loss =  0.20323272   Train Acc:  0.9266   Validation Loss =  0.3381141   Validation Acc:  0.8831\n",
      "Iteration  488 : Train Loss =  0.20455436   Train Acc:  0.9278333   Validation Loss =  0.34080455   Validation Acc:  0.8812\n",
      "Iteration  489 : Train Loss =  0.19959716   Train Acc:  0.92868334   Validation Loss =  0.33556774   Validation Acc:  0.8844\n",
      "Iteration  490 : Train Loss =  0.19762659   Train Acc:  0.9306   Validation Loss =  0.33421612   Validation Acc:  0.8842\n",
      "Iteration  491 : Train Loss =  0.20108438   Train Acc:  0.92948335   Validation Loss =  0.33876276   Validation Acc:  0.8832\n",
      "Iteration  492 : Train Loss =  0.2014413   Train Acc:  0.92731667   Validation Loss =  0.3390959   Validation Acc:  0.8835\n",
      "Iteration  493 : Train Loss =  0.19660111   Train Acc:  0.93086666   Validation Loss =  0.3344681   Validation Acc:  0.8843\n",
      "Iteration  494 : Train Loss =  0.19718544   Train Acc:  0.9310833   Validation Loss =  0.33526158   Validation Acc:  0.8839\n",
      "Iteration  495 : Train Loss =  0.20036061   Train Acc:  0.9283   Validation Loss =  0.33866864   Validation Acc:  0.8844\n",
      "Iteration  496 : Train Loss =  0.19729514   Train Acc:  0.93116665   Validation Loss =  0.33608148   Validation Acc:  0.8831\n",
      "Iteration  497 : Train Loss =  0.19491139   Train Acc:  0.9313833   Validation Loss =  0.33379927   Validation Acc:  0.8849\n",
      "Iteration  498 : Train Loss =  0.19656788   Train Acc:  0.9302833   Validation Loss =  0.33596364   Validation Acc:  0.8854\n",
      "Iteration  499 : Train Loss =  0.19714804   Train Acc:  0.93121666   Validation Loss =  0.33700785   Validation Acc:  0.8836\n",
      "Iteration  500 : Train Loss =  0.19524653   Train Acc:  0.931   Validation Loss =  0.3353875   Validation Acc:  0.8849\n",
      "Iteration  501 : Train Loss =  0.19375823   Train Acc:  0.9316667   Validation Loss =  0.3341898   Validation Acc:  0.8853\n",
      "Iteration  502 : Train Loss =  0.19464111   Train Acc:  0.93196666   Validation Loss =  0.33547238   Validation Acc:  0.8843\n",
      "Iteration  503 : Train Loss =  0.19522806   Train Acc:  0.93091667   Validation Loss =  0.33606166   Validation Acc:  0.8845\n",
      "Iteration  504 : Train Loss =  0.1936582   Train Acc:  0.9325   Validation Loss =  0.33485374   Validation Acc:  0.8846\n",
      "Iteration  505 : Train Loss =  0.19270174   Train Acc:  0.9324667   Validation Loss =  0.33405557   Validation Acc:  0.8844\n",
      "Iteration  506 : Train Loss =  0.19312249   Train Acc:  0.93226665   Validation Loss =  0.33457208   Validation Acc:  0.8848\n",
      "Iteration  507 : Train Loss =  0.19330727   Train Acc:  0.93265   Validation Loss =  0.33524862   Validation Acc:  0.8839\n",
      "Iteration  508 : Train Loss =  0.1926447   Train Acc:  0.93233335   Validation Loss =  0.33468866   Validation Acc:  0.8847\n",
      "Iteration  509 : Train Loss =  0.19172902   Train Acc:  0.9327667   Validation Loss =  0.33426034   Validation Acc:  0.8855\n",
      "Iteration  510 : Train Loss =  0.19165957   Train Acc:  0.93303335   Validation Loss =  0.3345311   Validation Acc:  0.8855\n",
      "Iteration  511 : Train Loss =  0.19199179   Train Acc:  0.9325333   Validation Loss =  0.33520216   Validation Acc:  0.8845\n",
      "Iteration  512 : Train Loss =  0.19170934   Train Acc:  0.9332833   Validation Loss =  0.33532485   Validation Acc:  0.8857\n",
      "Iteration  513 : Train Loss =  0.19107734   Train Acc:  0.9331   Validation Loss =  0.33468324   Validation Acc:  0.8857\n",
      "Iteration  514 : Train Loss =  0.19079942   Train Acc:  0.93333334   Validation Loss =  0.33466992   Validation Acc:  0.8861\n",
      "Iteration  515 : Train Loss =  0.19126113   Train Acc:  0.933   Validation Loss =  0.33562094   Validation Acc:  0.8841\n",
      "Iteration  516 : Train Loss =  0.19195144   Train Acc:  0.93263334   Validation Loss =  0.33641776   Validation Acc:  0.8854\n",
      "Iteration  517 : Train Loss =  0.19309883   Train Acc:  0.93215   Validation Loss =  0.33830592   Validation Acc:  0.8839\n",
      "Iteration  518 : Train Loss =  0.19462448   Train Acc:  0.93095   Validation Loss =  0.3397019   Validation Acc:  0.8834\n",
      "Iteration  519 : Train Loss =  0.19950059   Train Acc:  0.92855   Validation Loss =  0.3452662   Validation Acc:  0.88\n",
      "Iteration  520 : Train Loss =  0.20281845   Train Acc:  0.92715   Validation Loss =  0.34887508   Validation Acc:  0.8803\n",
      "Iteration  521 : Train Loss =  0.20884177   Train Acc:  0.92365   Validation Loss =  0.35557204   Validation Acc:  0.8775\n",
      "Iteration  522 : Train Loss =  0.20251478   Train Acc:  0.92728335   Validation Loss =  0.34905776   Validation Acc:  0.881\n",
      "Iteration  523 : Train Loss =  0.19622622   Train Acc:  0.93023336   Validation Loss =  0.34262052   Validation Acc:  0.883\n",
      "Iteration  524 : Train Loss =  0.18925543   Train Acc:  0.9332   Validation Loss =  0.3355226   Validation Acc:  0.8868\n",
      "Iteration  525 : Train Loss =  0.1896126   Train Acc:  0.9335667   Validation Loss =  0.33638453   Validation Acc:  0.886\n",
      "Iteration  526 : Train Loss =  0.19411752   Train Acc:  0.93121666   Validation Loss =  0.34149173   Validation Acc:  0.8836\n",
      "Iteration  527 : Train Loss =  0.19510384   Train Acc:  0.93041664   Validation Loss =  0.3423761   Validation Acc:  0.8834\n",
      "Iteration  528 : Train Loss =  0.19374557   Train Acc:  0.9313833   Validation Loss =  0.34150004   Validation Acc:  0.8821\n",
      "Iteration  529 : Train Loss =  0.18929256   Train Acc:  0.93341666   Validation Loss =  0.33690542   Validation Acc:  0.8856\n",
      "Iteration  530 : Train Loss =  0.18770204   Train Acc:  0.93485   Validation Loss =  0.33600014   Validation Acc:  0.8855\n",
      "Iteration  531 : Train Loss =  0.1889931   Train Acc:  0.93326664   Validation Loss =  0.3376669   Validation Acc:  0.8851\n",
      "Iteration  532 : Train Loss =  0.1903768   Train Acc:  0.93271667   Validation Loss =  0.33951274   Validation Acc:  0.8842\n",
      "Iteration  533 : Train Loss =  0.19049825   Train Acc:  0.93231666   Validation Loss =  0.34009266   Validation Acc:  0.8842\n",
      "Iteration  534 : Train Loss =  0.18826592   Train Acc:  0.93383336   Validation Loss =  0.33755523   Validation Acc:  0.8861\n",
      "Iteration  535 : Train Loss =  0.18671402   Train Acc:  0.93516666   Validation Loss =  0.3368498   Validation Acc:  0.8849\n",
      "Iteration  536 : Train Loss =  0.18640907   Train Acc:  0.9346833   Validation Loss =  0.33626807   Validation Acc:  0.8857\n",
      "Iteration  537 : Train Loss =  0.18676913   Train Acc:  0.93451667   Validation Loss =  0.3373457   Validation Acc:  0.8866\n",
      "Iteration  538 : Train Loss =  0.18721208   Train Acc:  0.9342   Validation Loss =  0.33826917   Validation Acc:  0.885\n",
      "Iteration  539 : Train Loss =  0.18677677   Train Acc:  0.9345   Validation Loss =  0.3376256   Validation Acc:  0.886\n",
      "Iteration  540 : Train Loss =  0.18635316   Train Acc:  0.9349167   Validation Loss =  0.33817497   Validation Acc:  0.8846\n",
      "Iteration  541 : Train Loss =  0.18562888   Train Acc:  0.93521667   Validation Loss =  0.33694795   Validation Acc:  0.8862\n",
      "Iteration  542 : Train Loss =  0.18476112   Train Acc:  0.93523335   Validation Loss =  0.33702323   Validation Acc:  0.8853\n",
      "Iteration  543 : Train Loss =  0.18401913   Train Acc:  0.9356833   Validation Loss =  0.33630756   Validation Acc:  0.8855\n",
      "Iteration  544 : Train Loss =  0.18392465   Train Acc:  0.93588334   Validation Loss =  0.3364772   Validation Acc:  0.8865\n",
      "Iteration  545 : Train Loss =  0.1844655   Train Acc:  0.9359   Validation Loss =  0.3378142   Validation Acc:  0.885\n",
      "Iteration  546 : Train Loss =  0.1846286   Train Acc:  0.9357   Validation Loss =  0.3376395   Validation Acc:  0.8863\n",
      "Iteration  547 : Train Loss =  0.18415917   Train Acc:  0.93595   Validation Loss =  0.33809885   Validation Acc:  0.885\n",
      "Iteration  548 : Train Loss =  0.18324225   Train Acc:  0.93631667   Validation Loss =  0.33672723   Validation Acc:  0.8859\n",
      "Iteration  549 : Train Loss =  0.18261628   Train Acc:  0.9364667   Validation Loss =  0.33672547   Validation Acc:  0.8851\n",
      "Iteration  550 : Train Loss =  0.18232991   Train Acc:  0.9363833   Validation Loss =  0.33669662   Validation Acc:  0.8859\n",
      "Iteration  551 : Train Loss =  0.18222658   Train Acc:  0.93625   Validation Loss =  0.3367048   Validation Acc:  0.8851\n",
      "Iteration  552 : Train Loss =  0.18202357   Train Acc:  0.93663335   Validation Loss =  0.3371399   Validation Acc:  0.8853\n",
      "Iteration  553 : Train Loss =  0.18172123   Train Acc:  0.93666667   Validation Loss =  0.33666554   Validation Acc:  0.8858\n",
      "Iteration  554 : Train Loss =  0.18149154   Train Acc:  0.9367167   Validation Loss =  0.33705267   Validation Acc:  0.885\n",
      "Iteration  555 : Train Loss =  0.18142892   Train Acc:  0.93705   Validation Loss =  0.33715683   Validation Acc:  0.8866\n",
      "Iteration  556 : Train Loss =  0.18140985   Train Acc:  0.93693334   Validation Loss =  0.337487   Validation Acc:  0.8852\n",
      "Iteration  557 : Train Loss =  0.18124688   Train Acc:  0.93666667   Validation Loss =  0.33754408   Validation Acc:  0.8863\n",
      "Iteration  558 : Train Loss =  0.18105438   Train Acc:  0.93703336   Validation Loss =  0.3377281   Validation Acc:  0.8853\n",
      "Iteration  559 : Train Loss =  0.18083546   Train Acc:  0.9371   Validation Loss =  0.33763838   Validation Acc:  0.8862\n",
      "Iteration  560 : Train Loss =  0.18075329   Train Acc:  0.93685   Validation Loss =  0.338217   Validation Acc:  0.8852\n",
      "Iteration  561 : Train Loss =  0.1805648   Train Acc:  0.9372   Validation Loss =  0.3379206   Validation Acc:  0.8855\n",
      "Iteration  562 : Train Loss =  0.18043616   Train Acc:  0.93715   Validation Loss =  0.33851725   Validation Acc:  0.8855\n",
      "Iteration  563 : Train Loss =  0.1801318   Train Acc:  0.9371167   Validation Loss =  0.3381168   Validation Acc:  0.8854\n",
      "Iteration  564 : Train Loss =  0.17995214   Train Acc:  0.93735   Validation Loss =  0.33859786   Validation Acc:  0.8859\n",
      "Iteration  565 : Train Loss =  0.17973383   Train Acc:  0.9372333   Validation Loss =  0.33830646   Validation Acc:  0.8863\n",
      "Iteration  566 : Train Loss =  0.17983572   Train Acc:  0.93725   Validation Loss =  0.33900976   Validation Acc:  0.8849\n",
      "Iteration  567 : Train Loss =  0.17987393   Train Acc:  0.9374167   Validation Loss =  0.33896074   Validation Acc:  0.886\n",
      "Iteration  568 : Train Loss =  0.18026216   Train Acc:  0.9368167   Validation Loss =  0.34000558   Validation Acc:  0.8844\n",
      "Iteration  569 : Train Loss =  0.18043421   Train Acc:  0.937   Validation Loss =  0.34012154   Validation Acc:  0.8845\n",
      "Iteration  570 : Train Loss =  0.18132755   Train Acc:  0.9364167   Validation Loss =  0.34193197   Validation Acc:  0.8848\n",
      "Iteration  571 : Train Loss =  0.18154442   Train Acc:  0.93618333   Validation Loss =  0.3417675   Validation Acc:  0.8839\n",
      "Iteration  572 : Train Loss =  0.18291317   Train Acc:  0.93516666   Validation Loss =  0.3441769   Validation Acc:  0.8843\n",
      "Iteration  573 : Train Loss =  0.18217641   Train Acc:  0.9357833   Validation Loss =  0.34288964   Validation Acc:  0.8844\n",
      "Iteration  574 : Train Loss =  0.18254198   Train Acc:  0.93535   Validation Loss =  0.34434056   Validation Acc:  0.8841\n",
      "Iteration  575 : Train Loss =  0.18036059   Train Acc:  0.9367667   Validation Loss =  0.3415117   Validation Acc:  0.884\n",
      "Iteration  576 : Train Loss =  0.17907281   Train Acc:  0.93701667   Validation Loss =  0.34090087   Validation Acc:  0.8852\n",
      "Iteration  577 : Train Loss =  0.17745902   Train Acc:  0.938   Validation Loss =  0.3393492   Validation Acc:  0.8857\n",
      "Iteration  578 : Train Loss =  0.17672643   Train Acc:  0.9383   Validation Loss =  0.3389596   Validation Acc:  0.8861\n",
      "Iteration  579 : Train Loss =  0.17667216   Train Acc:  0.9389167   Validation Loss =  0.33936733   Validation Acc:  0.885\n",
      "Iteration  580 : Train Loss =  0.17727625   Train Acc:  0.9382   Validation Loss =  0.33989903   Validation Acc:  0.885\n",
      "Iteration  581 : Train Loss =  0.178049   Train Acc:  0.9382333   Validation Loss =  0.341568   Validation Acc:  0.8838\n",
      "Iteration  582 : Train Loss =  0.1793876   Train Acc:  0.93668336   Validation Loss =  0.3427934   Validation Acc:  0.8837\n",
      "Iteration  583 : Train Loss =  0.17983972   Train Acc:  0.93738335   Validation Loss =  0.34399948   Validation Acc:  0.8841\n",
      "Iteration  584 : Train Loss =  0.18034409   Train Acc:  0.9364667   Validation Loss =  0.34418985   Validation Acc:  0.8829\n",
      "Iteration  585 : Train Loss =  0.17866771   Train Acc:  0.93785   Validation Loss =  0.34318238   Validation Acc:  0.8844\n",
      "Iteration  586 : Train Loss =  0.17673099   Train Acc:  0.93806666   Validation Loss =  0.3412396   Validation Acc:  0.8848\n",
      "Iteration  587 : Train Loss =  0.17466247   Train Acc:  0.9393833   Validation Loss =  0.3397378   Validation Acc:  0.8848\n",
      "Iteration  588 : Train Loss =  0.17383854   Train Acc:  0.93975   Validation Loss =  0.33910236   Validation Acc:  0.8856\n",
      "Iteration  589 : Train Loss =  0.1743058   Train Acc:  0.9393333   Validation Loss =  0.33985126   Validation Acc:  0.8856\n",
      "Iteration  590 : Train Loss =  0.17541204   Train Acc:  0.93948334   Validation Loss =  0.34155723   Validation Acc:  0.8846\n",
      "Iteration  591 : Train Loss =  0.17703521   Train Acc:  0.9378333   Validation Loss =  0.3432728   Validation Acc:  0.8841\n",
      "Iteration  592 : Train Loss =  0.17730169   Train Acc:  0.93868333   Validation Loss =  0.34386826   Validation Acc:  0.884\n",
      "Iteration  593 : Train Loss =  0.17764911   Train Acc:  0.93728334   Validation Loss =  0.34413984   Validation Acc:  0.8847\n",
      "Iteration  594 : Train Loss =  0.175819   Train Acc:  0.93913335   Validation Loss =  0.34294906   Validation Acc:  0.8841\n",
      "Iteration  595 : Train Loss =  0.17451966   Train Acc:  0.93841666   Validation Loss =  0.34194598   Validation Acc:  0.885\n",
      "Iteration  596 : Train Loss =  0.17412825   Train Acc:  0.93941665   Validation Loss =  0.34192798   Validation Acc:  0.8851\n",
      "Iteration  597 : Train Loss =  0.17542444   Train Acc:  0.93871665   Validation Loss =  0.34361744   Validation Acc:  0.8841\n",
      "Iteration  598 : Train Loss =  0.17732686   Train Acc:  0.93765   Validation Loss =  0.34568754   Validation Acc:  0.8835\n",
      "Iteration  599 : Train Loss =  0.18146183   Train Acc:  0.9355   Validation Loss =  0.35068908   Validation Acc:  0.8826\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    learning_rate = 0.001\n",
    "    num_Iterations = 600\n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    validation_accuracy = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    # load and prepare the training and test data\n",
    "    tr_x, tr_y, te_x, te_y = load_Prepare_Data()\n",
    "    \n",
    "    # To avoid problems related to type conversion, all data is converted to  float32 data types\n",
    "    tr_x = tf.cast(tr_x, tf.float32)\n",
    "    te_x = tf.cast(te_x, tf.float32)\n",
    "    tr_y = tf.cast(tr_y, tf.float32)\n",
    "    te_y = tf.cast(te_y, tf.float32)\n",
    "    \n",
    "    #Initialize the values of the weights and bias for the 1st hidden layer\n",
    "    w1 = tf.Variable(tf.random.normal([ 300, tr_x.shape[0]], mean=0.0, stddev=0.05))\n",
    "    b1 = tf.Variable(tf.zeros([300, 1]))\n",
    "    #Initialize the values of the weights and bias for the 2nd hidden layer\n",
    "    w2 = tf.Variable(tf.random.normal([100, 300], mean=0.0, stddev=0.05))\n",
    "    b2 = tf.Variable(tf.zeros([100, 1]))\n",
    "    #Initialize the values of the weights and bias for the SoftMax layer\n",
    "    w3 = tf.Variable(tf.random.normal([ tr_y.shape[0], 100], mean=0.0, stddev=0.05))\n",
    "    b3 = tf.Variable(tf.zeros([tr_y.shape[0], 1]))\n",
    "    \n",
    "    # Iterate the training loop\n",
    "    for i in range(num_Iterations):\n",
    "        \n",
    "        # Create an instance of Gradient Tape to monitor the forward pass to calcualte the gradients based on the training data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = forward_pass(tr_x, w1, b1, w2, b2, w3, b3)\n",
    "            currentLoss = cross_entropy(tr_y, y_pred)\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        gradients = tape.gradient(currentLoss, [w1, b1, w2, b2, w3, b3])\n",
    "        # Determine the prediction accuracy for the training data\n",
    "        accuracy = calculate_accuracy(tr_y, y_pred)\n",
    "        \n",
    "        train_accuracy.append(accuracy)\n",
    "        train_loss.append(currentLoss)\n",
    "\n",
    "        # Calculate forward pass, loss and accuracy for the valdation data\n",
    "        te_y_pred = forward_pass(te_x, w1, b1, w2, b2, w3, b3) \n",
    "        te_currentLoss = cross_entropy(te_y, te_y_pred)\n",
    "        te_accuracy = calculate_accuracy(te_y, te_y_pred) \n",
    "        validation_accuracy.append(te_accuracy)\n",
    "        validation_loss.append(te_currentLoss)\n",
    "\n",
    "        print (\"Iteration \", i, \": Train Loss = \",currentLoss.numpy(), \"  Train Acc: \", accuracy.numpy(), \"  Validation Loss = \", te_currentLoss.numpy(), \"  Validation Acc: \", te_accuracy.numpy())\n",
    "        # Update the trainable parameters using Adam Optimizer\n",
    "        adam_optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2, w3, b3]))\n",
    "    \n",
    "    # Plot the training and the validation accuracy and loss\n",
    "    plt.plot(train_accuracy, label=\"Train Acc\")\n",
    "    plt.plot(train_loss, label=\"Train Loss\")\n",
    "    plt.plot(validation_accuracy, label=\"Validation Acc\")\n",
    "    plt.plot(validation_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"No. of Iterations\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show();\n",
    "  \n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Question1_2_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
