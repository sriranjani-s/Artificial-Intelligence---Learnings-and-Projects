{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFmPrF7MZN7W"
   },
   "source": [
    "# R00182510 - Assignment1 PART A - Task 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WSLCI4UupncQ",
    "outputId": "9629660a-d793-44c6-c1c2-7801c82b918b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "# To enable TF2\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y3JBeLDeppg5",
    "outputId": "725e9ed0-002e-4252-e246-276cfb28d660"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "random.seed(182510)\n",
    "\n",
    "# load and prepare the training and test data\n",
    "def load_Prepare_Data():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "    # load the training and test data    \n",
    "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "    # reshape the feature data\n",
    "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "    # noramlise feature data\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "\n",
    "    print( \"Shape of training features \", tr_x.shape)\n",
    "    print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "    # one hot encode the training labels and get the transpose\n",
    "    tr_y = np_utils.to_categorical(tr_y,10)\n",
    "    tr_y = tr_y.T\n",
    "    print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "    # one hot encode the test labels and get the transpose\n",
    "    te_y = np_utils.to_categorical(te_y,10)\n",
    "    te_y = te_y.T\n",
    "    print (\"Shape of testing labels \", te_y.shape)\n",
    "    \n",
    "    # Reshape the training data and test data so \n",
    "    # that the features becomes the rows of the matrix\n",
    "    tr_x = tr_x.T\n",
    "    te_x = te_x.T\n",
    "\n",
    "    print(\"Reshaped training data \", tr_x.shape)\n",
    "    print(\"Reshaped test data \",te_x.shape)\n",
    "    \n",
    "    return(tr_x, tr_y, te_x, te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xy06wmFTeK7F"
   },
   "outputs": [],
   "source": [
    "# push all training data through the hidden layers and the SoftMax layer\n",
    "def forward_pass(x, w_T1, b1, w_T2, b2, w_T3, b3):\n",
    "\n",
    "    # Multiply each training example by the weights and add the bias for the 1st hidden layer\n",
    "    A1 = tf.matmul(w_T1, x) + b1\n",
    "    H1 = tf.nn.relu(A1)\n",
    "    \n",
    "    # Multiply each training example by the weights and add the bias for the 2nd hidden layer\n",
    "    A2 = tf.matmul(w_T2, H1) + b2\n",
    "    H2 = tf.nn.relu(A2)\n",
    "\n",
    "    #SoftMax activation\n",
    "    A3 = tf.matmul(w_T3, H2) + b3\n",
    "    t = tf.math.exp(A3)\n",
    "    t_sum = tf.reduce_sum(t, 0)\n",
    "    t_sum = tf.reshape(t_sum,[1, -1])\n",
    "\n",
    "    y_pred_softmax = tf.divide(t, t_sum)\n",
    "\n",
    "    return y_pred_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxUFdp7WeQKZ"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_L2Reg(y, y_pred, w1, w2, w3, alpha):\n",
    "\n",
    "   # Calculate the cross entropy error for all training data \n",
    "    cross_entropy = -(tf.reduce_sum(tf.multiply(y, tf.math.log(y_pred)), 0))\n",
    "    \n",
    "    # Calculate the L2 regularization cost\n",
    "    regularization_cost_L2 = alpha * ((tf.reduce_sum(tf.math.square(w1))) + (tf.reduce_sum(tf.math.square(w2))) + (tf.reduce_sum(tf.math.square(w3))))\n",
    "\n",
    "    # Calculate cost which is the mean cross entropy error/ average loss across all training instances along with the L2 regularization cost\n",
    "    loss = tf.reduce_mean(cross_entropy) + regularization_cost_L2\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G9z2QqSVhQAl"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_L1Reg(y, y_pred, w1, w2, w3, alpha):\n",
    "\n",
    "   # Calculate the cross entropy error for all training data \n",
    "    cross_entropy = -(tf.reduce_sum(tf.multiply(y, tf.math.log(y_pred)), 0))\n",
    "    \n",
    "    # Calculate the L1 regularization cost\n",
    "    regularization_cost_L1 = alpha * (tf.reduce_sum(tf.abs(w1)) + tf.reduce_sum(tf.abs(w2)) + tf.reduce_sum(tf.abs(w3)))\n",
    "\n",
    "    # Calculate cost which is the mean cross entropy error/ average loss across all training instances along with the L1 regularization cost\n",
    "    loss = tf.reduce_mean(cross_entropy) + regularization_cost_L1\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHKpvgB3eSu9"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, y_pred_softmax):\n",
    "\n",
    "    # Calculate the predictions in the form of a boolean array, by considering only the class with the highest probability\n",
    "    # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
    "    predictions_bool = tf.equal(tf.argmax(y_pred_softmax, 0), tf.argmax(y, 0))\n",
    "    predictions_correct = tf.cast(predictions_bool, tf.float32)\n",
    "\n",
    "    # Finally, we just determine the mean value of the correct predictions\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "  \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPtHq216Y9PY"
   },
   "source": [
    "### Implementation of L2 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aHd4zmAGeWOE",
    "outputId": "a9c3e9d4-05a0-491b-90dc-cc5a925277cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n",
      "Reshaped training data  (784, 60000)\n",
      "Reshaped test data  (784, 10000)\n",
      "Iteration  0 : Train Loss =  4.302066   Train Acc:  0.08865   Validation Loss =  4.3022394   Validation Acc:  0.0878\n",
      "Iteration  1 : Train Loss =  4.1597505   Train Acc:  0.2262   Validation Loss =  4.1607275   Validation Acc:  0.2205\n",
      "Iteration  2 : Train Loss =  4.0413666   Train Acc:  0.34323335   Validation Loss =  4.0431914   Validation Acc:  0.3338\n",
      "Iteration  3 : Train Loss =  3.9261315   Train Acc:  0.40605   Validation Loss =  3.9287057   Validation Acc:  0.3993\n",
      "Iteration  4 : Train Loss =  3.8074946   Train Acc:  0.44908333   Validation Loss =  3.8109639   Validation Acc:  0.4431\n",
      "Iteration  5 : Train Loss =  3.6836445   Train Acc:  0.48913333   Validation Loss =  3.6881213   Validation Acc:  0.4805\n",
      "Iteration  6 : Train Loss =  3.5561934   Train Acc:  0.5376667   Validation Loss =  3.5616221   Validation Acc:  0.5295\n",
      "Iteration  7 : Train Loss =  3.4280274   Train Acc:  0.57105   Validation Loss =  3.4343994   Validation Acc:  0.5659\n",
      "Iteration  8 : Train Loss =  3.3015037   Train Acc:  0.58918333   Validation Loss =  3.3086581   Validation Acc:  0.584\n",
      "Iteration  9 : Train Loss =  3.1790128   Train Acc:  0.6084667   Validation Loss =  3.186789   Validation Acc:  0.6023\n",
      "Iteration  10 : Train Loss =  3.0610077   Train Acc:  0.63638335   Validation Loss =  3.069315   Validation Acc:  0.6302\n",
      "Iteration  11 : Train Loss =  2.9470363   Train Acc:  0.6592   Validation Loss =  2.9559507   Validation Acc:  0.6501\n",
      "Iteration  12 : Train Loss =  2.8396063   Train Acc:  0.67025   Validation Loss =  2.849298   Validation Acc:  0.6621\n",
      "Iteration  13 : Train Loss =  2.7408142   Train Acc:  0.67405   Validation Loss =  2.751287   Validation Acc:  0.6653\n",
      "Iteration  14 : Train Loss =  2.650323   Train Acc:  0.6749   Validation Loss =  2.6615086   Validation Acc:  0.6657\n",
      "Iteration  15 : Train Loss =  2.567922   Train Acc:  0.67586666   Validation Loss =  2.5797021   Validation Acc:  0.6671\n",
      "Iteration  16 : Train Loss =  2.4942355   Train Acc:  0.67756665   Validation Loss =  2.5064929   Validation Acc:  0.6693\n",
      "Iteration  17 : Train Loss =  2.4287286   Train Acc:  0.67938334   Validation Loss =  2.4416714   Validation Acc:  0.6714\n",
      "Iteration  18 : Train Loss =  2.3695688   Train Acc:  0.6831333   Validation Loss =  2.3834405   Validation Acc:  0.6733\n",
      "Iteration  19 : Train Loss =  2.3153923   Train Acc:  0.68745   Validation Loss =  2.3304596   Validation Acc:  0.675\n",
      "Iteration  20 : Train Loss =  2.265949   Train Acc:  0.6925167   Validation Loss =  2.282353   Validation Acc:  0.6806\n",
      "Iteration  21 : Train Loss =  2.2206805   Train Acc:  0.69913334   Validation Loss =  2.2383175   Validation Acc:  0.6847\n",
      "Iteration  22 : Train Loss =  2.1782403   Train Acc:  0.7054167   Validation Loss =  2.196686   Validation Acc:  0.6901\n",
      "Iteration  23 : Train Loss =  2.1373339   Train Acc:  0.71085   Validation Loss =  2.1562853   Validation Acc:  0.697\n",
      "Iteration  24 : Train Loss =  2.0980282   Train Acc:  0.7169167   Validation Loss =  2.1173472   Validation Acc:  0.7034\n",
      "Iteration  25 : Train Loss =  2.060273   Train Acc:  0.72218335   Validation Loss =  2.0798025   Validation Acc:  0.7106\n",
      "Iteration  26 : Train Loss =  2.0232782   Train Acc:  0.7281   Validation Loss =  2.043057   Validation Acc:  0.7161\n",
      "Iteration  27 : Train Loss =  1.9869773   Train Acc:  0.73473334   Validation Loss =  2.0071495   Validation Acc:  0.7228\n",
      "Iteration  28 : Train Loss =  1.9515231   Train Acc:  0.7410333   Validation Loss =  1.9722555   Validation Acc:  0.7275\n",
      "Iteration  29 : Train Loss =  1.9168756   Train Acc:  0.74591666   Validation Loss =  1.9379817   Validation Acc:  0.7346\n",
      "Iteration  30 : Train Loss =  1.8828928   Train Acc:  0.75091666   Validation Loss =  1.9042635   Validation Acc:  0.7388\n",
      "Iteration  31 : Train Loss =  1.8494236   Train Acc:  0.75575   Validation Loss =  1.8704113   Validation Acc:  0.744\n",
      "Iteration  32 : Train Loss =  1.8168826   Train Acc:  0.76075   Validation Loss =  1.8376391   Validation Acc:  0.7471\n",
      "Iteration  33 : Train Loss =  1.7855219   Train Acc:  0.7658167   Validation Loss =  1.8058585   Validation Acc:  0.7527\n",
      "Iteration  34 : Train Loss =  1.7549198   Train Acc:  0.77085   Validation Loss =  1.7753437   Validation Acc:  0.7579\n",
      "Iteration  35 : Train Loss =  1.7244353   Train Acc:  0.7752   Validation Loss =  1.7450676   Validation Acc:  0.7633\n",
      "Iteration  36 : Train Loss =  1.6944768   Train Acc:  0.78015   Validation Loss =  1.7151124   Validation Acc:  0.7686\n",
      "Iteration  37 : Train Loss =  1.6661155   Train Acc:  0.7844167   Validation Loss =  1.6869974   Validation Acc:  0.7714\n",
      "Iteration  38 : Train Loss =  1.638864   Train Acc:  0.7869833   Validation Loss =  1.6597104   Validation Acc:  0.773\n",
      "Iteration  39 : Train Loss =  1.6117473   Train Acc:  0.79115   Validation Loss =  1.6325698   Validation Acc:  0.7763\n",
      "Iteration  40 : Train Loss =  1.5849879   Train Acc:  0.79338336   Validation Loss =  1.6060243   Validation Acc:  0.7791\n",
      "Iteration  41 : Train Loss =  1.5594249   Train Acc:  0.79583335   Validation Loss =  1.5804479   Validation Acc:  0.7811\n",
      "Iteration  42 : Train Loss =  1.5348682   Train Acc:  0.79903334   Validation Loss =  1.5560856   Validation Acc:  0.7854\n",
      "Iteration  43 : Train Loss =  1.510598   Train Acc:  0.80046666   Validation Loss =  1.5319235   Validation Acc:  0.7877\n",
      "Iteration  44 : Train Loss =  1.4866405   Train Acc:  0.8034833   Validation Loss =  1.5080571   Validation Acc:  0.7905\n",
      "Iteration  45 : Train Loss =  1.4636254   Train Acc:  0.80575   Validation Loss =  1.4853699   Validation Acc:  0.7928\n",
      "Iteration  46 : Train Loss =  1.4416249   Train Acc:  0.8063833   Validation Loss =  1.4634908   Validation Acc:  0.792\n",
      "Iteration  47 : Train Loss =  1.4200866   Train Acc:  0.80965   Validation Loss =  1.4422009   Validation Acc:  0.7948\n",
      "Iteration  48 : Train Loss =  1.3987644   Train Acc:  0.80971664   Validation Loss =  1.4210955   Validation Acc:  0.7969\n",
      "Iteration  49 : Train Loss =  1.3780094   Train Acc:  0.8120833   Validation Loss =  1.4006336   Validation Acc:  0.7996\n",
      "Iteration  50 : Train Loss =  1.3580568   Train Acc:  0.81381667   Validation Loss =  1.3811132   Validation Acc:  0.8007\n",
      "Iteration  51 : Train Loss =  1.3387588   Train Acc:  0.8141   Validation Loss =  1.3620138   Validation Acc:  0.8013\n",
      "Iteration  52 : Train Loss =  1.3198347   Train Acc:  0.8171   Validation Loss =  1.3434454   Validation Acc:  0.8031\n",
      "Iteration  53 : Train Loss =  1.3012147   Train Acc:  0.81726664   Validation Loss =  1.3249373   Validation Acc:  0.8037\n",
      "Iteration  54 : Train Loss =  1.2830784   Train Acc:  0.8193167   Validation Loss =  1.3071144   Validation Acc:  0.805\n",
      "Iteration  55 : Train Loss =  1.2655563   Train Acc:  0.82035   Validation Loss =  1.2897887   Validation Acc:  0.8061\n",
      "Iteration  56 : Train Loss =  1.2486367   Train Acc:  0.82128334   Validation Loss =  1.2731795   Validation Acc:  0.8063\n",
      "Iteration  57 : Train Loss =  1.2321899   Train Acc:  0.8232167   Validation Loss =  1.2568523   Validation Acc:  0.8086\n",
      "Iteration  58 : Train Loss =  1.2161222   Train Acc:  0.8232833   Validation Loss =  1.2410336   Validation Acc:  0.8101\n",
      "Iteration  59 : Train Loss =  1.2004647   Train Acc:  0.82505   Validation Loss =  1.2252101   Validation Acc:  0.811\n",
      "Iteration  60 : Train Loss =  1.1854115   Train Acc:  0.8251   Validation Loss =  1.2107224   Validation Acc:  0.812\n",
      "Iteration  61 : Train Loss =  1.1713896   Train Acc:  0.82591665   Validation Loss =  1.1960028   Validation Acc:  0.8132\n",
      "Iteration  62 : Train Loss =  1.1587644   Train Acc:  0.8249   Validation Loss =  1.1848307   Validation Acc:  0.8108\n",
      "Iteration  63 : Train Loss =  1.1470573   Train Acc:  0.8243833   Validation Loss =  1.1713884   Validation Acc:  0.812\n",
      "Iteration  64 : Train Loss =  1.1307425   Train Acc:  0.8267   Validation Loss =  1.156685   Validation Acc:  0.8129\n",
      "Iteration  65 : Train Loss =  1.1162856   Train Acc:  0.82926667   Validation Loss =  1.1417661   Validation Acc:  0.8156\n",
      "Iteration  66 : Train Loss =  1.1058593   Train Acc:  0.82788336   Validation Loss =  1.1306857   Validation Acc:  0.8163\n",
      "Iteration  67 : Train Loss =  1.0932846   Train Acc:  0.8287   Validation Loss =  1.1193123   Validation Acc:  0.815\n",
      "Iteration  68 : Train Loss =  1.0799062   Train Acc:  0.83108336   Validation Loss =  1.1050903   Validation Acc:  0.8184\n",
      "Iteration  69 : Train Loss =  1.0690773   Train Acc:  0.83091664   Validation Loss =  1.0942376   Validation Acc:  0.8186\n",
      "Iteration  70 : Train Loss =  1.0586684   Train Acc:  0.8308   Validation Loss =  1.0846153   Validation Acc:  0.8164\n",
      "Iteration  71 : Train Loss =  1.0468087   Train Acc:  0.8322   Validation Loss =  1.0720353   Validation Acc:  0.8197\n",
      "Iteration  72 : Train Loss =  1.0356956   Train Acc:  0.8337333   Validation Loss =  1.0610611   Validation Acc:  0.8206\n",
      "Iteration  73 : Train Loss =  1.0263821   Train Acc:  0.83321667   Validation Loss =  1.0522995   Validation Acc:  0.8189\n",
      "Iteration  74 : Train Loss =  1.0164453   Train Acc:  0.83386666   Validation Loss =  1.0415729   Validation Acc:  0.8204\n",
      "Iteration  75 : Train Loss =  1.0056164   Train Acc:  0.83498335   Validation Loss =  1.0312028   Validation Acc:  0.8209\n",
      "Iteration  76 : Train Loss =  0.9965218   Train Acc:  0.835   Validation Loss =  1.0224674   Validation Acc:  0.8222\n",
      "Iteration  77 : Train Loss =  0.98808193   Train Acc:  0.83523333   Validation Loss =  1.0133227   Validation Acc:  0.8214\n",
      "Iteration  78 : Train Loss =  0.9784107   Train Acc:  0.83595   Validation Loss =  1.0044851   Validation Acc:  0.823\n",
      "Iteration  79 : Train Loss =  0.9692786   Train Acc:  0.83631665   Validation Loss =  0.9948642   Validation Acc:  0.8226\n",
      "Iteration  80 : Train Loss =  0.961541   Train Acc:  0.8364   Validation Loss =  0.987219   Validation Acc:  0.823\n",
      "Iteration  81 : Train Loss =  0.954142   Train Acc:  0.83625   Validation Loss =  0.98008907   Validation Acc:  0.8215\n",
      "Iteration  82 : Train Loss =  0.946363   Train Acc:  0.83665   Validation Loss =  0.97239804   Validation Acc:  0.8223\n",
      "Iteration  83 : Train Loss =  0.939021   Train Acc:  0.8366   Validation Loss =  0.96453166   Validation Acc:  0.8226\n",
      "Iteration  84 : Train Loss =  0.9324865   Train Acc:  0.836   Validation Loss =  0.9592035   Validation Acc:  0.8229\n",
      "Iteration  85 : Train Loss =  0.924019   Train Acc:  0.8378   Validation Loss =  0.9494287   Validation Acc:  0.8246\n",
      "Iteration  86 : Train Loss =  0.9145121   Train Acc:  0.8390833   Validation Loss =  0.9409759   Validation Acc:  0.8261\n",
      "Iteration  87 : Train Loss =  0.9070606   Train Acc:  0.8398   Validation Loss =  0.93347394   Validation Acc:  0.8257\n",
      "Iteration  88 : Train Loss =  0.90190184   Train Acc:  0.83931667   Validation Loss =  0.92777133   Validation Acc:  0.8263\n",
      "Iteration  89 : Train Loss =  0.8962816   Train Acc:  0.83855   Validation Loss =  0.92333174   Validation Acc:  0.8256\n",
      "Iteration  90 : Train Loss =  0.8885018   Train Acc:  0.83973336   Validation Loss =  0.9145653   Validation Acc:  0.8265\n",
      "Iteration  91 : Train Loss =  0.8809938   Train Acc:  0.8411   Validation Loss =  0.90759695   Validation Acc:  0.8271\n",
      "Iteration  92 : Train Loss =  0.87521553   Train Acc:  0.84108335   Validation Loss =  0.9022061   Validation Acc:  0.8271\n",
      "Iteration  93 : Train Loss =  0.87005377   Train Acc:  0.8405333   Validation Loss =  0.89630383   Validation Acc:  0.8282\n",
      "Iteration  94 : Train Loss =  0.863904   Train Acc:  0.8411667   Validation Loss =  0.89104193   Validation Acc:  0.8274\n",
      "Iteration  95 : Train Loss =  0.8574972   Train Acc:  0.84153336   Validation Loss =  0.88413525   Validation Acc:  0.829\n",
      "Iteration  96 : Train Loss =  0.85204136   Train Acc:  0.8419167   Validation Loss =  0.8788239   Validation Acc:  0.8294\n",
      "Iteration  97 : Train Loss =  0.8471067   Train Acc:  0.84153336   Validation Loss =  0.87443477   Validation Acc:  0.827\n",
      "Iteration  98 : Train Loss =  0.8419672   Train Acc:  0.84216666   Validation Loss =  0.8686894   Validation Acc:  0.829\n",
      "Iteration  99 : Train Loss =  0.83587855   Train Acc:  0.8429667   Validation Loss =  0.86326647   Validation Acc:  0.8302\n",
      "Iteration  100 : Train Loss =  0.8302671   Train Acc:  0.8434   Validation Loss =  0.85737026   Validation Acc:  0.8293\n",
      "Iteration  101 : Train Loss =  0.8255596   Train Acc:  0.8437833   Validation Loss =  0.85266924   Validation Acc:  0.8312\n",
      "Iteration  102 : Train Loss =  0.8213541   Train Acc:  0.8435   Validation Loss =  0.8489646   Validation Acc:  0.8285\n",
      "Iteration  103 : Train Loss =  0.8171971   Train Acc:  0.8434167   Validation Loss =  0.8443322   Validation Acc:  0.8309\n",
      "Iteration  104 : Train Loss =  0.812299   Train Acc:  0.8439   Validation Loss =  0.83998275   Validation Acc:  0.8286\n",
      "Iteration  105 : Train Loss =  0.8074609   Train Acc:  0.8446   Validation Loss =  0.8348731   Validation Acc:  0.8315\n",
      "Iteration  106 : Train Loss =  0.80289364   Train Acc:  0.84515   Validation Loss =  0.8304214   Validation Acc:  0.8311\n",
      "Iteration  107 : Train Loss =  0.79898024   Train Acc:  0.84491664   Validation Loss =  0.8268837   Validation Acc:  0.8309\n",
      "Iteration  108 : Train Loss =  0.7954457   Train Acc:  0.84455   Validation Loss =  0.82273096   Validation Acc:  0.8323\n",
      "Iteration  109 : Train Loss =  0.79215324   Train Acc:  0.8448667   Validation Loss =  0.8203403   Validation Acc:  0.8301\n",
      "Iteration  110 : Train Loss =  0.78796417   Train Acc:  0.84465   Validation Loss =  0.81532085   Validation Acc:  0.8324\n",
      "Iteration  111 : Train Loss =  0.7834065   Train Acc:  0.84573334   Validation Loss =  0.8116375   Validation Acc:  0.8314\n",
      "Iteration  112 : Train Loss =  0.77852523   Train Acc:  0.84665   Validation Loss =  0.8062725   Validation Acc:  0.8332\n",
      "Iteration  113 : Train Loss =  0.7743293   Train Acc:  0.84681666   Validation Loss =  0.80225885   Validation Acc:  0.8326\n",
      "Iteration  114 : Train Loss =  0.7710191   Train Acc:  0.84643334   Validation Loss =  0.7992542   Validation Acc:  0.8317\n",
      "Iteration  115 : Train Loss =  0.7684138   Train Acc:  0.8464   Validation Loss =  0.7961203   Validation Acc:  0.8324\n",
      "Iteration  116 : Train Loss =  0.76543665   Train Acc:  0.8459   Validation Loss =  0.7940778   Validation Acc:  0.8318\n",
      "Iteration  117 : Train Loss =  0.7624748   Train Acc:  0.84618336   Validation Loss =  0.7902757   Validation Acc:  0.8321\n",
      "Iteration  118 : Train Loss =  0.75821054   Train Acc:  0.8463333   Validation Loss =  0.786828   Validation Acc:  0.8312\n",
      "Iteration  119 : Train Loss =  0.75435615   Train Acc:  0.84763336   Validation Loss =  0.78244424   Validation Acc:  0.8331\n",
      "Iteration  120 : Train Loss =  0.7509931   Train Acc:  0.8477333   Validation Loss =  0.77930534   Validation Acc:  0.8345\n",
      "Iteration  121 : Train Loss =  0.7486521   Train Acc:  0.84776664   Validation Loss =  0.7772352   Validation Acc:  0.8343\n",
      "Iteration  122 : Train Loss =  0.7468492   Train Acc:  0.8469333   Validation Loss =  0.77492726   Validation Acc:  0.8347\n",
      "Iteration  123 : Train Loss =  0.74503577   Train Acc:  0.84713334   Validation Loss =  0.77392256   Validation Acc:  0.8326\n",
      "Iteration  124 : Train Loss =  0.7416023   Train Acc:  0.8469667   Validation Loss =  0.76978266   Validation Acc:  0.8344\n",
      "Iteration  125 : Train Loss =  0.7373105   Train Acc:  0.84825   Validation Loss =  0.7660295   Validation Acc:  0.8356\n",
      "Iteration  126 : Train Loss =  0.7334333   Train Acc:  0.84893334   Validation Loss =  0.76209795   Validation Acc:  0.835\n",
      "Iteration  127 : Train Loss =  0.7312383   Train Acc:  0.8489   Validation Loss =  0.7595404   Validation Acc:  0.8342\n",
      "Iteration  128 : Train Loss =  0.72969276   Train Acc:  0.84823334   Validation Loss =  0.7588564   Validation Acc:  0.8334\n",
      "Iteration  129 : Train Loss =  0.72765726   Train Acc:  0.8487667   Validation Loss =  0.7558876   Validation Acc:  0.8341\n",
      "Iteration  130 : Train Loss =  0.7239526   Train Acc:  0.84928334   Validation Loss =  0.7531976   Validation Acc:  0.8342\n",
      "Iteration  131 : Train Loss =  0.71997976   Train Acc:  0.8501   Validation Loss =  0.7486447   Validation Acc:  0.8357\n",
      "Iteration  132 : Train Loss =  0.71715635   Train Acc:  0.85101664   Validation Loss =  0.7459963   Validation Acc:  0.8367\n",
      "Iteration  133 : Train Loss =  0.71555704   Train Acc:  0.85038334   Validation Loss =  0.74472165   Validation Acc:  0.8352\n",
      "Iteration  134 : Train Loss =  0.71400785   Train Acc:  0.85001665   Validation Loss =  0.7425779   Validation Acc:  0.8351\n",
      "Iteration  135 : Train Loss =  0.7117697   Train Acc:  0.85035   Validation Loss =  0.7411039   Validation Acc:  0.8356\n",
      "Iteration  136 : Train Loss =  0.708781   Train Acc:  0.8505833   Validation Loss =  0.73750746   Validation Acc:  0.8361\n",
      "Iteration  137 : Train Loss =  0.70603096   Train Acc:  0.85143334   Validation Loss =  0.73521924   Validation Acc:  0.8367\n",
      "Iteration  138 : Train Loss =  0.70429504   Train Acc:  0.85155   Validation Loss =  0.7334026   Validation Acc:  0.8378\n",
      "Iteration  139 : Train Loss =  0.7035152   Train Acc:  0.85101664   Validation Loss =  0.73252475   Validation Acc:  0.8356\n",
      "Iteration  140 : Train Loss =  0.7033583   Train Acc:  0.8498667   Validation Loss =  0.7326797   Validation Acc:  0.8374\n",
      "Iteration  141 : Train Loss =  0.7034613   Train Acc:  0.8479667   Validation Loss =  0.732697   Validation Acc:  0.8329\n",
      "Iteration  142 : Train Loss =  0.70316964   Train Acc:  0.8476667   Validation Loss =  0.73239625   Validation Acc:  0.8342\n",
      "Iteration  143 : Train Loss =  0.69937   Train Acc:  0.8485   Validation Loss =  0.7290443   Validation Acc:  0.8334\n",
      "Iteration  144 : Train Loss =  0.695879   Train Acc:  0.85031664   Validation Loss =  0.72476536   Validation Acc:  0.8358\n",
      "Iteration  145 : Train Loss =  0.6919867   Train Acc:  0.85181665   Validation Loss =  0.7216314   Validation Acc:  0.8378\n",
      "Iteration  146 : Train Loss =  0.6907021   Train Acc:  0.85136664   Validation Loss =  0.71978754   Validation Acc:  0.8375\n",
      "Iteration  147 : Train Loss =  0.6898279   Train Acc:  0.8509167   Validation Loss =  0.7192431   Validation Acc:  0.8373\n",
      "Iteration  148 : Train Loss =  0.687361   Train Acc:  0.85148335   Validation Loss =  0.7171919   Validation Acc:  0.8355\n",
      "Iteration  149 : Train Loss =  0.68540674   Train Acc:  0.85175   Validation Loss =  0.7145889   Validation Acc:  0.8381\n",
      "Iteration  150 : Train Loss =  0.6836279   Train Acc:  0.85275   Validation Loss =  0.7135589   Validation Acc:  0.8386\n",
      "Iteration  151 : Train Loss =  0.68178165   Train Acc:  0.85228336   Validation Loss =  0.71106243   Validation Acc:  0.8375\n",
      "Iteration  152 : Train Loss =  0.67962307   Train Acc:  0.85285   Validation Loss =  0.70910585   Validation Acc:  0.8378\n",
      "Iteration  153 : Train Loss =  0.67772466   Train Acc:  0.85315   Validation Loss =  0.70752764   Validation Acc:  0.8377\n",
      "Iteration  154 : Train Loss =  0.67686385   Train Acc:  0.85291666   Validation Loss =  0.70609945   Validation Acc:  0.838\n",
      "Iteration  155 : Train Loss =  0.675724   Train Acc:  0.8534833   Validation Loss =  0.7057252   Validation Acc:  0.839\n",
      "Iteration  156 : Train Loss =  0.6732627   Train Acc:  0.8534833   Validation Loss =  0.7027068   Validation Acc:  0.8388\n",
      "Iteration  157 : Train Loss =  0.6709585   Train Acc:  0.85391665   Validation Loss =  0.700607   Validation Acc:  0.8394\n",
      "Iteration  158 : Train Loss =  0.669918   Train Acc:  0.85438335   Validation Loss =  0.69985366   Validation Acc:  0.8402\n",
      "Iteration  159 : Train Loss =  0.66947407   Train Acc:  0.85375   Validation Loss =  0.698838   Validation Acc:  0.8385\n",
      "Iteration  160 : Train Loss =  0.6681745   Train Acc:  0.85428333   Validation Loss =  0.6982564   Validation Acc:  0.8398\n",
      "Iteration  161 : Train Loss =  0.66567343   Train Acc:  0.85428333   Validation Loss =  0.6952258   Validation Acc:  0.8399\n",
      "Iteration  162 : Train Loss =  0.6636577   Train Acc:  0.85543334   Validation Loss =  0.69344616   Validation Acc:  0.8398\n",
      "Iteration  163 : Train Loss =  0.662765   Train Acc:  0.85546666   Validation Loss =  0.6927806   Validation Acc:  0.8413\n",
      "Iteration  164 : Train Loss =  0.66225404   Train Acc:  0.8545833   Validation Loss =  0.6918709   Validation Acc:  0.8403\n",
      "Iteration  165 : Train Loss =  0.6611575   Train Acc:  0.85535   Validation Loss =  0.69136596   Validation Acc:  0.8401\n",
      "Iteration  166 : Train Loss =  0.65929276   Train Acc:  0.85495   Validation Loss =  0.6889895   Validation Acc:  0.8406\n",
      "Iteration  167 : Train Loss =  0.6575371   Train Acc:  0.85566664   Validation Loss =  0.6876013   Validation Acc:  0.8416\n",
      "Iteration  168 : Train Loss =  0.65636474   Train Acc:  0.85608333   Validation Loss =  0.6863845   Validation Acc:  0.8417\n",
      "Iteration  169 : Train Loss =  0.65551937   Train Acc:  0.85565   Validation Loss =  0.68540084   Validation Acc:  0.8415\n",
      "Iteration  170 : Train Loss =  0.65447927   Train Acc:  0.8563167   Validation Loss =  0.68469095   Validation Acc:  0.8419\n",
      "Iteration  171 : Train Loss =  0.65306455   Train Acc:  0.8555833   Validation Loss =  0.68297213   Validation Acc:  0.8413\n",
      "Iteration  172 : Train Loss =  0.65161014   Train Acc:  0.85658336   Validation Loss =  0.68180513   Validation Acc:  0.8419\n",
      "Iteration  173 : Train Loss =  0.6504072   Train Acc:  0.8566833   Validation Loss =  0.68049383   Validation Acc:  0.8406\n",
      "Iteration  174 : Train Loss =  0.6495377   Train Acc:  0.8562833   Validation Loss =  0.67971754   Validation Acc:  0.8415\n",
      "Iteration  175 : Train Loss =  0.6488476   Train Acc:  0.8566833   Validation Loss =  0.6791203   Validation Acc:  0.8421\n",
      "Iteration  176 : Train Loss =  0.648133   Train Acc:  0.85605   Validation Loss =  0.6783586   Validation Acc:  0.8414\n",
      "Iteration  177 : Train Loss =  0.6474005   Train Acc:  0.8567   Validation Loss =  0.6776626   Validation Acc:  0.8424\n",
      "Iteration  178 : Train Loss =  0.6466911   Train Acc:  0.85595   Validation Loss =  0.67708206   Validation Acc:  0.8393\n",
      "Iteration  179 : Train Loss =  0.64658225   Train Acc:  0.856   Validation Loss =  0.6767872   Validation Acc:  0.8418\n",
      "Iteration  180 : Train Loss =  0.64633816   Train Acc:  0.855   Validation Loss =  0.6770542   Validation Acc:  0.8387\n",
      "Iteration  181 : Train Loss =  0.64766794   Train Acc:  0.8548167   Validation Loss =  0.6777084   Validation Acc:  0.841\n",
      "Iteration  182 : Train Loss =  0.64626575   Train Acc:  0.8543   Validation Loss =  0.6771338   Validation Acc:  0.838\n",
      "Iteration  183 : Train Loss =  0.64526343   Train Acc:  0.85476667   Validation Loss =  0.67525434   Validation Acc:  0.8411\n",
      "Iteration  184 : Train Loss =  0.64130306   Train Acc:  0.85695   Validation Loss =  0.6720198   Validation Acc:  0.841\n",
      "Iteration  185 : Train Loss =  0.6385985   Train Acc:  0.8574167   Validation Loss =  0.66888595   Validation Acc:  0.8428\n",
      "Iteration  186 : Train Loss =  0.6379169   Train Acc:  0.8579   Validation Loss =  0.66835034   Validation Acc:  0.8433\n",
      "Iteration  187 : Train Loss =  0.6385035   Train Acc:  0.85676664   Validation Loss =  0.66929704   Validation Acc:  0.8402\n",
      "Iteration  188 : Train Loss =  0.6395225   Train Acc:  0.85576665   Validation Loss =  0.669827   Validation Acc:  0.8433\n",
      "Iteration  189 : Train Loss =  0.63769543   Train Acc:  0.85658336   Validation Loss =  0.66861266   Validation Acc:  0.8401\n",
      "Iteration  190 : Train Loss =  0.63553923   Train Acc:  0.8574333   Validation Loss =  0.6658989   Validation Acc:  0.8428\n",
      "Iteration  191 : Train Loss =  0.63324165   Train Acc:  0.8588333   Validation Loss =  0.6638718   Validation Acc:  0.8431\n",
      "Iteration  192 : Train Loss =  0.6324945   Train Acc:  0.8588167   Validation Loss =  0.66319776   Validation Acc:  0.8431\n",
      "Iteration  193 : Train Loss =  0.63285714   Train Acc:  0.85775   Validation Loss =  0.66334873   Validation Acc:  0.8428\n",
      "Iteration  194 : Train Loss =  0.6327045   Train Acc:  0.85758334   Validation Loss =  0.6637019   Validation Acc:  0.8414\n",
      "Iteration  195 : Train Loss =  0.6320566   Train Acc:  0.8578167   Validation Loss =  0.66264087   Validation Acc:  0.8437\n",
      "Iteration  196 : Train Loss =  0.6300683   Train Acc:  0.85866666   Validation Loss =  0.6609963   Validation Acc:  0.8426\n",
      "Iteration  197 : Train Loss =  0.6285125   Train Acc:  0.8593   Validation Loss =  0.65923196   Validation Acc:  0.8434\n",
      "Iteration  198 : Train Loss =  0.6277439   Train Acc:  0.8588667   Validation Loss =  0.6584072   Validation Acc:  0.8428\n",
      "Iteration  199 : Train Loss =  0.6276183   Train Acc:  0.85966665   Validation Loss =  0.6586093   Validation Acc:  0.8431\n",
      "Iteration  200 : Train Loss =  0.6276078   Train Acc:  0.85796666   Validation Loss =  0.6582491   Validation Acc:  0.8438\n",
      "Iteration  201 : Train Loss =  0.6269628   Train Acc:  0.8592167   Validation Loss =  0.65819174   Validation Acc:  0.8425\n",
      "Iteration  202 : Train Loss =  0.6259557   Train Acc:  0.85796666   Validation Loss =  0.6566943   Validation Acc:  0.8441\n",
      "Iteration  203 : Train Loss =  0.6246927   Train Acc:  0.86036664   Validation Loss =  0.65583134   Validation Acc:  0.8435\n",
      "Iteration  204 : Train Loss =  0.62371767   Train Acc:  0.85913336   Validation Loss =  0.6545439   Validation Acc:  0.8437\n",
      "Iteration  205 : Train Loss =  0.6233074   Train Acc:  0.86008334   Validation Loss =  0.6543482   Validation Acc:  0.8441\n",
      "Iteration  206 : Train Loss =  0.6235007   Train Acc:  0.8588667   Validation Loss =  0.6545231   Validation Acc:  0.8432\n",
      "Iteration  207 : Train Loss =  0.6243698   Train Acc:  0.8591667   Validation Loss =  0.6555659   Validation Acc:  0.8451\n",
      "Iteration  208 : Train Loss =  0.62568104   Train Acc:  0.8569833   Validation Loss =  0.6568186   Validation Acc:  0.8411\n",
      "Iteration  209 : Train Loss =  0.62737006   Train Acc:  0.85693336   Validation Loss =  0.65874475   Validation Acc:  0.844\n",
      "Iteration  210 : Train Loss =  0.62819636   Train Acc:  0.85426664   Validation Loss =  0.6591289   Validation Acc:  0.8393\n",
      "Iteration  211 : Train Loss =  0.6265766   Train Acc:  0.8571333   Validation Loss =  0.6580942   Validation Acc:  0.8431\n",
      "Iteration  212 : Train Loss =  0.6226605   Train Acc:  0.85683334   Validation Loss =  0.65339077   Validation Acc:  0.843\n",
      "Iteration  213 : Train Loss =  0.62007654   Train Acc:  0.8597   Validation Loss =  0.6515615   Validation Acc:  0.8426\n",
      "Iteration  214 : Train Loss =  0.62036324   Train Acc:  0.85903335   Validation Loss =  0.6514356   Validation Acc:  0.8452\n",
      "Iteration  215 : Train Loss =  0.62067544   Train Acc:  0.85828334   Validation Loss =  0.6520878   Validation Acc:  0.8421\n",
      "Iteration  216 : Train Loss =  0.6196781   Train Acc:  0.85976666   Validation Loss =  0.6512524   Validation Acc:  0.8455\n",
      "Iteration  217 : Train Loss =  0.61716396   Train Acc:  0.85945   Validation Loss =  0.6483097   Validation Acc:  0.8432\n",
      "Iteration  218 : Train Loss =  0.61605775   Train Acc:  0.86118335   Validation Loss =  0.64769137   Validation Acc:  0.8437\n",
      "Iteration  219 : Train Loss =  0.617088   Train Acc:  0.8596   Validation Loss =  0.6483447   Validation Acc:  0.8463\n",
      "Iteration  220 : Train Loss =  0.6168653   Train Acc:  0.85925   Validation Loss =  0.6484988   Validation Acc:  0.8423\n",
      "Iteration  221 : Train Loss =  0.61534715   Train Acc:  0.86036664   Validation Loss =  0.64680874   Validation Acc:  0.8457\n",
      "Iteration  222 : Train Loss =  0.6129958   Train Acc:  0.86081666   Validation Loss =  0.64439833   Validation Acc:  0.845\n",
      "Iteration  223 : Train Loss =  0.6125   Train Acc:  0.8615   Validation Loss =  0.6440252   Validation Acc:  0.8444\n",
      "Iteration  224 : Train Loss =  0.6136068   Train Acc:  0.8605   Validation Loss =  0.6449421   Validation Acc:  0.8457\n",
      "Iteration  225 : Train Loss =  0.61348367   Train Acc:  0.8602167   Validation Loss =  0.64515436   Validation Acc:  0.8432\n",
      "Iteration  226 : Train Loss =  0.6123247   Train Acc:  0.8608   Validation Loss =  0.6437483   Validation Acc:  0.846\n",
      "Iteration  227 : Train Loss =  0.6104216   Train Acc:  0.8620333   Validation Loss =  0.64210665   Validation Acc:  0.844\n",
      "Iteration  228 : Train Loss =  0.6097799   Train Acc:  0.86125   Validation Loss =  0.64131474   Validation Acc:  0.8453\n",
      "Iteration  229 : Train Loss =  0.6100376   Train Acc:  0.8614333   Validation Loss =  0.6416733   Validation Acc:  0.8456\n",
      "Iteration  230 : Train Loss =  0.6099189   Train Acc:  0.86076665   Validation Loss =  0.64160943   Validation Acc:  0.8449\n",
      "Iteration  231 : Train Loss =  0.60912615   Train Acc:  0.86163336   Validation Loss =  0.6407518   Validation Acc:  0.8456\n",
      "Iteration  232 : Train Loss =  0.60802   Train Acc:  0.86233336   Validation Loss =  0.6398382   Validation Acc:  0.8442\n",
      "Iteration  233 : Train Loss =  0.60753083   Train Acc:  0.8616667   Validation Loss =  0.6391513   Validation Acc:  0.8463\n",
      "Iteration  234 : Train Loss =  0.60756147   Train Acc:  0.8620333   Validation Loss =  0.6394734   Validation Acc:  0.846\n",
      "Iteration  235 : Train Loss =  0.60747963   Train Acc:  0.86111665   Validation Loss =  0.639118   Validation Acc:  0.8457\n",
      "Iteration  236 : Train Loss =  0.6069   Train Acc:  0.86186665   Validation Loss =  0.6388786   Validation Acc:  0.8463\n",
      "Iteration  237 : Train Loss =  0.6059587   Train Acc:  0.8620333   Validation Loss =  0.6376937   Validation Acc:  0.8456\n",
      "Iteration  238 : Train Loss =  0.6051423   Train Acc:  0.86298335   Validation Loss =  0.63714737   Validation Acc:  0.8464\n",
      "Iteration  239 : Train Loss =  0.6047045   Train Acc:  0.86235   Validation Loss =  0.6365331   Validation Acc:  0.8467\n",
      "Iteration  240 : Train Loss =  0.60452974   Train Acc:  0.86245   Validation Loss =  0.63653886   Validation Acc:  0.8452\n",
      "Iteration  241 : Train Loss =  0.60443354   Train Acc:  0.86223334   Validation Loss =  0.6363241   Validation Acc:  0.8465\n",
      "Iteration  242 : Train Loss =  0.60402167   Train Acc:  0.86228335   Validation Loss =  0.636114   Validation Acc:  0.8454\n",
      "Iteration  243 : Train Loss =  0.60356474   Train Acc:  0.86253333   Validation Loss =  0.63550484   Validation Acc:  0.8471\n",
      "Iteration  244 : Train Loss =  0.6029718   Train Acc:  0.86303335   Validation Loss =  0.63516235   Validation Acc:  0.8446\n",
      "Iteration  245 : Train Loss =  0.6026976   Train Acc:  0.8620667   Validation Loss =  0.6345804   Validation Acc:  0.8474\n",
      "Iteration  246 : Train Loss =  0.60280085   Train Acc:  0.8631833   Validation Loss =  0.63511056   Validation Acc:  0.8461\n",
      "Iteration  247 : Train Loss =  0.60336757   Train Acc:  0.86181664   Validation Loss =  0.6351702   Validation Acc:  0.848\n",
      "Iteration  248 : Train Loss =  0.6047847   Train Acc:  0.8617333   Validation Loss =  0.6373369   Validation Acc:  0.8458\n",
      "Iteration  249 : Train Loss =  0.60618794   Train Acc:  0.8594667   Validation Loss =  0.6379094   Validation Acc:  0.8462\n",
      "Iteration  250 : Train Loss =  0.60822415   Train Acc:  0.8595   Validation Loss =  0.6410295   Validation Acc:  0.844\n",
      "Iteration  251 : Train Loss =  0.60749257   Train Acc:  0.85908335   Validation Loss =  0.6392022   Validation Acc:  0.846\n",
      "Iteration  252 : Train Loss =  0.6046404   Train Acc:  0.8606   Validation Loss =  0.6373266   Validation Acc:  0.8437\n",
      "Iteration  253 : Train Loss =  0.60124695   Train Acc:  0.86181664   Validation Loss =  0.63326246   Validation Acc:  0.847\n",
      "Iteration  254 : Train Loss =  0.5996456   Train Acc:  0.86226666   Validation Loss =  0.6318912   Validation Acc:  0.8463\n",
      "Iteration  255 : Train Loss =  0.6007145   Train Acc:  0.8623667   Validation Loss =  0.63337255   Validation Acc:  0.8464\n",
      "Iteration  256 : Train Loss =  0.601686   Train Acc:  0.86078334   Validation Loss =  0.63367134   Validation Acc:  0.8469\n",
      "Iteration  257 : Train Loss =  0.60102403   Train Acc:  0.8625   Validation Loss =  0.6338676   Validation Acc:  0.8457\n",
      "Iteration  258 : Train Loss =  0.5984286   Train Acc:  0.86268336   Validation Loss =  0.63056886   Validation Acc:  0.8489\n",
      "Iteration  259 : Train Loss =  0.59706813   Train Acc:  0.86371666   Validation Loss =  0.62953   Validation Acc:  0.8461\n",
      "Iteration  260 : Train Loss =  0.5978328   Train Acc:  0.86293334   Validation Loss =  0.63048077   Validation Acc:  0.8471\n",
      "Iteration  261 : Train Loss =  0.59898496   Train Acc:  0.8619   Validation Loss =  0.6312288   Validation Acc:  0.8464\n",
      "Iteration  262 : Train Loss =  0.598663   Train Acc:  0.8626   Validation Loss =  0.63147247   Validation Acc:  0.8462\n",
      "Iteration  263 : Train Loss =  0.5967895   Train Acc:  0.86261666   Validation Loss =  0.6291313   Validation Acc:  0.8471\n",
      "Iteration  264 : Train Loss =  0.5952632   Train Acc:  0.86438334   Validation Loss =  0.62781537   Validation Acc:  0.8479\n",
      "Iteration  265 : Train Loss =  0.5951909   Train Acc:  0.86465   Validation Loss =  0.6279547   Validation Acc:  0.8474\n",
      "Iteration  266 : Train Loss =  0.5957867   Train Acc:  0.86333334   Validation Loss =  0.6281613   Validation Acc:  0.8497\n",
      "Iteration  267 : Train Loss =  0.59582824   Train Acc:  0.8637667   Validation Loss =  0.6287677   Validation Acc:  0.8466\n",
      "Iteration  268 : Train Loss =  0.59466696   Train Acc:  0.8637   Validation Loss =  0.6271186   Validation Acc:  0.8484\n",
      "Iteration  269 : Train Loss =  0.59344524   Train Acc:  0.8647   Validation Loss =  0.62621486   Validation Acc:  0.8479\n",
      "Iteration  270 : Train Loss =  0.5930307   Train Acc:  0.8647   Validation Loss =  0.6258343   Validation Acc:  0.848\n",
      "Iteration  271 : Train Loss =  0.59329695   Train Acc:  0.86408335   Validation Loss =  0.62593293   Validation Acc:  0.8486\n",
      "Iteration  272 : Train Loss =  0.59350765   Train Acc:  0.8646167   Validation Loss =  0.6265407   Validation Acc:  0.8473\n",
      "Iteration  273 : Train Loss =  0.5930321   Train Acc:  0.8638833   Validation Loss =  0.62566245   Validation Acc:  0.8489\n",
      "Iteration  274 : Train Loss =  0.59221697   Train Acc:  0.86515   Validation Loss =  0.62520087   Validation Acc:  0.8478\n",
      "Iteration  275 : Train Loss =  0.5916242   Train Acc:  0.86478335   Validation Loss =  0.6244774   Validation Acc:  0.849\n",
      "Iteration  276 : Train Loss =  0.59158707   Train Acc:  0.8646333   Validation Loss =  0.6244663   Validation Acc:  0.8475\n",
      "Iteration  277 : Train Loss =  0.5920553   Train Acc:  0.8649   Validation Loss =  0.62511915   Validation Acc:  0.8484\n",
      "Iteration  278 : Train Loss =  0.5929177   Train Acc:  0.86371666   Validation Loss =  0.62575185   Validation Acc:  0.8467\n",
      "Iteration  279 : Train Loss =  0.5943877   Train Acc:  0.86315   Validation Loss =  0.6275001   Validation Acc:  0.8479\n",
      "Iteration  280 : Train Loss =  0.5966866   Train Acc:  0.86148334   Validation Loss =  0.6297505   Validation Acc:  0.845\n",
      "Iteration  281 : Train Loss =  0.60205346   Train Acc:  0.8598   Validation Loss =  0.6351146   Validation Acc:  0.8446\n",
      "Iteration  282 : Train Loss =  0.6007778   Train Acc:  0.8589   Validation Loss =  0.63411486   Validation Acc:  0.842\n",
      "Iteration  283 : Train Loss =  0.60073024   Train Acc:  0.8605   Validation Loss =  0.6334533   Validation Acc:  0.8456\n",
      "Iteration  284 : Train Loss =  0.5916585   Train Acc:  0.86446667   Validation Loss =  0.62457246   Validation Acc:  0.8462\n",
      "Iteration  285 : Train Loss =  0.5900534   Train Acc:  0.86403334   Validation Loss =  0.6227398   Validation Acc:  0.8471\n",
      "Iteration  286 : Train Loss =  0.5945512   Train Acc:  0.86298335   Validation Loss =  0.6275135   Validation Acc:  0.8476\n",
      "Iteration  287 : Train Loss =  0.59511864   Train Acc:  0.86151665   Validation Loss =  0.62850475   Validation Acc:  0.8453\n",
      "Iteration  288 : Train Loss =  0.59302443   Train Acc:  0.86361665   Validation Loss =  0.62602526   Validation Acc:  0.8492\n",
      "Iteration  289 : Train Loss =  0.5884261   Train Acc:  0.8656333   Validation Loss =  0.62155557   Validation Acc:  0.8479\n",
      "Iteration  290 : Train Loss =  0.5893749   Train Acc:  0.8642   Validation Loss =  0.62235034   Validation Acc:  0.8471\n",
      "Iteration  291 : Train Loss =  0.5927584   Train Acc:  0.86293334   Validation Loss =  0.62589407   Validation Acc:  0.848\n",
      "Iteration  292 : Train Loss =  0.5907486   Train Acc:  0.86378336   Validation Loss =  0.62408227   Validation Acc:  0.8473\n",
      "Iteration  293 : Train Loss =  0.5876577   Train Acc:  0.8653833   Validation Loss =  0.6208259   Validation Acc:  0.8509\n",
      "Iteration  294 : Train Loss =  0.58680224   Train Acc:  0.8659   Validation Loss =  0.6201091   Validation Acc:  0.8491\n",
      "Iteration  295 : Train Loss =  0.5886309   Train Acc:  0.86411667   Validation Loss =  0.6219052   Validation Acc:  0.8472\n",
      "Iteration  296 : Train Loss =  0.5896113   Train Acc:  0.86403334   Validation Loss =  0.6229168   Validation Acc:  0.8497\n",
      "Iteration  297 : Train Loss =  0.5871222   Train Acc:  0.86505   Validation Loss =  0.62027025   Validation Acc:  0.8488\n",
      "Iteration  298 : Train Loss =  0.5853996   Train Acc:  0.8660167   Validation Loss =  0.6185237   Validation Acc:  0.8492\n",
      "Iteration  299 : Train Loss =  0.58606315   Train Acc:  0.8657333   Validation Loss =  0.61926913   Validation Acc:  0.8506\n",
      "Iteration  300 : Train Loss =  0.5870601   Train Acc:  0.8649   Validation Loss =  0.6203362   Validation Acc:  0.848\n",
      "Iteration  301 : Train Loss =  0.58654165   Train Acc:  0.8654   Validation Loss =  0.6199275   Validation Acc:  0.8493\n",
      "Iteration  302 : Train Loss =  0.5848514   Train Acc:  0.86585   Validation Loss =  0.6180744   Validation Acc:  0.8495\n",
      "Iteration  303 : Train Loss =  0.58428025   Train Acc:  0.8666   Validation Loss =  0.61764085   Validation Acc:  0.8491\n",
      "Iteration  304 : Train Loss =  0.5849343   Train Acc:  0.86621666   Validation Loss =  0.6182281   Validation Acc:  0.8509\n",
      "Iteration  305 : Train Loss =  0.58504933   Train Acc:  0.86591667   Validation Loss =  0.61842644   Validation Acc:  0.8492\n",
      "Iteration  306 : Train Loss =  0.58430773   Train Acc:  0.8664   Validation Loss =  0.6177701   Validation Acc:  0.8509\n",
      "Iteration  307 : Train Loss =  0.58335334   Train Acc:  0.8666833   Validation Loss =  0.6167342   Validation Acc:  0.8496\n",
      "Iteration  308 : Train Loss =  0.58320594   Train Acc:  0.86686665   Validation Loss =  0.61680067   Validation Acc:  0.8492\n",
      "Iteration  309 : Train Loss =  0.58360076   Train Acc:  0.8664333   Validation Loss =  0.6170809   Validation Acc:  0.8515\n",
      "Iteration  310 : Train Loss =  0.58351845   Train Acc:  0.8663667   Validation Loss =  0.6171534   Validation Acc:  0.8492\n",
      "Iteration  311 : Train Loss =  0.5830136   Train Acc:  0.86651665   Validation Loss =  0.6165972   Validation Acc:  0.8516\n",
      "Iteration  312 : Train Loss =  0.58230275   Train Acc:  0.8673   Validation Loss =  0.61588913   Validation Acc:  0.8491\n",
      "Iteration  313 : Train Loss =  0.5820036   Train Acc:  0.86693335   Validation Loss =  0.61559165   Validation Acc:  0.8505\n",
      "Iteration  314 : Train Loss =  0.58206624   Train Acc:  0.86703336   Validation Loss =  0.6157099   Validation Acc:  0.8505\n",
      "Iteration  315 : Train Loss =  0.58209705   Train Acc:  0.8664167   Validation Loss =  0.6157054   Validation Acc:  0.8496\n",
      "Iteration  316 : Train Loss =  0.5818645   Train Acc:  0.86705   Validation Loss =  0.615668   Validation Acc:  0.8503\n",
      "Iteration  317 : Train Loss =  0.5814253   Train Acc:  0.86651665   Validation Loss =  0.61501336   Validation Acc:  0.8496\n",
      "Iteration  318 : Train Loss =  0.58104277   Train Acc:  0.8673   Validation Loss =  0.614909   Validation Acc:  0.85\n",
      "Iteration  319 : Train Loss =  0.58088905   Train Acc:  0.8671   Validation Loss =  0.6144584   Validation Acc:  0.8519\n",
      "Iteration  320 : Train Loss =  0.5809876   Train Acc:  0.8674   Validation Loss =  0.6148999   Validation Acc:  0.8493\n",
      "Iteration  321 : Train Loss =  0.58129865   Train Acc:  0.8663667   Validation Loss =  0.614966   Validation Acc:  0.852\n",
      "Iteration  322 : Train Loss =  0.5817798   Train Acc:  0.8671333   Validation Loss =  0.6158293   Validation Acc:  0.8492\n",
      "Iteration  323 : Train Loss =  0.58263594   Train Acc:  0.86575   Validation Loss =  0.61633676   Validation Acc:  0.8517\n",
      "Iteration  324 : Train Loss =  0.58424824   Train Acc:  0.86581665   Validation Loss =  0.6184633   Validation Acc:  0.8486\n",
      "Iteration  325 : Train Loss =  0.58592176   Train Acc:  0.86371666   Validation Loss =  0.61946887   Validation Acc:  0.8495\n",
      "Iteration  326 : Train Loss =  0.58884144   Train Acc:  0.86331666   Validation Loss =  0.623327   Validation Acc:  0.8478\n",
      "Iteration  327 : Train Loss =  0.5880689   Train Acc:  0.8616667   Validation Loss =  0.6214924   Validation Acc:  0.8476\n",
      "Iteration  328 : Train Loss =  0.5860014   Train Acc:  0.8642167   Validation Loss =  0.620547   Validation Acc:  0.8491\n",
      "Iteration  329 : Train Loss =  0.5813916   Train Acc:  0.8657167   Validation Loss =  0.6149712   Validation Acc:  0.8505\n",
      "Iteration  330 : Train Loss =  0.57919455   Train Acc:  0.8674   Validation Loss =  0.6133227   Validation Acc:  0.8515\n",
      "Iteration  331 : Train Loss =  0.58046967   Train Acc:  0.8671333   Validation Loss =  0.6147536   Validation Acc:  0.8492\n",
      "Iteration  332 : Train Loss =  0.5825183   Train Acc:  0.86523336   Validation Loss =  0.61622465   Validation Acc:  0.8508\n",
      "Iteration  333 : Train Loss =  0.5827714   Train Acc:  0.86525   Validation Loss =  0.617487   Validation Acc:  0.85\n",
      "Iteration  334 : Train Loss =  0.57985455   Train Acc:  0.8665   Validation Loss =  0.61357766   Validation Acc:  0.8521\n",
      "Iteration  335 : Train Loss =  0.5777414   Train Acc:  0.8677667   Validation Loss =  0.61204886   Validation Acc:  0.851\n",
      "Iteration  336 : Train Loss =  0.5782439   Train Acc:  0.86831665   Validation Loss =  0.61260825   Validation Acc:  0.8496\n",
      "Iteration  337 : Train Loss =  0.580021   Train Acc:  0.8666667   Validation Loss =  0.61388856   Validation Acc:  0.8528\n",
      "Iteration  338 : Train Loss =  0.58040786   Train Acc:  0.8660833   Validation Loss =  0.61502826   Validation Acc:  0.8497\n",
      "Iteration  339 : Train Loss =  0.578586   Train Acc:  0.8666   Validation Loss =  0.61243916   Validation Acc:  0.8534\n",
      "Iteration  340 : Train Loss =  0.576761   Train Acc:  0.8685167   Validation Loss =  0.6109892   Validation Acc:  0.8499\n",
      "Iteration  341 : Train Loss =  0.57680225   Train Acc:  0.86815   Validation Loss =  0.61124283   Validation Acc:  0.8507\n",
      "Iteration  342 : Train Loss =  0.57778454   Train Acc:  0.8675167   Validation Loss =  0.61178356   Validation Acc:  0.8527\n",
      "Iteration  343 : Train Loss =  0.57811594   Train Acc:  0.8674667   Validation Loss =  0.61282796   Validation Acc:  0.8504\n",
      "Iteration  344 : Train Loss =  0.57698804   Train Acc:  0.86716664   Validation Loss =  0.61112976   Validation Acc:  0.8531\n",
      "Iteration  345 : Train Loss =  0.5757981   Train Acc:  0.8692833   Validation Loss =  0.61015606   Validation Acc:  0.8508\n",
      "Iteration  346 : Train Loss =  0.57558095   Train Acc:  0.8685667   Validation Loss =  0.610084   Validation Acc:  0.8511\n",
      "Iteration  347 : Train Loss =  0.5760479   Train Acc:  0.8678833   Validation Loss =  0.61020315   Validation Acc:  0.8522\n",
      "Iteration  348 : Train Loss =  0.5763574   Train Acc:  0.8681167   Validation Loss =  0.61102307   Validation Acc:  0.8503\n",
      "Iteration  349 : Train Loss =  0.5759718   Train Acc:  0.8674167   Validation Loss =  0.61030185   Validation Acc:  0.8522\n",
      "Iteration  350 : Train Loss =  0.5753303   Train Acc:  0.86885   Validation Loss =  0.6098491   Validation Acc:  0.8503\n",
      "Iteration  351 : Train Loss =  0.5749926   Train Acc:  0.86841667   Validation Loss =  0.6096042   Validation Acc:  0.8508\n",
      "Iteration  352 : Train Loss =  0.5752454   Train Acc:  0.8688833   Validation Loss =  0.6096965   Validation Acc:  0.8524\n",
      "Iteration  353 : Train Loss =  0.57588375   Train Acc:  0.8685333   Validation Loss =  0.6105448   Validation Acc:  0.8507\n",
      "Iteration  354 : Train Loss =  0.57725954   Train Acc:  0.86723334   Validation Loss =  0.6118354   Validation Acc:  0.8527\n",
      "Iteration  355 : Train Loss =  0.57820135   Train Acc:  0.8674667   Validation Loss =  0.6127225   Validation Acc:  0.8499\n",
      "Iteration  356 : Train Loss =  0.5811366   Train Acc:  0.86558336   Validation Loss =  0.6159625   Validation Acc:  0.8501\n",
      "Iteration  357 : Train Loss =  0.5823464   Train Acc:  0.8645333   Validation Loss =  0.6168657   Validation Acc:  0.8477\n",
      "Iteration  358 : Train Loss =  0.58488494   Train Acc:  0.8631167   Validation Loss =  0.61980593   Validation Acc:  0.8482\n",
      "Iteration  359 : Train Loss =  0.5820841   Train Acc:  0.8641667   Validation Loss =  0.6165246   Validation Acc:  0.8482\n",
      "Iteration  360 : Train Loss =  0.5772099   Train Acc:  0.86685   Validation Loss =  0.6117524   Validation Acc:  0.8511\n",
      "Iteration  361 : Train Loss =  0.5736749   Train Acc:  0.8684833   Validation Loss =  0.608042   Validation Acc:  0.8529\n",
      "Iteration  362 : Train Loss =  0.57456446   Train Acc:  0.86866665   Validation Loss =  0.60914093   Validation Acc:  0.8511\n",
      "Iteration  363 : Train Loss =  0.578199   Train Acc:  0.8667833   Validation Loss =  0.61302257   Validation Acc:  0.852\n",
      "Iteration  364 : Train Loss =  0.57912725   Train Acc:  0.8660333   Validation Loss =  0.6139724   Validation Acc:  0.8497\n",
      "Iteration  365 : Train Loss =  0.57687664   Train Acc:  0.86735   Validation Loss =  0.6118473   Validation Acc:  0.8512\n",
      "Iteration  366 : Train Loss =  0.57317126   Train Acc:  0.86875   Validation Loss =  0.60773563   Validation Acc:  0.851\n",
      "Iteration  367 : Train Loss =  0.572313   Train Acc:  0.8694   Validation Loss =  0.60704374   Validation Acc:  0.8514\n",
      "Iteration  368 : Train Loss =  0.57409406   Train Acc:  0.86843336   Validation Loss =  0.60886836   Validation Acc:  0.8526\n",
      "Iteration  369 : Train Loss =  0.5754466   Train Acc:  0.8674   Validation Loss =  0.61021304   Validation Acc:  0.8516\n",
      "Iteration  370 : Train Loss =  0.5749085   Train Acc:  0.8682333   Validation Loss =  0.61007524   Validation Acc:  0.8521\n",
      "Iteration  371 : Train Loss =  0.5727436   Train Acc:  0.86876667   Validation Loss =  0.60748065   Validation Acc:  0.8514\n",
      "Iteration  372 : Train Loss =  0.57149106   Train Acc:  0.8693333   Validation Loss =  0.60637546   Validation Acc:  0.8521\n",
      "Iteration  373 : Train Loss =  0.57193923   Train Acc:  0.86943334   Validation Loss =  0.6068149   Validation Acc:  0.8514\n",
      "Iteration  374 : Train Loss =  0.57295656   Train Acc:  0.86845   Validation Loss =  0.60752344   Validation Acc:  0.8522\n",
      "Iteration  375 : Train Loss =  0.57300395   Train Acc:  0.86875   Validation Loss =  0.6081074   Validation Acc:  0.8516\n",
      "Iteration  376 : Train Loss =  0.5718485   Train Acc:  0.86935   Validation Loss =  0.6066106   Validation Acc:  0.852\n",
      "Iteration  377 : Train Loss =  0.57086384   Train Acc:  0.86973333   Validation Loss =  0.6058272   Validation Acc:  0.8528\n",
      "Iteration  378 : Train Loss =  0.5708466   Train Acc:  0.86976665   Validation Loss =  0.6059531   Validation Acc:  0.8519\n",
      "Iteration  379 : Train Loss =  0.57140005   Train Acc:  0.8689833   Validation Loss =  0.60607666   Validation Acc:  0.8533\n",
      "Iteration  380 : Train Loss =  0.5715592   Train Acc:  0.8692833   Validation Loss =  0.6067562   Validation Acc:  0.8516\n",
      "Iteration  381 : Train Loss =  0.570961   Train Acc:  0.86931664   Validation Loss =  0.6058009   Validation Acc:  0.8533\n",
      "Iteration  382 : Train Loss =  0.57013583   Train Acc:  0.8703167   Validation Loss =  0.60526586   Validation Acc:  0.8524\n",
      "Iteration  383 : Train Loss =  0.56966096   Train Acc:  0.87025   Validation Loss =  0.60485375   Validation Acc:  0.8523\n",
      "Iteration  384 : Train Loss =  0.56967634   Train Acc:  0.87013334   Validation Loss =  0.6046916   Validation Acc:  0.8523\n",
      "Iteration  385 : Train Loss =  0.5699372   Train Acc:  0.8703   Validation Loss =  0.60522103   Validation Acc:  0.8521\n",
      "Iteration  386 : Train Loss =  0.5700625   Train Acc:  0.8695667   Validation Loss =  0.6051018   Validation Acc:  0.8535\n",
      "Iteration  387 : Train Loss =  0.5698519   Train Acc:  0.87048334   Validation Loss =  0.6051507   Validation Acc:  0.8521\n",
      "Iteration  388 : Train Loss =  0.569368   Train Acc:  0.86976665   Validation Loss =  0.6045462   Validation Acc:  0.8537\n",
      "Iteration  389 : Train Loss =  0.56894076   Train Acc:  0.87083334   Validation Loss =  0.6043124   Validation Acc:  0.8521\n",
      "Iteration  390 : Train Loss =  0.5687783   Train Acc:  0.8703833   Validation Loss =  0.6040721   Validation Acc:  0.8531\n",
      "Iteration  391 : Train Loss =  0.5688948   Train Acc:  0.8704   Validation Loss =  0.60434335   Validation Acc:  0.8519\n",
      "Iteration  392 : Train Loss =  0.5692481   Train Acc:  0.87056667   Validation Loss =  0.6046728   Validation Acc:  0.8537\n",
      "Iteration  393 : Train Loss =  0.56960285   Train Acc:  0.8700333   Validation Loss =  0.6051258   Validation Acc:  0.8523\n",
      "Iteration  394 : Train Loss =  0.57051194   Train Acc:  0.86986667   Validation Loss =  0.60606366   Validation Acc:  0.8538\n",
      "Iteration  395 : Train Loss =  0.5713439   Train Acc:  0.8693333   Validation Loss =  0.6069443   Validation Acc:  0.8518\n",
      "Iteration  396 : Train Loss =  0.5741529   Train Acc:  0.86791664   Validation Loss =  0.6096847   Validation Acc:  0.8541\n",
      "Iteration  397 : Train Loss =  0.5760993   Train Acc:  0.86691666   Validation Loss =  0.61187565   Validation Acc:  0.8488\n",
      "Iteration  398 : Train Loss =  0.58038   Train Acc:  0.86516666   Validation Loss =  0.6157912   Validation Acc:  0.8508\n",
      "Iteration  399 : Train Loss =  0.5792074   Train Acc:  0.86576664   Validation Loss =  0.615149   Validation Acc:  0.8478\n",
      "Iteration  400 : Train Loss =  0.57470495   Train Acc:  0.86775   Validation Loss =  0.6098897   Validation Acc:  0.8531\n",
      "Iteration  401 : Train Loss =  0.56936246   Train Acc:  0.8702667   Validation Loss =  0.60508454   Validation Acc:  0.8525\n",
      "Iteration  402 : Train Loss =  0.56812876   Train Acc:  0.8702   Validation Loss =  0.60361785   Validation Acc:  0.8528\n",
      "Iteration  403 : Train Loss =  0.571713   Train Acc:  0.86903334   Validation Loss =  0.6074037   Validation Acc:  0.8554\n",
      "Iteration  404 : Train Loss =  0.5734876   Train Acc:  0.86798334   Validation Loss =  0.609509   Validation Acc:  0.8508\n",
      "Iteration  405 : Train Loss =  0.5725387   Train Acc:  0.86826664   Validation Loss =  0.60812145   Validation Acc:  0.8532\n",
      "Iteration  406 : Train Loss =  0.5678527   Train Acc:  0.8710667   Validation Loss =  0.6036019   Validation Acc:  0.8524\n",
      "Iteration  407 : Train Loss =  0.56641394   Train Acc:  0.8710667   Validation Loss =  0.6020748   Validation Acc:  0.8529\n",
      "Iteration  408 : Train Loss =  0.56871676   Train Acc:  0.86985   Validation Loss =  0.60431296   Validation Acc:  0.8556\n",
      "Iteration  409 : Train Loss =  0.5704495   Train Acc:  0.86955   Validation Loss =  0.6065078   Validation Acc:  0.8518\n",
      "Iteration  410 : Train Loss =  0.56982654   Train Acc:  0.86925   Validation Loss =  0.60556376   Validation Acc:  0.8553\n",
      "Iteration  411 : Train Loss =  0.5667949   Train Acc:  0.8710833   Validation Loss =  0.6025857   Validation Acc:  0.8531\n",
      "Iteration  412 : Train Loss =  0.56595504   Train Acc:  0.87151664   Validation Loss =  0.60179746   Validation Acc:  0.853\n",
      "Iteration  413 : Train Loss =  0.5673677   Train Acc:  0.87111664   Validation Loss =  0.60290974   Validation Acc:  0.8538\n",
      "Iteration  414 : Train Loss =  0.5684309   Train Acc:  0.8703667   Validation Loss =  0.6044542   Validation Acc:  0.8527\n",
      "Iteration  415 : Train Loss =  0.56740826   Train Acc:  0.87016666   Validation Loss =  0.60304916   Validation Acc:  0.8557\n",
      "Iteration  416 : Train Loss =  0.56556773   Train Acc:  0.87168336   Validation Loss =  0.60147196   Validation Acc:  0.8535\n",
      "Iteration  417 : Train Loss =  0.56513083   Train Acc:  0.87175   Validation Loss =  0.60109115   Validation Acc:  0.8531\n",
      "Iteration  418 : Train Loss =  0.5661647   Train Acc:  0.87151664   Validation Loss =  0.6019385   Validation Acc:  0.8559\n",
      "Iteration  419 : Train Loss =  0.5668454   Train Acc:  0.87133336   Validation Loss =  0.6029781   Validation Acc:  0.8536\n",
      "Iteration  420 : Train Loss =  0.56612194   Train Acc:  0.87091666   Validation Loss =  0.6018714   Validation Acc:  0.856\n",
      "Iteration  421 : Train Loss =  0.5649599   Train Acc:  0.8717   Validation Loss =  0.60102373   Validation Acc:  0.8529\n",
      "Iteration  422 : Train Loss =  0.5646716   Train Acc:  0.87193334   Validation Loss =  0.60066456   Validation Acc:  0.8536\n",
      "Iteration  423 : Train Loss =  0.56542265   Train Acc:  0.87135   Validation Loss =  0.6015104   Validation Acc:  0.8543\n",
      "Iteration  424 : Train Loss =  0.5658184   Train Acc:  0.8718   Validation Loss =  0.60199225   Validation Acc:  0.8543\n",
      "Iteration  425 : Train Loss =  0.5657636   Train Acc:  0.87128335   Validation Loss =  0.60188913   Validation Acc:  0.8549\n",
      "Iteration  426 : Train Loss =  0.56499416   Train Acc:  0.8718167   Validation Loss =  0.60097396   Validation Acc:  0.8535\n",
      "Iteration  427 : Train Loss =  0.56493473   Train Acc:  0.8713833   Validation Loss =  0.60118   Validation Acc:  0.8537\n",
      "Iteration  428 : Train Loss =  0.5657492   Train Acc:  0.87158334   Validation Loss =  0.6015543   Validation Acc:  0.8543\n",
      "Iteration  429 : Train Loss =  0.566944   Train Acc:  0.87006664   Validation Loss =  0.60341537   Validation Acc:  0.8532\n",
      "Iteration  430 : Train Loss =  0.5679463   Train Acc:  0.86985   Validation Loss =  0.6037161   Validation Acc:  0.8533\n",
      "Iteration  431 : Train Loss =  0.5687874   Train Acc:  0.8689833   Validation Loss =  0.605383   Validation Acc:  0.8523\n",
      "Iteration  432 : Train Loss =  0.5698831   Train Acc:  0.8691667   Validation Loss =  0.6057695   Validation Acc:  0.8518\n",
      "Iteration  433 : Train Loss =  0.5714262   Train Acc:  0.8681833   Validation Loss =  0.6080079   Validation Acc:  0.8514\n",
      "Iteration  434 : Train Loss =  0.5701664   Train Acc:  0.86915   Validation Loss =  0.60613525   Validation Acc:  0.8522\n",
      "Iteration  435 : Train Loss =  0.5682396   Train Acc:  0.8700333   Validation Loss =  0.6045592   Validation Acc:  0.8533\n",
      "Iteration  436 : Train Loss =  0.5641719   Train Acc:  0.87203336   Validation Loss =  0.60013604   Validation Acc:  0.8533\n",
      "Iteration  437 : Train Loss =  0.5629107   Train Acc:  0.87236667   Validation Loss =  0.5990075   Validation Acc:  0.8545\n",
      "Iteration  438 : Train Loss =  0.5645802   Train Acc:  0.87158334   Validation Loss =  0.60097307   Validation Acc:  0.8547\n",
      "Iteration  439 : Train Loss =  0.566447   Train Acc:  0.8709667   Validation Loss =  0.6026818   Validation Acc:  0.8537\n",
      "Iteration  440 : Train Loss =  0.5676649   Train Acc:  0.8701   Validation Loss =  0.6043243   Validation Acc:  0.8521\n",
      "Iteration  441 : Train Loss =  0.5659342   Train Acc:  0.87076664   Validation Loss =  0.60205066   Validation Acc:  0.8539\n",
      "Iteration  442 : Train Loss =  0.5639908   Train Acc:  0.8714833   Validation Loss =  0.60041213   Validation Acc:  0.8537\n",
      "Iteration  443 : Train Loss =  0.56269664   Train Acc:  0.8724833   Validation Loss =  0.59875244   Validation Acc:  0.8557\n",
      "Iteration  444 : Train Loss =  0.56266475   Train Acc:  0.8728167   Validation Loss =  0.59894425   Validation Acc:  0.8542\n",
      "Iteration  445 : Train Loss =  0.5634434   Train Acc:  0.87203336   Validation Loss =  0.5998413   Validation Acc:  0.8547\n",
      "Iteration  446 : Train Loss =  0.5638274   Train Acc:  0.8722   Validation Loss =  0.6001824   Validation Acc:  0.8531\n",
      "Iteration  447 : Train Loss =  0.56395113   Train Acc:  0.87188333   Validation Loss =  0.6006224   Validation Acc:  0.8544\n",
      "Iteration  448 : Train Loss =  0.56336606   Train Acc:  0.87205   Validation Loss =  0.59963197   Validation Acc:  0.8551\n",
      "Iteration  449 : Train Loss =  0.56268406   Train Acc:  0.87228334   Validation Loss =  0.5992639   Validation Acc:  0.8533\n",
      "Iteration  450 : Train Loss =  0.56216615   Train Acc:  0.8724833   Validation Loss =  0.59838927   Validation Acc:  0.8567\n",
      "Iteration  451 : Train Loss =  0.5619472   Train Acc:  0.87285   Validation Loss =  0.5984377   Validation Acc:  0.8545\n",
      "Iteration  452 : Train Loss =  0.561906   Train Acc:  0.87265   Validation Loss =  0.5983716   Validation Acc:  0.8549\n",
      "Iteration  453 : Train Loss =  0.5618762   Train Acc:  0.8728167   Validation Loss =  0.59835947   Validation Acc:  0.8552\n",
      "Iteration  454 : Train Loss =  0.56192994   Train Acc:  0.8725833   Validation Loss =  0.5985962   Validation Acc:  0.8551\n",
      "Iteration  455 : Train Loss =  0.56190157   Train Acc:  0.87275   Validation Loss =  0.5982454   Validation Acc:  0.856\n",
      "Iteration  456 : Train Loss =  0.56183773   Train Acc:  0.87266666   Validation Loss =  0.5985344   Validation Acc:  0.8545\n",
      "Iteration  457 : Train Loss =  0.56167877   Train Acc:  0.8728333   Validation Loss =  0.598007   Validation Acc:  0.8561\n",
      "Iteration  458 : Train Loss =  0.56146157   Train Acc:  0.8727   Validation Loss =  0.5982387   Validation Acc:  0.8546\n",
      "Iteration  459 : Train Loss =  0.56113183   Train Acc:  0.87278336   Validation Loss =  0.5976364   Validation Acc:  0.8565\n",
      "Iteration  460 : Train Loss =  0.5608277   Train Acc:  0.87326664   Validation Loss =  0.5975864   Validation Acc:  0.8548\n",
      "Iteration  461 : Train Loss =  0.56055915   Train Acc:  0.87305   Validation Loss =  0.5971991   Validation Acc:  0.8557\n",
      "Iteration  462 : Train Loss =  0.5603789   Train Acc:  0.87343335   Validation Loss =  0.5970198   Validation Acc:  0.8549\n",
      "Iteration  463 : Train Loss =  0.56033456   Train Acc:  0.8732333   Validation Loss =  0.5970918   Validation Acc:  0.8563\n",
      "Iteration  464 : Train Loss =  0.5603728   Train Acc:  0.87355   Validation Loss =  0.5969849   Validation Acc:  0.8549\n",
      "Iteration  465 : Train Loss =  0.5604966   Train Acc:  0.87325   Validation Loss =  0.5974196   Validation Acc:  0.8544\n",
      "Iteration  466 : Train Loss =  0.56067437   Train Acc:  0.87296665   Validation Loss =  0.59730184   Validation Acc:  0.8569\n",
      "Iteration  467 : Train Loss =  0.5608803   Train Acc:  0.87303334   Validation Loss =  0.5978974   Validation Acc:  0.8545\n",
      "Iteration  468 : Train Loss =  0.5610937   Train Acc:  0.87296665   Validation Loss =  0.59768516   Validation Acc:  0.8555\n",
      "Iteration  469 : Train Loss =  0.5612869   Train Acc:  0.8725167   Validation Loss =  0.59835315   Validation Acc:  0.8542\n",
      "Iteration  470 : Train Loss =  0.5614142   Train Acc:  0.87305   Validation Loss =  0.5979862   Validation Acc:  0.8551\n",
      "Iteration  471 : Train Loss =  0.5614479   Train Acc:  0.87245   Validation Loss =  0.59858775   Validation Acc:  0.8545\n",
      "Iteration  472 : Train Loss =  0.56138766   Train Acc:  0.873   Validation Loss =  0.5980083   Validation Acc:  0.8557\n",
      "Iteration  473 : Train Loss =  0.56124765   Train Acc:  0.8725167   Validation Loss =  0.598428   Validation Acc:  0.8545\n",
      "Iteration  474 : Train Loss =  0.5610581   Train Acc:  0.8731833   Validation Loss =  0.59776723   Validation Acc:  0.8559\n",
      "Iteration  475 : Train Loss =  0.5609646   Train Acc:  0.87265   Validation Loss =  0.59812754   Validation Acc:  0.8553\n",
      "Iteration  476 : Train Loss =  0.56085575   Train Acc:  0.87336665   Validation Loss =  0.59767807   Validation Acc:  0.855\n",
      "Iteration  477 : Train Loss =  0.56125104   Train Acc:  0.8725   Validation Loss =  0.59837   Validation Acc:  0.8551\n",
      "Iteration  478 : Train Loss =  0.56140983   Train Acc:  0.87338334   Validation Loss =  0.59835374   Validation Acc:  0.8555\n",
      "Iteration  479 : Train Loss =  0.56255955   Train Acc:  0.87198335   Validation Loss =  0.5996231   Validation Acc:  0.8557\n",
      "Iteration  480 : Train Loss =  0.562848   Train Acc:  0.87228334   Validation Loss =  0.5999052   Validation Acc:  0.8547\n",
      "Iteration  481 : Train Loss =  0.56411177   Train Acc:  0.87135   Validation Loss =  0.60113555   Validation Acc:  0.8563\n",
      "Iteration  482 : Train Loss =  0.5636368   Train Acc:  0.87195   Validation Loss =  0.6007872   Validation Acc:  0.8546\n",
      "Iteration  483 : Train Loss =  0.56297505   Train Acc:  0.87215   Validation Loss =  0.59991604   Validation Acc:  0.8548\n",
      "Iteration  484 : Train Loss =  0.5613826   Train Acc:  0.87336665   Validation Loss =  0.598588   Validation Acc:  0.8544\n",
      "Iteration  485 : Train Loss =  0.55970997   Train Acc:  0.8735   Validation Loss =  0.59658724   Validation Acc:  0.8564\n",
      "Iteration  486 : Train Loss =  0.5589685   Train Acc:  0.8735667   Validation Loss =  0.59628236   Validation Acc:  0.8558\n",
      "Iteration  487 : Train Loss =  0.5592904   Train Acc:  0.8739167   Validation Loss =  0.59627146   Validation Acc:  0.8562\n",
      "Iteration  488 : Train Loss =  0.56070554   Train Acc:  0.87268335   Validation Loss =  0.59814346   Validation Acc:  0.8553\n",
      "Iteration  489 : Train Loss =  0.56199753   Train Acc:  0.8725833   Validation Loss =  0.5990778   Validation Acc:  0.8545\n",
      "Iteration  490 : Train Loss =  0.56433046   Train Acc:  0.8717   Validation Loss =  0.6017763   Validation Acc:  0.8541\n",
      "Iteration  491 : Train Loss =  0.5630251   Train Acc:  0.8718167   Validation Loss =  0.6001096   Validation Acc:  0.8539\n",
      "Iteration  492 : Train Loss =  0.56206805   Train Acc:  0.87225   Validation Loss =  0.59936273   Validation Acc:  0.8558\n",
      "Iteration  493 : Train Loss =  0.55904484   Train Acc:  0.8742667   Validation Loss =  0.5960463   Validation Acc:  0.8553\n",
      "Iteration  494 : Train Loss =  0.5576339   Train Acc:  0.87435   Validation Loss =  0.5948194   Validation Acc:  0.8558\n",
      "Iteration  495 : Train Loss =  0.5580671   Train Acc:  0.8741   Validation Loss =  0.5952052   Validation Acc:  0.8566\n",
      "Iteration  496 : Train Loss =  0.5593965   Train Acc:  0.87368333   Validation Loss =  0.5966765   Validation Acc:  0.8559\n",
      "Iteration  497 : Train Loss =  0.56073433   Train Acc:  0.8724333   Validation Loss =  0.5980517   Validation Acc:  0.8565\n",
      "Iteration  498 : Train Loss =  0.56006205   Train Acc:  0.8735333   Validation Loss =  0.5973565   Validation Acc:  0.8554\n",
      "Iteration  499 : Train Loss =  0.55917555   Train Acc:  0.8733   Validation Loss =  0.59651464   Validation Acc:  0.8563\n",
      "Iteration  500 : Train Loss =  0.5576741   Train Acc:  0.8746167   Validation Loss =  0.59487593   Validation Acc:  0.856\n",
      "Iteration  501 : Train Loss =  0.55717874   Train Acc:  0.87438333   Validation Loss =  0.59465086   Validation Acc:  0.8564\n",
      "Iteration  502 : Train Loss =  0.5577258   Train Acc:  0.8742   Validation Loss =  0.59495485   Validation Acc:  0.8569\n",
      "Iteration  503 : Train Loss =  0.5589188   Train Acc:  0.87416667   Validation Loss =  0.59661764   Validation Acc:  0.8557\n",
      "Iteration  504 : Train Loss =  0.5597735   Train Acc:  0.8735   Validation Loss =  0.5970065   Validation Acc:  0.8562\n",
      "Iteration  505 : Train Loss =  0.5604018   Train Acc:  0.8728667   Validation Loss =  0.59815925   Validation Acc:  0.8557\n",
      "Iteration  506 : Train Loss =  0.5599916   Train Acc:  0.87333333   Validation Loss =  0.59711635   Validation Acc:  0.8559\n",
      "Iteration  507 : Train Loss =  0.55993927   Train Acc:  0.87256664   Validation Loss =  0.5977036   Validation Acc:  0.8556\n",
      "Iteration  508 : Train Loss =  0.5600817   Train Acc:  0.8731833   Validation Loss =  0.59720755   Validation Acc:  0.855\n",
      "Iteration  509 : Train Loss =  0.5610799   Train Acc:  0.8720833   Validation Loss =  0.59902734   Validation Acc:  0.8548\n",
      "Iteration  510 : Train Loss =  0.5623237   Train Acc:  0.87201667   Validation Loss =  0.59954554   Validation Acc:  0.8546\n",
      "Iteration  511 : Train Loss =  0.56341815   Train Acc:  0.87075   Validation Loss =  0.60142183   Validation Acc:  0.853\n",
      "Iteration  512 : Train Loss =  0.56133246   Train Acc:  0.8727   Validation Loss =  0.59848976   Validation Acc:  0.8547\n",
      "Iteration  513 : Train Loss =  0.5586485   Train Acc:  0.87285   Validation Loss =  0.5962919   Validation Acc:  0.8559\n",
      "Iteration  514 : Train Loss =  0.5562581   Train Acc:  0.87488335   Validation Loss =  0.59348965   Validation Acc:  0.8566\n",
      "Iteration  515 : Train Loss =  0.5561366   Train Acc:  0.87548333   Validation Loss =  0.5935634   Validation Acc:  0.8568\n",
      "Iteration  516 : Train Loss =  0.5577228   Train Acc:  0.87381667   Validation Loss =  0.5954888   Validation Acc:  0.8562\n",
      "Iteration  517 : Train Loss =  0.5588147   Train Acc:  0.87408334   Validation Loss =  0.59632087   Validation Acc:  0.8556\n",
      "Iteration  518 : Train Loss =  0.5595004   Train Acc:  0.87291664   Validation Loss =  0.59750396   Validation Acc:  0.8549\n",
      "Iteration  519 : Train Loss =  0.55855906   Train Acc:  0.87403333   Validation Loss =  0.59594405   Validation Acc:  0.8562\n",
      "Iteration  520 : Train Loss =  0.5575438   Train Acc:  0.8735667   Validation Loss =  0.59536755   Validation Acc:  0.8554\n",
      "Iteration  521 : Train Loss =  0.55680573   Train Acc:  0.8746167   Validation Loss =  0.59417737   Validation Acc:  0.8572\n",
      "Iteration  522 : Train Loss =  0.55673504   Train Acc:  0.8749667   Validation Loss =  0.59452075   Validation Acc:  0.8564\n",
      "Iteration  523 : Train Loss =  0.5568862   Train Acc:  0.8745667   Validation Loss =  0.5945424   Validation Acc:  0.8572\n",
      "Iteration  524 : Train Loss =  0.5568556   Train Acc:  0.875   Validation Loss =  0.59472406   Validation Acc:  0.8571\n",
      "Iteration  525 : Train Loss =  0.5566451   Train Acc:  0.87478334   Validation Loss =  0.5944704   Validation Acc:  0.8566\n",
      "Iteration  526 : Train Loss =  0.5558631   Train Acc:  0.8754167   Validation Loss =  0.5935788   Validation Acc:  0.857\n",
      "Iteration  527 : Train Loss =  0.55532604   Train Acc:  0.8754   Validation Loss =  0.5930954   Validation Acc:  0.8573\n",
      "Iteration  528 : Train Loss =  0.55510765   Train Acc:  0.8753833   Validation Loss =  0.59269667   Validation Acc:  0.857\n",
      "Iteration  529 : Train Loss =  0.5553483   Train Acc:  0.87513334   Validation Loss =  0.59322256   Validation Acc:  0.8571\n",
      "Iteration  530 : Train Loss =  0.55580616   Train Acc:  0.8749167   Validation Loss =  0.59345716   Validation Acc:  0.8575\n",
      "Iteration  531 : Train Loss =  0.5562874   Train Acc:  0.87505   Validation Loss =  0.59428906   Validation Acc:  0.8561\n",
      "Iteration  532 : Train Loss =  0.55637294   Train Acc:  0.87471664   Validation Loss =  0.5940505   Validation Acc:  0.8573\n",
      "Iteration  533 : Train Loss =  0.556153   Train Acc:  0.875   Validation Loss =  0.5940868   Validation Acc:  0.8562\n",
      "Iteration  534 : Train Loss =  0.5555539   Train Acc:  0.8748   Validation Loss =  0.59319204   Validation Acc:  0.8575\n",
      "Iteration  535 : Train Loss =  0.5549362   Train Acc:  0.87535   Validation Loss =  0.5928345   Validation Acc:  0.8571\n",
      "Iteration  536 : Train Loss =  0.5544175   Train Acc:  0.8760167   Validation Loss =  0.59218276   Validation Acc:  0.8575\n",
      "Iteration  537 : Train Loss =  0.55419445   Train Acc:  0.8761   Validation Loss =  0.5921774   Validation Acc:  0.8581\n",
      "Iteration  538 : Train Loss =  0.55422413   Train Acc:  0.8763667   Validation Loss =  0.5921361   Validation Acc:  0.8575\n",
      "Iteration  539 : Train Loss =  0.5544393   Train Acc:  0.8755   Validation Loss =  0.59246725   Validation Acc:  0.8578\n",
      "Iteration  540 : Train Loss =  0.55466527   Train Acc:  0.8759   Validation Loss =  0.5925774   Validation Acc:  0.8572\n",
      "Iteration  541 : Train Loss =  0.5550151   Train Acc:  0.87505   Validation Loss =  0.59304845   Validation Acc:  0.857\n",
      "Iteration  542 : Train Loss =  0.5550867   Train Acc:  0.8756667   Validation Loss =  0.5929585   Validation Acc:  0.8573\n",
      "Iteration  543 : Train Loss =  0.5553293   Train Acc:  0.8745833   Validation Loss =  0.59348005   Validation Acc:  0.8563\n",
      "Iteration  544 : Train Loss =  0.5552088   Train Acc:  0.8759   Validation Loss =  0.593066   Validation Acc:  0.8567\n",
      "Iteration  545 : Train Loss =  0.55540395   Train Acc:  0.87448335   Validation Loss =  0.5936785   Validation Acc:  0.8578\n",
      "Iteration  546 : Train Loss =  0.55547184   Train Acc:  0.8754333   Validation Loss =  0.59328264   Validation Acc:  0.8576\n",
      "Iteration  547 : Train Loss =  0.55601895   Train Acc:  0.8742833   Validation Loss =  0.59439564   Validation Acc:  0.856\n",
      "Iteration  548 : Train Loss =  0.5568946   Train Acc:  0.8747333   Validation Loss =  0.59461933   Validation Acc:  0.8568\n",
      "Iteration  549 : Train Loss =  0.55840594   Train Acc:  0.87296665   Validation Loss =  0.5969248   Validation Acc:  0.8553\n",
      "Iteration  550 : Train Loss =  0.560449   Train Acc:  0.8724667   Validation Loss =  0.5981181   Validation Acc:  0.8539\n",
      "Iteration  551 : Train Loss =  0.5625519   Train Acc:  0.8712   Validation Loss =  0.60122824   Validation Acc:  0.8549\n",
      "Iteration  552 : Train Loss =  0.56303734   Train Acc:  0.87045   Validation Loss =  0.6007119   Validation Acc:  0.8529\n",
      "Iteration  553 : Train Loss =  0.56135964   Train Acc:  0.8711   Validation Loss =  0.59994024   Validation Acc:  0.8557\n",
      "Iteration  554 : Train Loss =  0.5569655   Train Acc:  0.8743333   Validation Loss =  0.59475625   Validation Acc:  0.8565\n",
      "Iteration  555 : Train Loss =  0.55374336   Train Acc:  0.87558335   Validation Loss =  0.59197855   Validation Acc:  0.857\n",
      "Iteration  556 : Train Loss =  0.5536183   Train Acc:  0.8756667   Validation Loss =  0.5918962   Validation Acc:  0.8578\n",
      "Iteration  557 : Train Loss =  0.55594206   Train Acc:  0.8752667   Validation Loss =  0.594014   Validation Acc:  0.8565\n",
      "Iteration  558 : Train Loss =  0.55817354   Train Acc:  0.87296665   Validation Loss =  0.5969917   Validation Acc:  0.8553\n",
      "Iteration  559 : Train Loss =  0.5583565   Train Acc:  0.87411666   Validation Loss =  0.59631634   Validation Acc:  0.8555\n",
      "Iteration  560 : Train Loss =  0.55641544   Train Acc:  0.8735833   Validation Loss =  0.59514904   Validation Acc:  0.8558\n",
      "Iteration  561 : Train Loss =  0.5539293   Train Acc:  0.87626666   Validation Loss =  0.5919047   Validation Acc:  0.8576\n",
      "Iteration  562 : Train Loss =  0.55281377   Train Acc:  0.8760333   Validation Loss =  0.5911259   Validation Acc:  0.8577\n",
      "Iteration  563 : Train Loss =  0.5534899   Train Acc:  0.8757   Validation Loss =  0.59181917   Validation Acc:  0.8572\n",
      "Iteration  564 : Train Loss =  0.5546734   Train Acc:  0.87551665   Validation Loss =  0.5926882   Validation Acc:  0.8581\n",
      "Iteration  565 : Train Loss =  0.5551574   Train Acc:  0.87465   Validation Loss =  0.59381014   Validation Acc:  0.8572\n",
      "Iteration  566 : Train Loss =  0.5541291   Train Acc:  0.87595   Validation Loss =  0.5921347   Validation Acc:  0.8582\n",
      "Iteration  567 : Train Loss =  0.55280244   Train Acc:  0.8754   Validation Loss =  0.59138715   Validation Acc:  0.8576\n",
      "Iteration  568 : Train Loss =  0.5521002   Train Acc:  0.8770667   Validation Loss =  0.5903752   Validation Acc:  0.8587\n",
      "Iteration  569 : Train Loss =  0.5523633   Train Acc:  0.8764833   Validation Loss =  0.59074354   Validation Acc:  0.858\n",
      "Iteration  570 : Train Loss =  0.5530521   Train Acc:  0.8757833   Validation Loss =  0.5916853   Validation Acc:  0.8572\n",
      "Iteration  571 : Train Loss =  0.5533562   Train Acc:  0.8759   Validation Loss =  0.59155184   Validation Acc:  0.8584\n",
      "Iteration  572 : Train Loss =  0.5530368   Train Acc:  0.87525   Validation Loss =  0.5917449   Validation Acc:  0.8574\n",
      "Iteration  573 : Train Loss =  0.5523152   Train Acc:  0.8766   Validation Loss =  0.59051   Validation Acc:  0.8586\n",
      "Iteration  574 : Train Loss =  0.55173403   Train Acc:  0.8767667   Validation Loss =  0.5902868   Validation Acc:  0.8579\n",
      "Iteration  575 : Train Loss =  0.55160946   Train Acc:  0.87708336   Validation Loss =  0.59003425   Validation Acc:  0.8588\n",
      "Iteration  576 : Train Loss =  0.5518717   Train Acc:  0.8768333   Validation Loss =  0.59026843   Validation Acc:  0.8591\n",
      "Iteration  577 : Train Loss =  0.5522419   Train Acc:  0.8760833   Validation Loss =  0.59093165   Validation Acc:  0.8576\n",
      "Iteration  578 : Train Loss =  0.5523877   Train Acc:  0.8764833   Validation Loss =  0.5907208   Validation Acc:  0.859\n",
      "Iteration  579 : Train Loss =  0.55234045   Train Acc:  0.87615   Validation Loss =  0.5910663   Validation Acc:  0.8582\n",
      "Iteration  580 : Train Loss =  0.5521776   Train Acc:  0.87626666   Validation Loss =  0.59056365   Validation Acc:  0.8593\n",
      "Iteration  581 : Train Loss =  0.5523554   Train Acc:  0.8764   Validation Loss =  0.59094155   Validation Acc:  0.859\n",
      "Iteration  582 : Train Loss =  0.5535326   Train Acc:  0.87565   Validation Loss =  0.5921626   Validation Acc:  0.8568\n",
      "Iteration  583 : Train Loss =  0.55582285   Train Acc:  0.87486666   Validation Loss =  0.5942759   Validation Acc:  0.8565\n",
      "Iteration  584 : Train Loss =  0.56239516   Train Acc:  0.8718167   Validation Loss =  0.6014445   Validation Acc:  0.8546\n",
      "Iteration  585 : Train Loss =  0.56597346   Train Acc:  0.8692333   Validation Loss =  0.60441196   Validation Acc:  0.8513\n",
      "Iteration  586 : Train Loss =  0.57612365   Train Acc:  0.86675   Validation Loss =  0.61533684   Validation Acc:  0.8512\n",
      "Iteration  587 : Train Loss =  0.56106657   Train Acc:  0.8717333   Validation Loss =  0.5991045   Validation Acc:  0.8537\n",
      "Iteration  588 : Train Loss =  0.55276465   Train Acc:  0.87598336   Validation Loss =  0.5907084   Validation Acc:  0.8577\n",
      "Iteration  589 : Train Loss =  0.5539513   Train Acc:  0.8753167   Validation Loss =  0.59217197   Validation Acc:  0.8579\n",
      "Iteration  590 : Train Loss =  0.55927396   Train Acc:  0.87305   Validation Loss =  0.59736925   Validation Acc:  0.8545\n",
      "Iteration  591 : Train Loss =  0.5620247   Train Acc:  0.8719   Validation Loss =  0.60096014   Validation Acc:  0.8554\n",
      "Iteration  592 : Train Loss =  0.5535404   Train Acc:  0.87621665   Validation Loss =  0.59186155   Validation Acc:  0.8579\n",
      "Iteration  593 : Train Loss =  0.551468   Train Acc:  0.87701666   Validation Loss =  0.5896385   Validation Acc:  0.8589\n",
      "Iteration  594 : Train Loss =  0.55598927   Train Acc:  0.87335   Validation Loss =  0.59482247   Validation Acc:  0.8567\n",
      "Iteration  595 : Train Loss =  0.5577723   Train Acc:  0.8735333   Validation Loss =  0.5960474   Validation Acc:  0.8561\n",
      "Iteration  596 : Train Loss =  0.5564781   Train Acc:  0.8735   Validation Loss =  0.59535265   Validation Acc:  0.8565\n",
      "Iteration  597 : Train Loss =  0.55150217   Train Acc:  0.877   Validation Loss =  0.5900435   Validation Acc:  0.8584\n",
      "Iteration  598 : Train Loss =  0.55109876   Train Acc:  0.87738335   Validation Loss =  0.5895764   Validation Acc:  0.8592\n",
      "Iteration  599 : Train Loss =  0.5540853   Train Acc:  0.8746333   Validation Loss =  0.59321785   Validation Acc:  0.8574\n",
      "0.003\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEWCAYAAABPON1ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeZwU1bn4/89T1dvsMwzDjgIurLOw\nCBEUxSUaNRIlGhSNRL8x8ebG5JrEJN4YjTcmJnp/SYzXmE29LhcjrjEgBjW4BKOCgiyKsik7zAyz\nTy9VdX5/VE87wAADzEz3DM/79epXd9dy6qnq7qdOna46JcYYlFJKZS4r3QEopZQ6ME3USimV4TRR\nK6VUhtNErZRSGU4TtVJKZThN1EopleE0UavDIiKzROTvBxh/uohs7sqY0ulI1/dg2/MIyr1JRP7U\n0eWqrqWJuhsTkY0islNEcloN+38isqjVeyMiK0TEajXspyLy4JEs2xjzqDHms3st5/jDLU9EFonI\n/2tj+Iki8qyI7BKRahF5QUSGH6CcB0UkLiINyekXisiIw42rq+y9PQ9HWzsLY8zPjDH7bFfVvWii\n7v5s4FsHmWYAMLMLYukMhcBfgeFAX+At4NmDzPNLY0wuMBDYAvy5UyM8QiISSHcMKrNpou7+7gS+\nKyKFB5jml8BP2pMQROQVEZmRfD0lWVM+P/n+TBFZlnw9W0ReT75+NTn78mRN9kutyvtOsta/TUS+\ncqgrZ4x5yxjzZ2NMtTEmAfwKGC4ixe2Ytxl4HKhoFc8AEXkyWUPfICLXtxqXJSL/KyK7ReR9Ebmx\ndQ1176OGZO39p20tW0R+ICLrRKReRFaLyEWtxs0WkX+KyK9EpAq4da/teWNyO7Y8Ei1HQCLylWRs\n9SKyXkS+lhyeAzwPDGg13wARuVVEHmm17AtFZJWI1CSPYka2GrdRRL4rIu+JSK2I/EVEIgfbzqrz\naaLu/pYAi4DvHmCap4A6YHY7ynsFOD35+jRgPTC11ftX9p7BGNMyvtwYk2uM+UvyfT+gAL9mew3w\nPyJS1I4YDmQqsN0YU3WwCZPJ6zJgbfK9BTwHLE/GdCbwbRE5JznLLcAQYBhwNnDFEcS5DjgVf/1/\nAjwiIv1bjZ+Ev237Are3ntEY88vkdswFRgK7gJZtuhO4AMgHvgL8SkTGGWMagc8BW1vmNcZs3Wt7\nnAjMAb4NlADzgedEJNRqskuBc4GhQBnt+86oTqaJumf4MfBNESnZz3gD3AzcvNePsi2v4Cdk8JPi\nz1u9bzNRH0ACuM0YkzDGzAca8JswDouIDAL+B7jhIJN+V0RqgHrgFODK5PCTgBJjzG3GmLgxZj3w\nRz5tFroU+JkxZrcxZjNw9+HGaoyZa4zZaozxkjuuj4CJrSbZaoz5rTHGSdb89yEiWcAzwG+MMc8n\ny51njFlnfK8Af8ffIbTHl4B5xpiFyaOTu4AsYHKrae5Oxl2Nv1OraKMc1cU0UfcAxpiVwN+AHxxg\nmvnAZuBrBynuDeBEEemL/yN9CBgsIr3xE82rB5p5L1XGGKfV+yYg9xDmT0nuhP4O3GuMmXOQye8y\nxhTi146b+XTncCx+00BNywO4Cb9WC35b/qZW5bR+fajxfllElrVazhig9yGW/WdgjTHmF63K/ZyI\n/Cv5R2kNcN5e5R7IAODjljfGGC8Zx8BW02xv9fqwPy/VsTRR9xy3AF9lzx/d3v4TPzFl728CY0wT\nsBT/D8qVxpg4sBi/FrvOGFPZYRG3U7K55O/AX40xtx9s+hbGmE/w1+M3ydrpJmCDMaaw1SPPGHNe\ncpZtwKBWRQzeq8gm9tx2/fYT77H4NfV/B4qTO42VgLQO70Cxi8gPgBPxm4xahoWBJ/Frwn2T5c5v\nVe7BusLcir+zailP8Ndxy0HmU2mmibqHMMasxW/HvP4A0yzCTxhXHaS4V/CTTEszx6K93rdlB37b\n7pEIiEik1SMoIvnAC8A/jTH7PWLYH2PMQvwEdS3+GSP1IvL95B+HtoiMEZGTkpM/DvxQRIpEZCD+\nOre2DLg8Od+5fNoktLcc/KS5C/w/APFr1O0iIp/D/xwv2qtZJASEk+U6yelan9K3AygWkYL9FP04\ncH7yT+Eg8B0ghr8jVhlME3XPcht+kjiQHwG9DjLNK0AenzZz7P2+LbcC/5s81L/04KG26Xf4TRUt\njweAi/Dblr+y15kQxxxCuXcCNwIB/D/iKoANQCXwJ/w//MDffpuT414EnsBPZC2+BXweqAFm4bcf\n78MYsxr4b/xmpB1AKfDPQ4j3S/h/9r3fan3vM8bU4yfwx4HdwOX4py62LPcD/D8L1yc/hwF7xbUG\n/w/S3ybX/fPA55NHTSqDid44QKm2ich1wExjzP5qzkp1Ca1RK5UkIv3FP3fcEv/qx+8AT6c7LqX0\niiilPhUCfo9/DnEN8Bhwb1ojUgpt+lBKqYynTR9KKZXhOqXpo3fv3mbIkCGdUbRSSvVIS5curTTG\ntHl1cack6iFDhrBkyZLOKFoppXokEfl4f+O06UMppTKcJmqllMpwmqiVUirD6XnUSnVjiUSCzZs3\nE41G0x2KaqdIJMKgQYMIBoPtnkcTtVLd2ObNm8nLy2PIkCH4neGpTGaMoaqqis2bNzN06NB2z6dN\nH0p1Y9FolOLiYk3S3YSIUFxcfMhHQJqolermNEl3L4fzeWVMojaeR+X3Z9Hw+D3pDkUppTJKxiRq\nsSyq5i+lYeHz6Q5FKdVOVVVVVFRUUFFRQb9+/Rg4cGDqfTx+4G6ulyxZwvXX7/c+F/u1bNkyRIQF\nCxYcbtjdTkb9mWhnW7i1dekOQynVTsXFxSxbtgyAW2+9ldzcXL773e+mxjuOQyDQdpqZMGECEyZM\nOORlzpkzh1NOOYU5c+Zw7rnnHl7g3UzG1KgBAjlB3LqmdIehlDoCs2fP5utf/zqTJk3ixhtv5K23\n3uLkk09m7NixTJ48mTVr1gCwaNEiLrjgAsBP8ldffTWnn346w4YN4+67274BvDGGuXPn8uCDD7Jw\n4cI9/pT7xS9+QWlpKeXl5fzgB/5d29auXctZZ51FeXk548aNY926dZ289p0js2rUeRESVY3pDkOp\nbuknz61i9daOPSIdNSCfWz4/+pDn27x5M4sXL8a2berq6njttdcIBAK8+OKL3HTTTTz55JP7zPPB\nBx/wj3/8g/r6eoYPH8511123z7nGixcvZujQoRx33HGcfvrpzJs3jxkzZvD888/z7LPP8uabb5Kd\nnU11dTUAs2bN4gc/+AEXXXQR0WgUz/MOb0OkWUYl6kB+LtFNtekOQyl1hC655BJs2wagtraWq666\nio8++ggRIZFItDnP+eefTzgcJhwO06dPH3bs2MGgQYP2mGbOnDnMnDkTgJkzZ/LQQw8xY8YMXnzx\nRb7yla+Qne3fJL5Xr17U19ezZcsWLrroIsC/0KS7yqhEbRcV4DRvwXgeYmVUq4xSGe9war6dJSfn\n03ss33zzzUybNo2nn36ajRs3cvrpp7c5TzgcTr22bRvHcfYY77ouTz75JM8++yy333576uKR+vr6\nTlmHTJJR2TBQXAxG8HZtSncoSqkOUltby8CBAwF48MEHD7ucl156ibKyMjZt2sTGjRv5+OOPmTFj\nBk8//TRnn302DzzwAE1N/n9c1dXV5OXlMWjQIJ55xr9ZfCwWS43vbjIqUdvFfQBwtmxMbyBKqQ5z\n44038sMf/pCxY8fuU0s+FHPmzEk1Y7SYMWNG6uyPCy+8kAkTJlBRUcFdd90FwMMPP8zdd99NWVkZ\nkydPZvv27Ue0LunSKfdMnDBhgjmcGwc0zL2HTTf/D8f+fz8k+7wvd3hcSvU077//PiNHjkx3GOoQ\ntfW5ichSY0yb5ytmVI060Nf/48DZsSXNkSilVObIqERt9/d7k3J3ds/DE6WU6gztTtQiYovIuyLy\nt84KJjDoOADc6srOWoRSSnU7h1Kj/hbwfmcFAiBZuVhBg1O9uzMXo5RS3Uq7ErWIDALOB/7UueFA\nIMfC2a0XvSilVIv21qh/DdwI7Pf6SxG5VkSWiMiSXbt2HXZAdm4It7Z7nuuolFKd4aCJWkQuAHYa\nY5YeaDpjzB+MMROMMRNKSkoOO6BAfhZOw4G7R1RKZYau7uZ0yJAhVFYeff9htecS8inAhSJyHhAB\n8kXkEWPMFZ0SUGE+jWu1jVqp7iAd3ZwejQ5aozbG/NAYM8gYMwSYCbzcWUkawC7uhRcXTLM2fyjV\nHXVmN6dt2bhxI2eccQZlZWWceeaZfPLJJwDMnTuXMWPGUF5eztSpUwFYtWoVEydOpKKigrKyMj76\n6KMOXvvOkVGdMgEEevvNJs7mtQRPKEtzNEp1I8//ALav6Ngy+5XC5+445Nk6q5vTtnzzm9/kqquu\n4qqrruL+++/n+uuv55lnnuG2227jhRdeYODAgdTU1ABw33338a1vfYtZs2YRj8dxXfeQ1y0dDilR\nG2MWAYs6JZKkQN/+ADhb1mmiVqqb6qxuTtvyxhtv8NRTTwFw5ZVXcuONNwIwZcoUZs+ezaWXXsrF\nF18MwMknn8ztt9/O5s2bufjiiznhhBM6YnU7XebVqPsfA4C77ZM0R6JUN3MYNd/O0hndnB6q++67\njzfffJN58+Yxfvx4li5dyuWXX86kSZOYN28e5513Hr///e8544wzjmg5XSGjLiEHsAcMAcDZrv19\nKNUTdFQ3p/szefJkHnvsMQAeffRRTj31VADWrVvHpEmTuO222ygpKWHTpk2sX7+eYcOGcf311zN9\n+nTee++9Do+nM2Rcog4MPhEAZ9eONEeilOoIHdXNaYuysjIGDRrEoEGDuOGGG/jtb3/LAw88QFlZ\nGQ8//DC/+c1vAPje975HaWkpY8aMYfLkyZSXl/P4448zZswYKioqWLlyJV/+cvfopTOjujkFwBjW\nlI6kYMrx9Pt9p3UrolSPoN2cdk/duptTAESwswV3d8fepFMppbqrzEvUQCA3iFOrdyNXSinI1ESd\nn41TH0t3GEoplREyMlHbhbm4jUf+p4NSSvUEGZmoA72KcGOCiUXTHYpSSqVdZibqlsvIt21MaxxK\nKZUJMjJR2336AeBuWpvmSJRSBzJt2jReeOGFPYb9+te/5rrrrtvvPKeffjotp++ed955qX44Wrv1\n1lu56667DrjsZ555htWrV6fe//jHP+bFF188lPAP6Nvf/jYDBw7E8/bbDX+XychEHeg3GABn28dp\njkQpdSCXXXZZ6qrAFo899hiXXXZZu+afP38+hYWFh7XsvRP1bbfdxllnnXVYZe3N8zyefvppBg8e\nzCuvvNIhZR6JzEzUA/27ketl5Eplti9+8YvMmzcvdZOAjRs3snXrVk499VSuu+46JkyYwOjRo7nl\nllvanL/1jQBuv/12TjzxRE455ZRUV6gAf/zjHznppJMoLy9nxowZNDU1sXjxYv7617/yve99j4qK\nCtatW8fs2bN54oknAHjppZcYO3YspaWlXH311cRisdTybrnlFsaNG0dpaSkffPBBm3EtWrSI0aNH\nc9111zFnzpzU8B07dnDRRRdRXl5OeXk5ixcvBuChhx6irKyM8vJyrrzyyiPcqvvKuE6ZAOyWu5Hr\nZeRKtdsv3voFH1S3nXgO14heI/j+xO/vd3yvXr2YOHEizz//PNOnT+exxx7j0ksvRUS4/fbb6dWr\nF67rcuaZZ/Lee+9RVtZ2j5hLly7lscceY9myZTiOw7hx4xg/fjwAF198MV/96lcB+NGPfsSf//xn\nvvnNb3LhhRdywQUX8MUvfnGPsqLRKLNnz+all17ixBNP5Mtf/jK/+93v+Pa3vw1A7969eeedd7j3\n3nu56667+NOf9r0V7Jw5c7jsssuYPn06N910E4lEgmAwyPXXX89pp53G008/jeu6NDQ0sGrVKn76\n05+yePFievfuTXV19WFt6wPJyBq1VTwQsQ3OUXjLHaW6m9bNH62bPR5//HHGjRvH2LFjWbVq1R7N\nFHt77bXXuOiii8jOziY/P58LL7wwNW7lypWceuqplJaW8uijj7Jq1aoDxrNmzRqGDh3KiSf6/QZd\nddVVvPrqq6nxLV2ejh8/no0bN+4zfzweZ/78+XzhC18gPz+fSZMmpdrhX3755VT7u23bFBQU8PLL\nL3PJJZfQu3dvwN95dbSMrFGLZRHIEr0buVKH4EA13840ffp0/uM//oN33nmHpqYmxo8fz4YNG7jr\nrrt4++23KSoqYvbs2USjh3e67ezZs3nmmWcoLy/nwQcfZNGiRUcUb0t3qvvrSvWFF16gpqaG0tJS\nAJqamsjKykrdjSYdMrJGDWDnBnDrGtIdhlLqIHJzc5k2bRpXX311qjZdV1dHTk4OBQUF7Nixg+ef\nf/6AZUydOpVnnnmG5uZm6uvree6551Lj6uvr6d+/P4lEgkcffTQ1PC8vj/r6+n3KGj58OBs3bmTt\nWv+ssYcffpjTTjut3eszZ84c/vSnP7Fx40Y2btzIhg0bWLhwIU1NTZx55pn87ne/A8B1XWpraznj\njDOYO3cuVVVVAEdP0wdAID+CU6eXkSvVHVx22WUsX748lajLy8sZO3YsI0aM4PLLL2fKlCkHnH/c\nuHF86Utfory8nM997nOcdNJJqXH/9V//xaRJk5gyZQojRoxIDZ85cyZ33nknY8eOZd26danhkUiE\nBx54gEsuuYTS0lIsy+LrX/96u9ajqamJBQsWcP7556eG5eTkcMopp/Dcc8/xm9/8hn/84x+UlpYy\nfvx4Vq9ezejRo/nP//xPTjvtNMrLy7nhhhvataxDkXndnCZtu3Ia9Su3c+K773dQVEr1PNrNaffU\n/bs5TbKLCnGjBtNNbj6plFKdJWMTdaC4GIzg7tic7lCUUiqtMjdRlyQvI9+y7iBTKqVUz5axidru\nNwAAZ+vG9AailFJplrGJOtD/WACcHXoZuVLq6JaxidoemLyMfOe2NEeilFLplbmJut8QEINTWZXu\nUJRS+9ETuzldtGhRWq9CbEvGJmoJRQhEDE717nSHopTaj57azWmmydhEDWDnBHBr971EVCmVGXpq\nN6dtmTNnDqWlpYwZM4bvf9/vV8V1XWbPns2YMWMoLS3lV7/6FQB33303o0aNoqysjJkzZx7iVt1X\nRnbK1CKQG8apa053GEp1C9t/9jNi73dsN6fhkSPod9NN+x3fU7s53dvWrVv5/ve/z9KlSykqKuKz\nn/0szzzzDIMHD2bLli2sXLkSINWMc8cdd7BhwwbC4XCbTTuHKrNr1AXZuA3xdIehlDqAntbNaVve\nfvttTj/9dEpKSggEAsyaNYtXX32VYcOGsX79er75zW+yYMEC8vPzASgrK2PWrFk88sgjBAJHXh/O\n7Bp1YT5OUyXGGEQk3eEoldEOVPPtTD2tm9NDUVRUxPLly3nhhRe47777ePzxx7n//vuZN28er776\nKs899xy33347K1asOKKEndE16kBxMcYVTL32S61Upupp3Zy2ZeLEibzyyitUVlbiui5z5szhtNNO\no7KyEs/zmDFjBj/96U9555138DyPTZs2MW3aNH7xi19QW1tLQ8ORddmc0TVqu3cJAM7mtYRGtdmp\nlFIqA1x22WVcdNFFqSaQ1t2cDh48+JC6Oe3Tp0+b3ZyWlJQwadKkVHKeOXMmX/3qV7n77rtTfyLC\nnt2cOo7DSSed1O5uTlu89NJLDBo0KPV+7ty53HHHHUybNg1jDOeffz7Tp09n+fLlfOUrX0ndqfzn\nP/85rutyxRVXUFtbizGG66+//rDPbGmRsd2cAjQ8ciebfno/x95zG9lnXdIBkSnVs2g3p91Tj+nm\nFMDufwwA7tZP0hyJUkqlT0Yn6sDAIQA4ehm5UuooltmJetAJADiVO9MciVKZqzOaL1XnOZzPK6MT\nteQUYYU83Crt70OptkQiEaqqqjRZdxPGGKqqqohEIoc030HP+hCRCPAqEE5O/4Qxpu3rQTuaCIFs\nC6e6rksWp1R3M2jQIDZv3syuXbvSHYpqp0gksscZJe3RntPzYsAZxpgGEQkCr4vI88aYfx1OkIcq\nkB/GqTmycxCV6qmCwSBDhw5Ndxiqkx206cP4WjJlMPnosuOsYFEuTl2sqxanlFIZp11t1CJii8gy\nYCew0BjzZhvTXCsiS0RkSUcehgWKC0k0etoGp5Q6arUrURtjXGNMBTAImCgiY9qY5g/GmAnGmAkl\nJSUdFmCgTwl4grtza4eVqZRS3ckhnfVhjKkB/gGc2znh7CvQbyAAzsb3u2qRSimVUQ6aqEWkREQK\nk6+zgLOBju309gBSF71sWtdVi1RKqYzSnrM++gP/KyI2fmJ/3Bjzt84N61PBY5MXvWzZ2FWLVEqp\njHLQRG2MeQ8Y2wWxtMk+dgQAie16GblS6uiU0VcmAlj5fbDDHs5OvYxcKXV0yvhEjQiBXBunSm8e\noJQ6OmV+oqbl6sTGdIehlFJp0T0SdVEeTp3e5FYpdXTqHom6dxFOs4c5whtRKqVUd9QtEnWwT18w\ngrP143SHopRSXa5bJOrAwMEAOOtXpTkSpZTqet0iUQeH+edSx9fpZeRKqaNPt0jUoeHjAEh8rJeR\nK6WOPt0iUVt9hmKHPRJbtqQ7FKWU6nLdIlFjWQQLbBI79N6JSqmjT/dI1ECwVw6JKr0ll1Lq6NN9\nEnXfYhJ1Dsbz0h2KUkp1qW6TqEMDB2Bcwdm2Od2hKKVUl+o2iTo49HgAEqveTnMkSinVtbpNog6P\nHg9AbPU7aY5EKaW6VrdJ1IERkxDbI772w3SHopRSXarbJGrJLiRUKMQ+0buRK6WOLt0mUQOE++YR\n3643EFBKHV26VaIOHdOfRJ2L19SU7lCUUqrLdKtEHT7OvyN5bKWe+aGUOnp0q0QdqZgEQGzJa2mO\nRCmluk63StTBsWdgBTyiK95NdyhKKdVlulWiltzeREosoms/SXcoSinVZbpVogaIHFNCdFsDxnXT\nHYpSSnWJ7peoRw7HOBD7YGW6Q1FKqS7R7RJ11memAtC8aF6aI1FKqa7R7RJ1cPw5BLJcmt56I92h\nKKVUl+h2iVry+pA9METT6o8xxqQ7HKWU6nTdLlEDZI8ehlOfIPGJnv2hlOr5umeinnwqAE2Lnk9z\nJEop1fm6ZaIOnTwdO+TS9NqL6Q5FKaU6XbdM1FJyAtmDbBqXf6jt1EqpHq9bJmpEyB07Eqc+QWzN\nB+mORimlOlX3TNRAzmcvAKBx/uNpjuTo8fvf/55vfOMbqfebNm1i2rRpjBo1itGjR/Ob3/zmgPP2\n69eP8vJyjjvuOB566KEOj++ll17iiiuuOKR5Fi9ezI9//OPDWt7mzZv5y1/+0iFldcTyO9KCBQsY\nPnw4xx9/PHfcccchT9PWuGg0ysSJEykvL2f06NHccsstnRJ7j2SM6fDH+PHjTadrqDTrJh5nNl44\nrfOXpYwxxnzjG98wv/vd71Lvt27dapYuXWqMMaaurs6ccMIJZtWqVQed98033zTFxcUdHt9///d/\nmzvvvLPd0zuOc0TLe/DBB82NN954RGUczIFi7KzlO45jhg0bZtatW2disZgpKyvb53M90DT7G+d5\nnqmvrzfGGBOPx83EiRPNG2+80eHxd1fAErOfnNpta9TkFJNzfD5NH23Da2xMdzRHhffee4/S0tLU\n+/79+zNu3DgA8vLyGDlyJFu2bNnvvMOHDwdg6NChhEKh1LgNGzYwffp0JkyYwMSJE1mzZg0A77//\nPlOnTqWsrIw777yT44/370R/8skns2HDBgC2bNnC+PH+jY+XL19OeXk5AE888QSf+cxnKC8v55RT\nTmHXrl0AXHLJJXzta1/jM5/5DD//+c+55JJLeO01v9vcM844g4qKCioqKohEIjz++OP7Lev111/n\nhhtu4IknnqCiooL169enyvrggw9SZZ111llUVlYCcPHFF/OjH/2IqVOncswxx/Dii23/Gb53jO1d\n/v6246F66623OP744xk2bBihUIiZM2fy7LPPtnua/Y0TEXJzcwFIJBIkEglE5LBiPOrsL4MfyaNL\natTGmIbffs2sHj7C1C2Y1yXLO9oVFRWZmpqaNsdt2LDBDB482NTW1rY5vrCw0GzZssV4nmd+/OMf\nm/vvv98Y49eszjjjDLN27VpjjDHz5s0zs2fPNolEwowdO9a88847xhhjvv71r5vp06cb13VN//79\njed5xhhj5s+fb2bPnm2MMaa8vNzs3LnTGGNMZWVlatm33nqrueeee4wxxgwfPtzcfPPNqXEjRozY\nZ53uvfdec8kll6Rqs/sr65xzzjErVqzYp6xRo0aZd9991xhjzB133GFuuukmY4wxxx9/fKrG/9RT\nT6Xi3tveMbZn+fvbjq2dcsoppry8fJ/HwoUL95hu7ty55pprrkm9f+ihh8w3vvGNdk9zoHGO45jy\n8nKTk5PT6Ucj3Q0HqFEHDpbIRWQw8BDQFzDAH4wx+2+M7EJZZ38Rue8fNDz/BHnnnJfucHq0TZs2\nkZeXR0FBwT7jGhoamDFjBr/+9a/Jz89vc976+nrOO+88tmzZQllZGbfeeisAzzzzDKtWrWLGjBkA\nOI7DqaeeylNPPUV5eTljx44FYNSoUfTp04d169YxdOjQVE2spZafSCSora2lpKQEgAcffJC//OUv\nxGIxtm/fzs9+9jOi0SjV1dWpduRoNEo8Ht9jnR566CGef/55nnzySWzb3m9ZAGvWrGHEiBF7lLVg\nwQJOOeUUKioqUnH/9a9/pampidraWv7jP/4D8GuUhYWF+2yrvWNs7/L3tx1bazlySCfbtlm2bBk1\nNTVcdNFFrFy5kjFjxqQ7rIx30EQNOMB3jDHviEgesFREFhpjVndybAdlHX8aOf09Gv71DsYYPYzq\nRCtWrNij2aNFIpFgxowZzJo1i4svvni/806dOpWXX36Z3bt3M2bMGN544w0mT57M8uXLuf3227nm\nmmv2mOdHP/pRKtkBrFy5knPPPXefOJYsWcK1117L+++/z8iRIwE/2b711lu8/PLL5ObmMnXqVEaP\nHs2qVauYNGkSgYD/tV+1ahWjRo1KlTV37lweffRRnn32WYLB4AHLqqyspKCgYJ+yVq9evUd8K1as\nSA0fP358Kvm/9957bSaovR4q00MAACAASURBVGNs7/L3tx1bO/XUU6mvr99n+F133cVZZ52Vej9w\n4EA2bdqUer9582YGDhy4xzwHmqY98xcWFjJt2jQWLFigibodDtpGbYzZZox5J/m6HngfGHjgubqI\nHSRvwnCcmhjRVdrtaWfau30a/Gaza665hpEjR3LDDTcccN6WmnFRURGXX3458+b5vR/279+fF154\nAc/zAD+xGWMoLi7mww8/BGDZsmU88sgjlJeXU11dnaqJvv/++8ybN4+ysrI92qdXrFjB5MmTyc3N\n5cknn2Tx4sWUlpayYsUKysrKUnG1fv+3v/2Ne++9l6eeeopIJLLHNG2VtXHjRgYMGLBPWQMHDmT1\nar8Os379eh5++GG+/OUvs2LFij12PO+9994esbQV06Esf3/bsbXXXnuNZcuW7fNonaQBTjrpJD76\n6CM2bNhAPB7nscce48ILL2z3NPsbt2vXLmpqagBobm5m4cKFqSMCdWCH9GeiiAwBxgJvtjHuWhFZ\nIiJLWv646Qq5518CYmh4+pEuW+bRaMWKFfzhD39gyJAhDBkyhJNPPpl//vOfPPzww7z88supP+Hm\nz5/f5rwtiRrg85//fGq6q6++Gs/zGDlyJBUVFfziF79ARLjyyitZsmQJpaWl/PnPf2bIkCEMGzaM\nc845hwULFjBr1izmzp1LcXExffv2Zfny5alEOHv2bO69914mTpzIu+++y7Bhw8jJyTlgor7qqqvY\nvHkzU6ZMoaKigj//+c8HLGvEiBFUVlYyZswYFi9enCrryiuvZOvWrZSWljJz5kzuv/9+iouL90nU\n+zvk3zvG9i5/f9vxcAQCAe655x7OOeccRo4cyaWXXsro0aMBOO+889i6desBp9nfuG3btjFt2jTK\nyso46aSTOPvss7ngggsOK8ajjey9193vhCK5wCvA7caYpw407YQJE8ySJUs6ILx2aK7h43PG4wZK\nGPbyv7pmmarTNTQ0pM4QuPPOO6mtreWnP/1pmqNSqvOIyFJjzIS2xrWrRi0iQeBJ4NGDJekul1VI\nXukAYltriWtvej3Gr371K0aPHk1FRQUbN27k5ptvTndISqXNQWvU4h8//S9QbYz5dnsK7dIaNRD/\n2y9Z990H6PPvV1P879/rsuUqpVRHOdIa9RTgSuAMEVmWfGTUuXChKV8iXJigfoF2e6qU6nkOenqe\nMeZ1ILPPeys6lrzh+VS+uQ2nspJA797pjkgppTpM972EfC95Z58NQP1zT6Y5EqWU6lg9JlGHP3s1\nofwEdc/MTXcoSinVoXpMopY+w8kflU/Tmi0kduxIdzhKKdVhekyiBsg//3wA6p5+LM2RKKVUx+lR\niTp8xmwiRXHqns2sU72VUupI9KhETfFx5I8pIrphp178opTqMXpWogbyp38RMNTOeSDdoSilVIfo\ncYk6ePrV5PSLU/vsXzHJnsSUUqo763GJmtw+FEw+nkR1E03/0k6alFLdX89L1EDeJddiBT1qHvpd\nukNRSqkj1iMTtVX6eQqOc6l/fSnO7t3pDkcppY5Ij0zUBMIUXXgWxjHU/t9D6Y5GKaWOSM9M1ED4\ngm+T3SfG7v97BOO66Q5HKaUOW49N1PQZQdHkY0hUNdDwyqJ0R6OUUoet5yZqIO/y6wlkuez+42/T\nHYpSSh22Hp2oZfSFFI22aHx3DdE1a9IdjlJKHZYenaixAxTNugIr4FF1z3+nOxqllDosPTtRA/Zp\n/0bhiQnqXnqN+OYt6Q5HKaUOWY9P1GT3otel0wFD9X13pzsapZQ6ZD0/UQPBz32HgqFRap75G4nt\n29MdjlJKHZKjIlFTMIjeM6aB57LrV3emOxqllDokR0eiBkJfuJnCE5qp/et8YuvXpzscpZRqt6Mm\nUdNrGL0vuwDL9tj1y9vTHY1SSrXb0ZOogcD5N9NrVIz6RYtpXrYs3eEopVS7HFWJmrx+9LrycuyI\ny/Zbb9Y+QJRS3cLRlagB+8zv0ndigugHa6l5/PF0h6OUUgd11CVqsnuRf80Pye4TY+ddv8Spqkp3\nREopdUBHX6IG5KRr6HduX7zmZnbe8bN0h6OUUgd0VCZqLJvwlXdTPKKB2ufm0/Dqq+mOSCml9uvo\nTNQAg0+i96wLCRck2PqD7+stu5RSGevoTdSAdd7tDDgzgFtTw/abf4QxJt0hKaXUPo7qRE1WEZGr\n76FPWS31L75M1R/+mO6IlFJqH0d3ogY4/ix6zfoS+cc2setXv6Ju4cJ0R6SUUnvQRA3IuT+j/+cH\nEentsvV73yO6enW6Q1JKqRRN1ADBLKxZjzL4jBh2IMGm6/6NxM6d6Y5KKaUATdSfKhpC4Mo/MXjK\nDtzqXWz+xr/jRaPpjkoppTRR7+GEs4nMvI2BkyqJrljBlu98F5NIpDsqpdRRThP13k7+N/Iuv56+\n42ppeOkltnznu3jxeLqjUkodxQ6aqEXkfhHZKSIruyKgjDDtJnrNupy+Y2up//vf2fxv/4bX3Jzu\nqJRSR6n21KgfBM7t5Dgyiwicdye9/t919D+phsZ//pOPr7iCxBa9i7lSqusdNFEbY14Fqrsglswi\nAmfeTOF1NzFoSjXxtR+w4eIZNLz2erojU0odZTqsjVpErhWRJSKyZNeuXR1VbPpN/iZ5/3YXQ8/e\nQSDYxKZrr2XXPf+jNx1QSnWZDkvUxpg/GGMmGGMmlJSUdFSxmWHclYSuvp8hZ+2g4DhD5T338PGX\nryK+aVO6I1NKHQX0rI/2GnUh1tcW0v+z2fSftJvYquWsv3A6ux/7i9aulVKdShP1oehfhlz3OoVX\nXMuwz24lq1cz22+9lQ0XzyC65sN0R6eU6qHac3reHOANYLiIbBaRazo/rAwWzIKzbyN4/Qsc84Uc\nBpxcjbN5PRu+8AU2f+vbRN9/P90RKqV6GOmMPpgnTJhglixZ0uHlZpxEFBb9HOcfv6V6fV92rwnj\nNcfIOW0qvb/2dbLHjU13hB3KGINnwBIQETzPEHVcjAHXGIznP7uewTP+w/UMnof/2hg8zy+jZRpj\nICdsU5wTBqA+lsBx/WlrmxMELEEQ6qMJggGLgCUELAvbEgK2pN7nRgKEAxaVDTGaEy7ba6PkRQKI\nCAnHI+EaEq5H3PVwXIPBj81x/We31e9A8E/6sUQoyQvTJy9CfTTBxqpG4q4BY7Ati1DAIu54xB0X\nyxLqow5ZQRvbEmKOSzhgU5IXpiQvTMLxqGlO0BBzaIo51EcdwkG/ntQSm+e1+i2KpGJpPcgSoSg7\nSF4kyLbaKM1xh4BtUd0YRwQEwRKwLEmuhyQ/L39eEX+4/9r/HBpiDk1xN/UZN8VdbEuwLUl9Ri2f\nZctrr9V3wRLBsgRbBAMkHI9QwN8+xrR8b/zpPWMwAC2vDTieIea4iAi2gG0Jlny6/OrGOJGgnfos\nHc/zYwFskeTyP53PSq6zbUmyTL+scMAiHLT5pLqRprhLr+wQDTGHxriTmi/ueAAEAxaNsQSu8Qjb\nAYIBIWRbDCjM5riSHN5YV0VuOEBlY5ygJWSFbDbtbmJkv3zuuXwcttX6k2sfEVlqjJnQ5jhN1IfH\nGJP8gnm4n7xN9sIbsTavYNemwexeHYTGZhKlY6mdMYuGUWOJJX+MjmdwXD9xeMkyUgnDGFzPnybu\neMQcz39O+D+iYMAiaFkkPH+Y63nEXJe44xL34iTcKK4nJEwCx4vjmASu5+B4Lq7xMMbDEkDAJkIk\nECFkW8RMLQnPwXUtP0aiuK7nJ1ZjMMbzf2j+mpMdtMgL51DblCBmGgEPxAAuIBgvjFhxMALiggmC\nFUPEwxgLEEQcEA/EAQzGzfaHW1EwNlhxMBZggbifDk/GsCcbL5EP4iFWFLEcf34v5D9j/Hjw/Ngk\n2S2A5V9xKuJhvKAfj7FBHMSKgQlgvDDGiyDiIHYjxgSS4yywY+AF/DLsGMbJ/nSdxIARjJuLcbOw\nAnVgR8ENp9YJcf1t5wcDeIiYNtYvSVwQD+NmgbERuxGxoxg3TMD2wIphEPCCYDVhTAgkAcbGtGwL\nz8YYO7luIUQ87EAUy/anEwTL8ndkBhckhpgwiIMRl2SaB1wEG4OLmCz/MyQBBBBcDBbGGIwkEBMC\nK4qYoP8awUjMLxO/TJHkZ21ayvdfm9RnYQG2P19qm+F/mTHJ5X/63TCpbWj85XrZYEJ4HljBBiwB\nz7OTOywPg4OHC7gYcTHGfwbBIohnElhkQbwfsVg2kZydgIslATxiGDw8q4EIfXjry39HpGMTdeCQ\nS+shGmMOW2qaqW6MUx91qI8maIq7bKxspCGWYGt9Fc1xlwZ3N9FEnLjXQMx1SHhREl4Mx8QxEveT\ngsQRq5Rj+vRidP919B1bTa/V2ZS9+x69b32Xpl5hXhtVzD+H51Kb05K8LMAg4oLlAG7qtYibTGRu\n8kfvgQkgzQWAh7Eb/B8g+N/T4OFtg/qDjLeTj/3NaxVB1uEtussI/nYO2REsLKJuMxE7ggEidgTP\n+DU5x3MI2SEczyFoBckKZFOfqCfqRHG8OLYEyA3lYTBk2VlE3WZsCfjvA1mE7TDNzg5sCWJbNpZY\nRJ1mamOf0Ow0kh/qRU4wl5hbiYgQCYSxxcYWfwsbPCyxUg9hzx+6wWBJAMdz2d74CTE3xtD8odhW\nPgnXITuYi4dHTiAHxzgUhAqoj9cTsAMkXP+7EglESLgJEl6CuBunIdFA0AqRH+5NxI7geA4Allip\nqnxQgiAQtIIErSAJzy8raAVxPAeDodlpxhKLsB0m4SUIWkFcz8XDIyuQRVOiibxQHnE3TtSN4nou\n2cFswnaYgBXAMx7GGFzj4hlvj+ewHSYnmIPjOSTcBFnBLEJW6NO0bEwqKfoJX/Z4Bmh2mqmP19OY\naMTxHPrm9MUSi7gbx2AIWkECVoCABPznVg9jDHE3TsgOURur5cPdH1Id3c3xhWNSn3N2IBtLLArC\nBdhiH1aSPuj3uKfXqBOux+qtdby9sZp3PqliXe1adjRto9GtRgJ1WIFaJFCPBOrABLACTf5rObQz\nOQISJmSF/SYBt5kiJ46VcJiwWpj6nsWgbS6uBeuPz+P9CX34YHQ+blaIkB0iZAf9hxUiaAcJ2aHU\nDyNkhwhYAWqiNexs2kluKJeicBFZwSwsrNQXMmSHyAnmpH4okUDEL8/6NHGA/yN0PZeqaBUxN4Yg\nDMobRNyNE3NjZAeyyQvl7fFFT335kz8Az3hsrt9McVYxASuALTZZgSxibozcUC7NTnMqCYXsUKrc\ngBXwf4SeR9AOpn6IttjUxmpxPIeCcEHycNXGM37NKWSHUkkgZIX2+SFEnShV0SqCVjC1DQrDhTie\nQ9gO4xqXgOXXSVr/sNvLM15q3Q+HZ7zU9ldqf466po8tNc08/vYnvLZhDR/UvosXXoudtQkrUJ86\n3AWwsCkIFVMcKaE4qxcJL07/3BL65vShd1ZvXM+lJLuESCBCfig/WdPKStWgIoFIKiHu8yP2PNj4\nKiz7P1j9V2JVCWp3DqJuY4hEVQMSDJI1fjy5p55CzimnEj7+OMTeX/1VKdXTHTWJ+p1PdnPva0t5\nfecT2HkrsIJ1AOQGCikvqeDYggGUl5RzbMGx9M3uS69Ir66p6UTrYNXTsOz/MJ/8i+aqMPUNJ9C4\nxSa2yb+KU7KzyTv9NHLPOJPI6FGEhgzplEMopVRm6tGJ2hjD4nVV/PzlBayNPU8wbyUiwpT+pzHt\n2ClM6DeBoflDMyfpVa2DZY/Ciieg5mMSTTaNzcfRXF9E/epq3LpGAOzCQrLGjiVr3Fiyx40jOGgQ\nEgxiFxRozVupHqjHJupVW2u5Zf4iVkUfIZD3AWErh4tPuJirxsxiYO7ATl/+ETEGti2HD1+Aj1+H\nLe9iovVE6yJEE4NpbupL8yeNxLfs2GM2q6CA3ClTyBo7ltCQIURGjiDQu3eaVkIp1VF6XKLeXhvl\n9hf+xcKtjxAsfJuQHeHrZdcya9RlZAezO225ncpNwMf/hPWLYNPb8MliMB5O1KI5NhjH6oOxs4nu\n8mh4fyduzafnbNiFhQQHDyY0eBDBQYMJDh5EaPBg/3W/vkjgqD25R6luo9sn6t2NcV5bW8n6yire\n2rKadyr/SaDodSzLY8bxl/Dv475OcVZxhy0vIzRVw5Z3YNu7sHUZ1HwCjbugYQfG83CjFjG3H9Fo\nCfHGbBL1EK9qIrGzGlr3PWJZBHr3JtCvH8G+fQiUlGAXFmIXFGAVFGAXFBDs2xe7uBixbexevRBL\nz1BQqqt16/OoN1bVM/2x7+JGliGBJgCCxTB1wFn8YNINDM4fnOYIO0l2LzjhLP/RWqwB+eQNAtuW\nE6j8kJxda6DyPUj4bdvGg0Q0RMIaRDxeRCIWwWm2cerqiH2wk8Z/1eM1NPlNL22wcnMJDhiAlZeH\nnZuLlZuLlZebfJ2HlRXBbWzEq60j0K8f4eOGIaEwwYEDEMvCLinBCoU6e+sodVTJ+ER948J78HIX\nM7HkDMb3H8nI4hMZ2Wsk/XP7pzu09Ajnwgln+48Wngd1m2HXh0jtJ4RqNhGq/JCc2s1Q9xE07tyj\nCGPASwhuzML1sonbx+DZRRjPIl5jSDQZvGgVTs0O3KiL19SE19C0x41+JRzCxNq4l6QIVpZ/GYxd\nVIRVkI8VycKKhJFIFnZeHlZODsZzwfX8Z8fF2V2N2AFCxxxDoF8/JBwiNPgYTCyKlZdPoE8JVjiM\nhEJg2/4fqpYNGJxt22h49TUkEib/nHNwd+8m0H8AEgwgoTZOnVSqm8nopo+d9Q2cMfcs+oSO4+Ur\nHu2AyI5SThwatkP9Dv+5qQriTeA5ULcFtq/0E73nQv128Nq+87rnWRhHsAIuYkEiUUAiMAjj2SSc\nAgwWTjSIl/D7eHCjHl6zi+cYjOPhxR3chiZMcxQsy0+2to1YFlZWGIwhvm0nJhrtsFW38vKwexVh\nEglCAwbiJeLYefl+wrcEK5KFBINIwE/8YlsgFlgWXl0diR07iH34IcF+/ciZfDJOVTVWdjahY4/1\nD1/sAFYkDGIhoSBi2xi/Qwzc+joSH39M7KO1BPr1IzJmNF5tLYF+/QkOGIC7u5pAn77Y+Xn+3rP1\nNkk+43kY1/V3ko6DW19P01tv4e6uIf/885BwGLe2lqyKCpydO5FQCLuwEDwPKxLZZ3sYz8PEYnjR\nKCZ5H9BAf7/SY+JxrHC4w7Z9T+TW17P7scco/OIXCRQVdWjZ3bbp4ycvzUHsRr5aflW6Q+neAiEo\nPMZ/HIybgIYd4MahYZefwONNEG/Aaqz0e/fJLoZQLsEtSwjWbYVEM9Ru8hNX/XZ/3sNkQvl4dgFe\nwpDw+iK2gxsP4ppcjGvheRZYQZAAxhMwHoFAI1nWapzmIA0NwwiXhEjEc/Ga47iOhRv1EIFETQN2\nMIBbtQPjuOAZvFgM4yQToTH+0YnnYTwPKyebYO9e5FScQHzLLqoeeJBAcTFufX0qyR2MZGURGjyY\n5lUrqX3qqcPeLnsWKkggQPX//u8BJ7Nyc/1k7zhgWZhEos2dYKBPH7xYzN+J9O+/ZwJqORoRAdtC\n7IC/EwnYiGWndiLGSYBnUuNMNOYfiTU3Y+VkEyjqBZZFfP16JBIhOGAAdq8ivIZGf0edm4vX3Ixb\nW4sEAri1tbg1NURGjSJQ3AuvsRHjGSQU9I+swhEkHEIsCxNPYBwHCYVSR3Otj9hMNIZTXYVbVY27\nezfh4cOxexWR2LyFQHEvgscc43cR4rlI2N+5GSeBlZ3tdx7V1ITX2IhbU0Pd88/j1dVR/8LfsQsK\nMJ5LeOgwvOZmwiecgNfcRMk3vtEhH3NrGVuj/mhXJRc9+wXyQoW8fsWz2JaeO9wteC44Mf91tBai\nNRCr3/ORaP40AbQI5wMGNr0F8UY/6ddugkDYn75hh78TcWKQaPKHtXS8kz8IBlRA/TbYshSyekFz\nx9/m0wRzEacJY+fgRgYkOx8KYozt9xIXzAcJ4HeU4WIRJcAupHEXxgoRzyknmNhEgj4k3CICwWYS\nDQbj2v7O1AphCGCsABDAYPm1/ICNWILgIIlqsnKrkcYd1NcOgaxCLDtBtC6bYJ6FkSBeTMBL4NTH\nk0cJAQw2EhCscBAraCGmGcs04xGkeVMUKydCIDdAfGcDXswl2e2df3Qhlr+lXQ/jOv6OzHExruvH\nFgj4ZxaJgOtgHBeJhLGysrDCQbzmKE5NPbgeoWMHYeJxEjuqcHfvxsrN9Tv9amjEikSwCwsxTgI7\nJxsrJ0L0/bW4TY3YObl77Gy8eBwTjfo71GAQgkFMLPbpjsiyUkdrEg4T6NULu1cRdk4WTe+txMQT\nBAcOSCXvg7IsrJwcskYcR8DbRu3yKqysLLwG/ypjycrCq60lOHgwx82fhwQPvQOeblmjvvOfjyLB\nWn4y5S5N0t2JZUMoeYpkKBvyD/G/hPKZ7ZvOGD+ZtyyzZVi0BiKF/lkyoVxo3u0nbcuGuq1gh/0d\niBvz5/ecZEJKdisIn74W8Wvv+f2hYSey6U3IKkKaqgk07AA76DcrtTQVNe8Gr9kv1wpAMBtyT4K8\nfkjdVsLb34OBowjXbSFc9w5E+hAJxvzucp3m5E6oeb9NTwCEC8A+DvofR2Hsdf9oJxAhP9gMe1eW\n89qY300+Wuvb6nWfdmz7Q5W/1zJygH75/k44Wudvq3Cevw2itXvOe0rYPxJs3urvqEM5ye+XALbf\n255xwSQAg/FsJNmbHiaR/I40gVfpN/lhMKXHI4XHwO4NmFAeJncc4CG4eBIBsRHx8BKW37RlxRC3\nEYnWwJb5APQrLUHsOMYEIK834jTg5H2GQG7gsJL0wWRsol5R/TYB6c1nj/tMukNRmUgExN53WFby\nsL3oWP85p9VpmwPHH9kyyy49svnby3XAifpNSMbzj1KsgL9jCOd9ejSSaPaTlx2Eyo8gfyDEav1k\nF873j15ayklE/Vq72GCHIK+f/2ishE3/8ndquX2gYad/Gqgd9pfjxPydmtuy85A9j4Za79Ta2tHZ\nYX/58Ub/zKRgtr9j3L3RHx7O89cxVgeBiP/52UE/xnAeVK+H3R/7Z0EFIhBv8HdOeyzHSr2WVq/9\n70irafL6QygbWfuiv40GjEOitUjNGn95loXdcjQnNpaX8D+LULYfSygXpnwbBlRgLf4thPORhp1+\n75f5fQnueAXsCn8eu2NTa0Ym6rpolHo+4PjsU9IdilJdzw6AnXvw6YJZ/gOgf5n/nHOI1xMUDISC\nGYc2T3c3+ZtHXsboi468jEOQkVc2PLXqLcSOMXXwyekORSml0i4jE/XC9a8B8KUxZ6Q5EqWUSr+M\nTNRrapcR9gYyML8k3aEopVTaZVyi3lZXT9Reywn5FekORSmlMkLGJeq5K15HLIezhuofiUopBRmY\nqN/a9g4AF408Nc2RKKVUZsi40/M2NWzE9orolVWQ7lCUUiojZFyNusbZTEGgh3ZdqpRShyGjErXn\nGVx7J32zNFErpVSLjErU2+rrECtOcZbeA1AppVpkVKJeX70NgL7ZmqiVUqpFRiXqT2r8O5EMyNML\nXZRSqkVGJeot9bsAGJTfGX0tKqVU95RRiXpHo5+ohxb1PciUSil19MioRL0reVeOIZqolVIqJaMS\ndU20CrwssoJ6g02llGqRUYm6LlGDbfLTHYZSSmWUjErUTW4NYdFErZRSrWVUoo6bOrLtwnSHoZRS\nGSWjErUrdeQHNVErpVRrGZOoPc9jYHgs4/uNTXcoSimVUdrVzamInAv8BrCBPxlj7ujoQCzLYsGs\n+zq6WKWU6vYOWqMWERv4H+BzwCjgMhEZ1dmBKaWU8rWn6WMisNYYs94YEwceA6Z3blhKKaVatCdR\nDwQ2tXq/OTlsDyJyrYgsEZElu3bt6qj4lFLqqNdhfyYaY/5gjJlgjJlQUqK93ymlVEdpT6LeArS+\n5cqg5DCllFJdoD2J+m3gBBEZKiIhYCbw184NSymlVIuDnp5njHFE5N+BF/BPz7vfGLOq0yNTSikF\ntPM8amPMfGB+J8eilFKqDWKM6fhCRXYBHx/m7L2Byg4MJ516yrr0lPUAXZdMpesCxxpj2jwTo1MS\n9ZEQkSXGmAnpjqMj9JR16SnrAboumUrX5cAypq8PpZRSbdNErZRSGS4TE/Uf0h1AB+op69JT1gN0\nXTKVrssBZFwbtVJKqT1lYo1aKaVUK5qolVIqw2VMohaRc0VkjYisFZEfpDuegxGR+0Vkp4isbDWs\nl4gsFJGPks9FyeEiIncn1+09ERmXvsj3JSKDReQfIrJaRFaJyLeSw7vd+ohIRETeEpHlyXX5SXL4\nUBF5MxnzX5LdISAi4eT7tcnxQ9IZ/95ExBaRd0Xkb8n3/397Zx9zZV3G8c83H98iB6aOKOaIhbpU\nfFQyHqOGtBxz5ZZjTscSGps9GytlOYu5FWtrjRpJWklbpdjwJfGlci1fIJmtskmgYIhK0bI0ekEo\n51jYtz9+14F7x/PwPE9yOPcN12c7e35v931f3/P8znV+5zrnvn5N1bFd0iZJGyU9GW2Nm18AksZJ\nWi3pWUlbJA10W0stHHVDNye4DZjd1vZ5YI3tKcCaqEPRNSUeVwO3HCIbR8pe4LO23wtMBxbG899E\nPXuAWbbPAfqB2ZKmA0uBG22/B9gJLIjxC4Cd0X5jjKsT1wBbKvWm6gC4yHZ/5TfGTZxfUHa7+pnt\nM4BzKP+f7mqx3fMHMAA8VKkvBhb32q4R2D0J2FypbwUmRHkCsDXK3wGu7DSujg/gR8BHmq4HeCvw\nW+D9lDvF+trnGyWHzUCU+2Kcem172DMxXvSzgAcBNVFH2LQdOLmtrXHzCxgL/KH9ue22llqsqBnh\n5gQNYLztl6L8MjA+yo3RFx+ZzwWeoKF6IlywEdgBPAJsA16xvTeGVO3dpyX6dwEnHVqLh2Q5cD3w\n36ifRDN1ABh4WNJ6SVdHWxPn17uBvwG3Rkjqu5LG0GUtdXHUhx0ub5+N+u2jpLcB9wLX2t5d7WuS\nHtuv2+6nrEgvAM7olxBsBgAABMFJREFUsUmjRtJHgR221/faloPEDNvnUUIBCyV9qNrZoPnVB5wH\n3GL7XOBV9oc5gO5oqYujPlw2J/irpAkA8XdHtNden6SjKU56le37ormxegBsvwL8nBIiGCeplS2y\nau8+LdE/FvjHITa1Ex8ALpW0nbJP6SxKbLRpOgCw/ef4uwO4n/IG2sT59SLwou0nor6a4ri7qqUu\njvpw2Zzgx8C8KM+jxHpb7VfFN8DTgV2Vj0k9R5KA7wFbbH+90tU4PZJOkTQuysdTYu1bKA57Tgxr\n19LSOAdYGyuinmJ7se2JtidRXg9rbc+lYToAJI2RdEKrDFwMbKaB88v2y8CfJJ0eTR8Gfke3tfQ6\nOF8Jsl8CPEeJJ97Qa3tGYO+dwEvAfyjvsgsoMcE1wPPAo8DbY6wov2rZBmwCpvXa/jYtMygf1Z4G\nNsbjkibqAaYCG0LLZuAL0T4Z+A3wAnAPcGy0Hxf1F6J/cq81dNA0E3iwqTrC5qfi8Uzr9d3E+RX2\n9QNPxhx7ADix21ryFvIkSZKaU5fQR5IkSTIE6aiTJElqTjrqJEmSmpOOOkmSpOako06SJKk56aiT\ng44kS1pWqV8naUkXrnNnZCRb1Na+RNJ1UZ4v6Z0H8ZozJV1YqQ9KuupgnT9JOtE3/JAkGTV7gMsk\nfcX237txAUnvAN7nki3uQMyn/J76L6M4d5/359NoZybwb+CXALZXjPS8SfL/kivqpBvspewbt6i9\nQ9IkSWtjJbxG0qkHOpFKfulbI5fxBkkXRdfDwLsiv/EHhzh2DjANWBXjjpd0vqR1kRzoocptv49J\nWq6SK/kaSR9Tyeu8QdKjksZHwqpBYFHrum2r935Jvw5t91dyEj8maalKnuznWvZKOjPaNsYxU0b9\nTCdHBOmok27xLWCupLFt7TcDK21PBVYBNw1znoWUPDdnA1cCKyUdB1wKbHPJb/x4pwNtr6bcQTbX\nJUnT3rj+HNvnA98Hvlw55Bjb02wvA34BTHdJvHMXcL3t7cAKSj7oTte9HfhcaNsEfLHS12f7AuDa\nSvsg8I2wbRrlDtckeQMZ+ki6gu3dkm4HPgO8VukaAC6L8g+Arw5zqhkU54rtZyX9ETgN2H3Aozpz\nOnAW8EhJb8JRlDQALe6ulCcCd8eK+xhKDuIhiTekcbbXRdNKyi3dLVqJrtZT8pgD/Aq4QdJE4D7b\nz49WUHJkkCvqpJssp+RAGdNrQwIBz8RquN/22bYvrvS/WinfDHwzVvKfouTSeDPsib+vEwsk23dQ\nPhm8BvxU0qw3eY3kMCUdddI1bP8T+CH7t4uC8iXcFVGeC3QMW1R4PMYh6TTgVMouGSPlX8AJUd4K\nnCJpIM53tKQzhzhuLPvTUc6rtFfPtw/bu4CdlXj5J4B17eOqSJoM/N72TZRsa1OHl5MciaSjTrrN\nMuDkSv3TwCclPU1xZq2NdAclDXY4/tvAWyRtooQm5tve02HcUNwGrFDZ8eUoSgrQpZKeomQJvHCI\n45YA90haT9nWqsVPgI8P8SXmPOBroa0f+NIwtl0ObA7bzqLEuJPkDWT2vCRJkpqTK+okSZKak446\nSZKk5qSjTpIkqTnpqJMkSWpOOuokSZKak446SZKk5qSjTpIkqTn/A1M0oHakbo1PAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    num_Iterations = 600\n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    validation_accuracy = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    # load and prepare the training and test data\n",
    "    tr_x, tr_y, te_x, te_y = load_Prepare_Data()\n",
    "    \n",
    "    # To avoid problems related to type conversion, all data is converted to  float32 data types\n",
    "    tr_x = tf.cast(tr_x, tf.float32)\n",
    "    te_x = tf.cast(te_x, tf.float32)\n",
    "    tr_y = tf.cast(tr_y, tf.float32)\n",
    "    te_y = tf.cast(te_y, tf.float32)\n",
    "\n",
    "    # Set the L2 regularization rate \n",
    "    alpha = tf.Variable(0.003, tf.float32)\n",
    "    #Initialize the values of the weights and bias for the 1st hidden layer\n",
    "    w1 = tf.Variable(tf.random.normal([ 300, tr_x.shape[0]], mean=0.0, stddev=0.05))\n",
    "    b1 = tf.Variable(tf.zeros([300, 1]))\n",
    "    #Initialize the values of the weights and bias for the 2nd hidden layer\n",
    "    w2 = tf.Variable(tf.random.normal([100, 300], mean=0.0, stddev=0.05))\n",
    "    b2 = tf.Variable(tf.zeros([100, 1]))\n",
    "    #Initialize the values of the weights and bias for the SoftMax layer\n",
    "    w3 = tf.Variable(tf.random.normal([ tr_y.shape[0], 100], mean=0.0, stddev=0.05))\n",
    "    b3 = tf.Variable(tf.zeros([tr_y.shape[0], 1]))\n",
    "    \n",
    "    # Iterate the training loop\n",
    "    for i in range(num_Iterations):\n",
    "        \n",
    "        # Create an instance of Gradient Tape to monitor the forward pass to calcualte the gradients based on the training data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = forward_pass(tr_x, w1, b1, w2, b2, w3, b3)\n",
    "            currentLoss = cross_entropy_L2Reg(tr_y, y_pred, w1, w2, w3, alpha)\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        gradients = tape.gradient(currentLoss, [w1, b1, w2, b2, w3, b3])\n",
    "        # Determine the prediction accuracy for the training data\n",
    "        accuracy = calculate_accuracy(tr_y, y_pred)\n",
    "\n",
    "        train_accuracy.append(accuracy)\n",
    "        train_loss.append(currentLoss)\n",
    "\n",
    "        # Calculate forward pass, loss and accuracy for the valdation data\n",
    "        te_y_pred = forward_pass(te_x, w1, b1, w2, b2, w3, b3) \n",
    "        te_currentLoss = cross_entropy_L2Reg(te_y, te_y_pred, w1, w2, w3, alpha)\n",
    "        te_accuracy = calculate_accuracy(te_y, te_y_pred) \n",
    "        validation_accuracy.append(te_accuracy)\n",
    "        validation_loss.append(te_currentLoss)\n",
    "\n",
    "        print (\"Iteration \", i, \": Train Loss = \",currentLoss.numpy(), \"  Train Acc: \", accuracy.numpy(), \"  Validation Loss = \", te_currentLoss.numpy(), \"  Validation Acc: \", te_accuracy.numpy())\n",
    "        # Update the trainable parameters using Adam Optimizer\n",
    "        adam_optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2, w3, b3]))\n",
    "    print(alpha.numpy())\n",
    "    # Plot the training and the validation accuracy and loss        \n",
    "    plt.plot(train_accuracy, label=\"Train Acc\")\n",
    "    plt.plot(train_loss, label=\"Train Loss\")\n",
    "    plt.plot(validation_accuracy, label=\"Validation Acc\")\n",
    "    plt.plot(validation_loss, label=\"Validation Loss\")\n",
    "    plt.title('NN with L2 Regularization')\n",
    "    plt.xlabel(\"No. of Iterations\")\n",
    "    plt.text(200, 2, r'$L2\\ Regularization\\ rate=%.3f$' % (alpha.numpy()))\n",
    "    plt.legend()    \n",
    "    plt.show();\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttypDslqZJXC"
   },
   "source": [
    "### Implementation of L1 Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "D1lDI91Xg-FR",
    "outputId": "a8738f3d-c7c6-42f0-b9c0-88d10dc32e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n",
      "Reshaped training data  (784, 60000)\n",
      "Reshaped test data  (784, 10000)\n",
      "Iteration  0 : Train Loss =  5.480507   Train Acc:  0.1516   Validation Loss =  5.481229   Validation Acc:  0.1512\n",
      "Iteration  1 : Train Loss =  5.335094   Train Acc:  0.27553332   Validation Loss =  5.3362637   Validation Acc:  0.2684\n",
      "Iteration  2 : Train Loss =  5.203991   Train Acc:  0.30656666   Validation Loss =  5.205867   Validation Acc:  0.2951\n",
      "Iteration  3 : Train Loss =  5.073345   Train Acc:  0.37863332   Validation Loss =  5.0759506   Validation Acc:  0.3699\n",
      "Iteration  4 : Train Loss =  4.9392157   Train Acc:  0.47846666   Validation Loss =  4.942691   Validation Acc:  0.4672\n",
      "Iteration  5 : Train Loss =  4.8006086   Train Acc:  0.554   Validation Loss =  4.8049345   Validation Acc:  0.5458\n",
      "Iteration  6 : Train Loss =  4.6603675   Train Acc:  0.60585   Validation Loss =  4.665631   Validation Acc:  0.6006\n",
      "Iteration  7 : Train Loss =  4.5216875   Train Acc:  0.63173336   Validation Loss =  4.5278306   Validation Acc:  0.6271\n",
      "Iteration  8 : Train Loss =  4.3872585   Train Acc:  0.63491666   Validation Loss =  4.394269   Validation Acc:  0.6284\n",
      "Iteration  9 : Train Loss =  4.2577925   Train Acc:  0.64   Validation Loss =  4.265754   Validation Acc:  0.6332\n",
      "Iteration  10 : Train Loss =  4.13395   Train Acc:  0.6508667   Validation Loss =  4.1426983   Validation Acc:  0.6424\n",
      "Iteration  11 : Train Loss =  4.016876   Train Acc:  0.6613167   Validation Loss =  4.026167   Validation Acc:  0.6525\n",
      "Iteration  12 : Train Loss =  3.9077907   Train Acc:  0.66823334   Validation Loss =  3.9175308   Validation Acc:  0.6595\n",
      "Iteration  13 : Train Loss =  3.807511   Train Acc:  0.6705   Validation Loss =  3.8176272   Validation Acc:  0.6618\n",
      "Iteration  14 : Train Loss =  3.7162383   Train Acc:  0.67121667   Validation Loss =  3.7268114   Validation Acc:  0.6646\n",
      "Iteration  15 : Train Loss =  3.632947   Train Acc:  0.67408335   Validation Loss =  3.644102   Validation Acc:  0.6664\n",
      "Iteration  16 : Train Loss =  3.5567913   Train Acc:  0.67716664   Validation Loss =  3.568558   Validation Acc:  0.6677\n",
      "Iteration  17 : Train Loss =  3.4867318   Train Acc:  0.67865   Validation Loss =  3.4989753   Validation Acc:  0.6681\n",
      "Iteration  18 : Train Loss =  3.4220614   Train Acc:  0.67965   Validation Loss =  3.4347255   Validation Acc:  0.6709\n",
      "Iteration  19 : Train Loss =  3.3617144   Train Acc:  0.68266666   Validation Loss =  3.3748784   Validation Acc:  0.6738\n",
      "Iteration  20 : Train Loss =  3.3051014   Train Acc:  0.68525   Validation Loss =  3.318809   Validation Acc:  0.6747\n",
      "Iteration  21 : Train Loss =  3.2515264   Train Acc:  0.6887   Validation Loss =  3.2656577   Validation Acc:  0.6785\n",
      "Iteration  22 : Train Loss =  3.20035   Train Acc:  0.69156665   Validation Loss =  3.214963   Validation Acc:  0.682\n",
      "Iteration  23 : Train Loss =  3.1507554   Train Acc:  0.6953   Validation Loss =  3.1660314   Validation Acc:  0.6855\n",
      "Iteration  24 : Train Loss =  3.1025288   Train Acc:  0.69836664   Validation Loss =  3.1182852   Validation Acc:  0.6882\n",
      "Iteration  25 : Train Loss =  3.0556836   Train Acc:  0.7015167   Validation Loss =  3.071805   Validation Acc:  0.6915\n",
      "Iteration  26 : Train Loss =  3.0096438   Train Acc:  0.70523334   Validation Loss =  3.0261748   Validation Acc:  0.6957\n",
      "Iteration  27 : Train Loss =  2.9641895   Train Acc:  0.7108333   Validation Loss =  2.9809372   Validation Acc:  0.7003\n",
      "Iteration  28 : Train Loss =  2.9195037   Train Acc:  0.71678334   Validation Loss =  2.936689   Validation Acc:  0.7044\n",
      "Iteration  29 : Train Loss =  2.8753393   Train Acc:  0.72146666   Validation Loss =  2.8930998   Validation Acc:  0.7093\n",
      "Iteration  30 : Train Loss =  2.8314943   Train Acc:  0.72545   Validation Loss =  2.8494577   Validation Acc:  0.7142\n",
      "Iteration  31 : Train Loss =  2.7882798   Train Acc:  0.7291833   Validation Loss =  2.8064473   Validation Acc:  0.7183\n",
      "Iteration  32 : Train Loss =  2.7454677   Train Acc:  0.73495   Validation Loss =  2.7635689   Validation Acc:  0.7239\n",
      "Iteration  33 : Train Loss =  2.703139   Train Acc:  0.7409833   Validation Loss =  2.7211468   Validation Acc:  0.7305\n",
      "Iteration  34 : Train Loss =  2.6615076   Train Acc:  0.7471333   Validation Loss =  2.6794991   Validation Acc:  0.736\n",
      "Iteration  35 : Train Loss =  2.6203687   Train Acc:  0.75086665   Validation Loss =  2.638213   Validation Acc:  0.7404\n",
      "Iteration  36 : Train Loss =  2.5798385   Train Acc:  0.75545   Validation Loss =  2.5979168   Validation Acc:  0.7442\n",
      "Iteration  37 : Train Loss =  2.5400481   Train Acc:  0.7589   Validation Loss =  2.5581098   Validation Acc:  0.7477\n",
      "Iteration  38 : Train Loss =  2.500853   Train Acc:  0.7643   Validation Loss =  2.5191138   Validation Acc:  0.7533\n",
      "Iteration  39 : Train Loss =  2.4623675   Train Acc:  0.76915   Validation Loss =  2.48042   Validation Acc:  0.7591\n",
      "Iteration  40 : Train Loss =  2.4246435   Train Acc:  0.7743667   Validation Loss =  2.4428673   Validation Acc:  0.7657\n",
      "Iteration  41 : Train Loss =  2.387575   Train Acc:  0.77725   Validation Loss =  2.4057922   Validation Acc:  0.7675\n",
      "Iteration  42 : Train Loss =  2.3511863   Train Acc:  0.7799   Validation Loss =  2.3696616   Validation Acc:  0.7683\n",
      "Iteration  43 : Train Loss =  2.3154094   Train Acc:  0.78298336   Validation Loss =  2.3339877   Validation Acc:  0.7704\n",
      "Iteration  44 : Train Loss =  2.280231   Train Acc:  0.78665   Validation Loss =  2.2991168   Validation Acc:  0.7734\n",
      "Iteration  45 : Train Loss =  2.2457423   Train Acc:  0.7898   Validation Loss =  2.2648687   Validation Acc:  0.7781\n",
      "Iteration  46 : Train Loss =  2.211853   Train Acc:  0.79213333   Validation Loss =  2.2312088   Validation Acc:  0.7806\n",
      "Iteration  47 : Train Loss =  2.1786113   Train Acc:  0.7933   Validation Loss =  2.1981988   Validation Acc:  0.7823\n",
      "Iteration  48 : Train Loss =  2.1460383   Train Acc:  0.7948667   Validation Loss =  2.1658945   Validation Acc:  0.7833\n",
      "Iteration  49 : Train Loss =  2.1140602   Train Acc:  0.79618335   Validation Loss =  2.1342092   Validation Acc:  0.7859\n",
      "Iteration  50 : Train Loss =  2.0828006   Train Acc:  0.79891664   Validation Loss =  2.1032019   Validation Acc:  0.7883\n",
      "Iteration  51 : Train Loss =  2.0523615   Train Acc:  0.79891664   Validation Loss =  2.0730147   Validation Acc:  0.7894\n",
      "Iteration  52 : Train Loss =  2.0229502   Train Acc:  0.8020667   Validation Loss =  2.0439575   Validation Acc:  0.7915\n",
      "Iteration  53 : Train Loss =  1.9944323   Train Acc:  0.8002167   Validation Loss =  2.0155241   Validation Acc:  0.7913\n",
      "Iteration  54 : Train Loss =  1.9661663   Train Acc:  0.80373335   Validation Loss =  1.9876958   Validation Acc:  0.7927\n",
      "Iteration  55 : Train Loss =  1.9372845   Train Acc:  0.804   Validation Loss =  1.9586463   Validation Acc:  0.7944\n",
      "Iteration  56 : Train Loss =  1.9095278   Train Acc:  0.80691665   Validation Loss =  1.9314771   Validation Acc:  0.7957\n",
      "Iteration  57 : Train Loss =  1.8836446   Train Acc:  0.80831665   Validation Loss =  1.9054165   Validation Acc:  0.7963\n",
      "Iteration  58 : Train Loss =  1.8581109   Train Acc:  0.8068333   Validation Loss =  1.8803284   Validation Acc:  0.7964\n",
      "Iteration  59 : Train Loss =  1.832109   Train Acc:  0.8092667   Validation Loss =  1.8544528   Validation Acc:  0.7987\n",
      "Iteration  60 : Train Loss =  1.8069711   Train Acc:  0.81035   Validation Loss =  1.8293824   Validation Acc:  0.8001\n",
      "Iteration  61 : Train Loss =  1.7833662   Train Acc:  0.81041664   Validation Loss =  1.8062195   Validation Acc:  0.7993\n",
      "Iteration  62 : Train Loss =  1.7599597   Train Acc:  0.81188333   Validation Loss =  1.7826653   Validation Acc:  0.7985\n",
      "Iteration  63 : Train Loss =  1.7362542   Train Acc:  0.8117667   Validation Loss =  1.7594131   Validation Acc:  0.8001\n",
      "Iteration  64 : Train Loss =  1.7134008   Train Acc:  0.8129   Validation Loss =  1.736526   Validation Acc:  0.8029\n",
      "Iteration  65 : Train Loss =  1.6917186   Train Acc:  0.81385   Validation Loss =  1.715013   Validation Acc:  0.8025\n",
      "Iteration  66 : Train Loss =  1.6702318   Train Acc:  0.8138833   Validation Loss =  1.6937525   Validation Acc:  0.8033\n",
      "Iteration  67 : Train Loss =  1.6488116   Train Acc:  0.8158   Validation Loss =  1.6723778   Validation Acc:  0.804\n",
      "Iteration  68 : Train Loss =  1.6281806   Train Acc:  0.81661665   Validation Loss =  1.6519555   Validation Acc:  0.8042\n",
      "Iteration  69 : Train Loss =  1.6084049   Train Acc:  0.8161333   Validation Loss =  1.6321871   Validation Acc:  0.805\n",
      "Iteration  70 : Train Loss =  1.5888889   Train Acc:  0.81775   Validation Loss =  1.6128525   Validation Acc:  0.8042\n",
      "Iteration  71 : Train Loss =  1.569465   Train Acc:  0.8178667   Validation Loss =  1.5933512   Validation Acc:  0.8063\n",
      "Iteration  72 : Train Loss =  1.5506346   Train Acc:  0.81918335   Validation Loss =  1.5748769   Validation Acc:  0.8078\n",
      "Iteration  73 : Train Loss =  1.5326469   Train Acc:  0.82   Validation Loss =  1.5565053   Validation Acc:  0.8082\n",
      "Iteration  74 : Train Loss =  1.5152538   Train Acc:  0.81956667   Validation Loss =  1.5398378   Validation Acc:  0.8073\n",
      "Iteration  75 : Train Loss =  1.4982483   Train Acc:  0.82055   Validation Loss =  1.5219727   Validation Acc:  0.8093\n",
      "Iteration  76 : Train Loss =  1.4816766   Train Acc:  0.81986666   Validation Loss =  1.5065663   Validation Acc:  0.8077\n",
      "Iteration  77 : Train Loss =  1.4659017   Train Acc:  0.82045   Validation Loss =  1.4895169   Validation Acc:  0.8093\n",
      "Iteration  78 : Train Loss =  1.4494442   Train Acc:  0.8199667   Validation Loss =  1.4743487   Validation Acc:  0.8088\n",
      "Iteration  79 : Train Loss =  1.4330549   Train Acc:  0.8214667   Validation Loss =  1.4571035   Validation Acc:  0.81\n",
      "Iteration  80 : Train Loss =  1.4170156   Train Acc:  0.82285   Validation Loss =  1.4414186   Validation Acc:  0.8113\n",
      "Iteration  81 : Train Loss =  1.4023656   Train Acc:  0.82316667   Validation Loss =  1.4270422   Validation Acc:  0.8116\n",
      "Iteration  82 : Train Loss =  1.3887217   Train Acc:  0.82303333   Validation Loss =  1.412669   Validation Acc:  0.811\n",
      "Iteration  83 : Train Loss =  1.3746946   Train Acc:  0.82215   Validation Loss =  1.3995854   Validation Acc:  0.8112\n",
      "Iteration  84 : Train Loss =  1.3604381   Train Acc:  0.8239   Validation Loss =  1.3845985   Validation Acc:  0.8122\n",
      "Iteration  85 : Train Loss =  1.3463776   Train Acc:  0.82485   Validation Loss =  1.3709197   Validation Acc:  0.8136\n",
      "Iteration  86 : Train Loss =  1.3333619   Train Acc:  0.825   Validation Loss =  1.3580418   Validation Acc:  0.8136\n",
      "Iteration  87 : Train Loss =  1.3211274   Train Acc:  0.82481664   Validation Loss =  1.345294   Validation Acc:  0.814\n",
      "Iteration  88 : Train Loss =  1.3087592   Train Acc:  0.8245   Validation Loss =  1.3336527   Validation Acc:  0.8142\n",
      "Iteration  89 : Train Loss =  1.2962557   Train Acc:  0.82593334   Validation Loss =  1.3205235   Validation Acc:  0.8147\n",
      "Iteration  90 : Train Loss =  1.2838662   Train Acc:  0.8264833   Validation Loss =  1.3085864   Validation Acc:  0.8146\n",
      "Iteration  91 : Train Loss =  1.2722063   Train Acc:  0.82671666   Validation Loss =  1.2968514   Validation Acc:  0.8153\n",
      "Iteration  92 : Train Loss =  1.2611876   Train Acc:  0.8268167   Validation Loss =  1.2856021   Validation Acc:  0.8163\n",
      "Iteration  93 : Train Loss =  1.2504449   Train Acc:  0.82591665   Validation Loss =  1.2753458   Validation Acc:  0.8148\n",
      "Iteration  94 : Train Loss =  1.2397746   Train Acc:  0.82698333   Validation Loss =  1.2641304   Validation Acc:  0.8162\n",
      "Iteration  95 : Train Loss =  1.2290285   Train Acc:  0.82661664   Validation Loss =  1.253928   Validation Acc:  0.815\n",
      "Iteration  96 : Train Loss =  1.2185133   Train Acc:  0.8279333   Validation Loss =  1.2430041   Validation Acc:  0.8169\n",
      "Iteration  97 : Train Loss =  1.2083378   Train Acc:  0.82788336   Validation Loss =  1.2330713   Validation Acc:  0.8165\n",
      "Iteration  98 : Train Loss =  1.1985844   Train Acc:  0.8276167   Validation Loss =  1.2232604   Validation Acc:  0.8161\n",
      "Iteration  99 : Train Loss =  1.1892023   Train Acc:  0.8282833   Validation Loss =  1.2138042   Validation Acc:  0.8177\n",
      "Iteration  100 : Train Loss =  1.1801536   Train Acc:  0.82735   Validation Loss =  1.2049257   Validation Acc:  0.8159\n",
      "Iteration  101 : Train Loss =  1.1714267   Train Acc:  0.82805   Validation Loss =  1.1960366   Validation Acc:  0.8174\n",
      "Iteration  102 : Train Loss =  1.1630561   Train Acc:  0.827   Validation Loss =  1.1878052   Validation Acc:  0.8169\n",
      "Iteration  103 : Train Loss =  1.1551217   Train Acc:  0.8275333   Validation Loss =  1.1799078   Validation Acc:  0.8163\n",
      "Iteration  104 : Train Loss =  1.147927   Train Acc:  0.82733333   Validation Loss =  1.1724563   Validation Acc:  0.8167\n",
      "Iteration  105 : Train Loss =  1.1410398   Train Acc:  0.826   Validation Loss =  1.1662524   Validation Acc:  0.8145\n",
      "Iteration  106 : Train Loss =  1.1337615   Train Acc:  0.8265833   Validation Loss =  1.1579306   Validation Acc:  0.8167\n",
      "Iteration  107 : Train Loss =  1.1252714   Train Acc:  0.82608336   Validation Loss =  1.1507845   Validation Acc:  0.8151\n",
      "Iteration  108 : Train Loss =  1.1161323   Train Acc:  0.82865   Validation Loss =  1.1402946   Validation Acc:  0.8186\n",
      "Iteration  109 : Train Loss =  1.1086559   Train Acc:  0.82725   Validation Loss =  1.1337365   Validation Acc:  0.8172\n",
      "Iteration  110 : Train Loss =  1.1028221   Train Acc:  0.82766664   Validation Loss =  1.1276903   Validation Acc:  0.818\n",
      "Iteration  111 : Train Loss =  1.0959094   Train Acc:  0.8282   Validation Loss =  1.1205122   Validation Acc:  0.8177\n",
      "Iteration  112 : Train Loss =  1.0875816   Train Acc:  0.8287333   Validation Loss =  1.1126891   Validation Acc:  0.8194\n",
      "Iteration  113 : Train Loss =  1.0807984   Train Acc:  0.82986665   Validation Loss =  1.1053525   Validation Acc:  0.8188\n",
      "Iteration  114 : Train Loss =  1.0759438   Train Acc:  0.8276833   Validation Loss =  1.10096   Validation Acc:  0.8175\n",
      "Iteration  115 : Train Loss =  1.0703831   Train Acc:  0.82851666   Validation Loss =  1.0952466   Validation Acc:  0.8186\n",
      "Iteration  116 : Train Loss =  1.0629554   Train Acc:  0.82913333   Validation Loss =  1.0878181   Validation Acc:  0.8194\n",
      "Iteration  117 : Train Loss =  1.0559962   Train Acc:  0.8308333   Validation Loss =  1.0808914   Validation Acc:  0.8199\n",
      "Iteration  118 : Train Loss =  1.0510299   Train Acc:  0.83016664   Validation Loss =  1.0759022   Validation Acc:  0.82\n",
      "Iteration  119 : Train Loss =  1.0465178   Train Acc:  0.82881665   Validation Loss =  1.0714782   Validation Acc:  0.8196\n",
      "Iteration  120 : Train Loss =  1.0407007   Train Acc:  0.8304333   Validation Loss =  1.0655704   Validation Acc:  0.8199\n",
      "Iteration  121 : Train Loss =  1.0344126   Train Acc:  0.83071667   Validation Loss =  1.0595111   Validation Acc:  0.8191\n",
      "Iteration  122 : Train Loss =  1.0291952   Train Acc:  0.83068335   Validation Loss =  1.0539547   Validation Acc:  0.8212\n",
      "Iteration  123 : Train Loss =  1.024749   Train Acc:  0.8304667   Validation Loss =  1.0498856   Validation Acc:  0.8212\n",
      "Iteration  124 : Train Loss =  1.019887   Train Acc:  0.8305167   Validation Loss =  1.0447459   Validation Acc:  0.8214\n",
      "Iteration  125 : Train Loss =  1.0144482   Train Acc:  0.8313   Validation Loss =  1.0394723   Validation Acc:  0.8212\n",
      "Iteration  126 : Train Loss =  1.0093589   Train Acc:  0.83166665   Validation Loss =  1.0344454   Validation Acc:  0.8211\n",
      "Iteration  127 : Train Loss =  1.0050169   Train Acc:  0.83165   Validation Loss =  1.0298576   Validation Acc:  0.8216\n",
      "Iteration  128 : Train Loss =  1.0009327   Train Acc:  0.8311   Validation Loss =  1.0262165   Validation Acc:  0.8212\n",
      "Iteration  129 : Train Loss =  0.9964934   Train Acc:  0.8316   Validation Loss =  1.0212697   Validation Acc:  0.821\n",
      "Iteration  130 : Train Loss =  0.99180156   Train Acc:  0.83176666   Validation Loss =  1.0171009   Validation Acc:  0.822\n",
      "Iteration  131 : Train Loss =  0.9873675   Train Acc:  0.8326167   Validation Loss =  1.0122507   Validation Acc:  0.8217\n",
      "Iteration  132 : Train Loss =  0.9834299   Train Acc:  0.8319333   Validation Loss =  1.0086864   Validation Acc:  0.8211\n",
      "Iteration  133 : Train Loss =  0.97980165   Train Acc:  0.83253336   Validation Loss =  1.0047394   Validation Acc:  0.8219\n",
      "Iteration  134 : Train Loss =  0.97607946   Train Acc:  0.8315   Validation Loss =  1.0014168   Validation Acc:  0.82\n",
      "Iteration  135 : Train Loss =  0.9724252   Train Acc:  0.8325667   Validation Loss =  0.9973016   Validation Acc:  0.8219\n",
      "Iteration  136 : Train Loss =  0.96855927   Train Acc:  0.83183336   Validation Loss =  0.9940584   Validation Acc:  0.8211\n",
      "Iteration  137 : Train Loss =  0.96518815   Train Acc:  0.8326833   Validation Loss =  0.98989093   Validation Acc:  0.8218\n",
      "Iteration  138 : Train Loss =  0.9617069   Train Acc:  0.83128333   Validation Loss =  0.9873884   Validation Acc:  0.8223\n",
      "Iteration  139 : Train Loss =  0.95821804   Train Acc:  0.83283335   Validation Loss =  0.98288035   Validation Acc:  0.8223\n",
      "Iteration  140 : Train Loss =  0.9541787   Train Acc:  0.8321   Validation Loss =  0.97980845   Validation Acc:  0.8224\n",
      "Iteration  141 : Train Loss =  0.94988954   Train Acc:  0.83355   Validation Loss =  0.9747772   Validation Acc:  0.8228\n",
      "Iteration  142 : Train Loss =  0.94587666   Train Acc:  0.83335   Validation Loss =  0.9711902   Validation Acc:  0.8243\n",
      "Iteration  143 : Train Loss =  0.94245267   Train Acc:  0.83351666   Validation Loss =  0.9677018   Validation Acc:  0.8242\n",
      "Iteration  144 : Train Loss =  0.93948436   Train Acc:  0.83381665   Validation Loss =  0.9645282   Validation Acc:  0.8232\n",
      "Iteration  145 : Train Loss =  0.9367038   Train Acc:  0.8329833   Validation Loss =  0.96223843   Validation Acc:  0.8246\n",
      "Iteration  146 : Train Loss =  0.9338647   Train Acc:  0.83376664   Validation Loss =  0.9587891   Validation Acc:  0.8246\n",
      "Iteration  147 : Train Loss =  0.9308557   Train Acc:  0.8333333   Validation Loss =  0.9565224   Validation Acc:  0.8243\n",
      "Iteration  148 : Train Loss =  0.9277795   Train Acc:  0.834   Validation Loss =  0.95277244   Validation Acc:  0.8243\n",
      "Iteration  149 : Train Loss =  0.9246359   Train Acc:  0.83358335   Validation Loss =  0.9502368   Validation Acc:  0.8247\n",
      "Iteration  150 : Train Loss =  0.92184186   Train Acc:  0.83453333   Validation Loss =  0.9469956   Validation Acc:  0.8252\n",
      "Iteration  151 : Train Loss =  0.91917074   Train Acc:  0.8333667   Validation Loss =  0.9446746   Validation Acc:  0.8229\n",
      "Iteration  152 : Train Loss =  0.9167323   Train Acc:  0.83465   Validation Loss =  0.94207954   Validation Acc:  0.8245\n",
      "Iteration  153 : Train Loss =  0.91422737   Train Acc:  0.83325   Validation Loss =  0.9396957   Validation Acc:  0.8234\n",
      "Iteration  154 : Train Loss =  0.9115266   Train Acc:  0.83465   Validation Loss =  0.93699306   Validation Acc:  0.8255\n",
      "Iteration  155 : Train Loss =  0.90853107   Train Acc:  0.83353335   Validation Loss =  0.9340248   Validation Acc:  0.8236\n",
      "Iteration  156 : Train Loss =  0.9053934   Train Acc:  0.835   Validation Loss =  0.9308805   Validation Acc:  0.826\n",
      "Iteration  157 : Train Loss =  0.9023876   Train Acc:  0.8347833   Validation Loss =  0.92796856   Validation Acc:  0.8248\n",
      "Iteration  158 : Train Loss =  0.8996637   Train Acc:  0.83501667   Validation Loss =  0.92512655   Validation Acc:  0.8256\n",
      "Iteration  159 : Train Loss =  0.8971911   Train Acc:  0.8351333   Validation Loss =  0.9228835   Validation Acc:  0.8265\n",
      "Iteration  160 : Train Loss =  0.8948845   Train Acc:  0.8351667   Validation Loss =  0.9203379   Validation Acc:  0.8258\n",
      "Iteration  161 : Train Loss =  0.8926599   Train Acc:  0.83475   Validation Loss =  0.91847825   Validation Acc:  0.827\n",
      "Iteration  162 : Train Loss =  0.8905083   Train Acc:  0.8351667   Validation Loss =  0.9159642   Validation Acc:  0.8261\n",
      "Iteration  163 : Train Loss =  0.88839316   Train Acc:  0.8351667   Validation Loss =  0.91432065   Validation Acc:  0.8264\n",
      "Iteration  164 : Train Loss =  0.8862269   Train Acc:  0.83505   Validation Loss =  0.9116895   Validation Acc:  0.8261\n",
      "Iteration  165 : Train Loss =  0.883988   Train Acc:  0.8351167   Validation Loss =  0.90998423   Validation Acc:  0.8271\n",
      "Iteration  166 : Train Loss =  0.8816153   Train Acc:  0.83538336   Validation Loss =  0.90714216   Validation Acc:  0.8266\n",
      "Iteration  167 : Train Loss =  0.8792232   Train Acc:  0.8355833   Validation Loss =  0.9052443   Validation Acc:  0.8277\n",
      "Iteration  168 : Train Loss =  0.8768116   Train Acc:  0.8361   Validation Loss =  0.9024327   Validation Acc:  0.8269\n",
      "Iteration  169 : Train Loss =  0.8744621   Train Acc:  0.8362167   Validation Loss =  0.900481   Validation Acc:  0.8275\n",
      "Iteration  170 : Train Loss =  0.87226474   Train Acc:  0.8365667   Validation Loss =  0.897984   Validation Acc:  0.8275\n",
      "Iteration  171 : Train Loss =  0.8702439   Train Acc:  0.83666664   Validation Loss =  0.8963121   Validation Acc:  0.8262\n",
      "Iteration  172 : Train Loss =  0.8686032   Train Acc:  0.8368   Validation Loss =  0.89433855   Validation Acc:  0.8274\n",
      "Iteration  173 : Train Loss =  0.8675766   Train Acc:  0.8351667   Validation Loss =  0.8938016   Validation Acc:  0.8251\n",
      "Iteration  174 : Train Loss =  0.8681803   Train Acc:  0.8357   Validation Loss =  0.8938683   Validation Acc:  0.8249\n",
      "Iteration  175 : Train Loss =  0.86873996   Train Acc:  0.83248335   Validation Loss =  0.895269   Validation Acc:  0.8214\n",
      "Iteration  176 : Train Loss =  0.8708067   Train Acc:  0.832   Validation Loss =  0.8964647   Validation Acc:  0.8237\n",
      "Iteration  177 : Train Loss =  0.8634942   Train Acc:  0.83353335   Validation Loss =  0.88994896   Validation Acc:  0.8222\n",
      "Iteration  178 : Train Loss =  0.85719097   Train Acc:  0.8375   Validation Loss =  0.88320744   Validation Acc:  0.8289\n",
      "Iteration  179 : Train Loss =  0.85508287   Train Acc:  0.83755   Validation Loss =  0.8810494   Validation Acc:  0.8282\n",
      "Iteration  180 : Train Loss =  0.85635674   Train Acc:  0.8355333   Validation Loss =  0.88285947   Validation Acc:  0.824\n",
      "Iteration  181 : Train Loss =  0.8569623   Train Acc:  0.8355833   Validation Loss =  0.8828219   Validation Acc:  0.8247\n",
      "Iteration  182 : Train Loss =  0.8512391   Train Acc:  0.8362   Validation Loss =  0.8777298   Validation Acc:  0.8263\n",
      "Iteration  183 : Train Loss =  0.84752554   Train Acc:  0.83828336   Validation Loss =  0.8737939   Validation Acc:  0.8279\n",
      "Iteration  184 : Train Loss =  0.8476429   Train Acc:  0.8379833   Validation Loss =  0.8736334   Validation Acc:  0.8282\n",
      "Iteration  185 : Train Loss =  0.847198   Train Acc:  0.8362   Validation Loss =  0.8737424   Validation Acc:  0.8244\n",
      "Iteration  186 : Train Loss =  0.8445934   Train Acc:  0.8379167   Validation Loss =  0.87068176   Validation Acc:  0.8274\n",
      "Iteration  187 : Train Loss =  0.8409221   Train Acc:  0.83875   Validation Loss =  0.8673525   Validation Acc:  0.8279\n",
      "Iteration  188 : Train Loss =  0.8399179   Train Acc:  0.8381   Validation Loss =  0.86642295   Validation Acc:  0.8274\n",
      "Iteration  189 : Train Loss =  0.83993614   Train Acc:  0.83813334   Validation Loss =  0.86612797   Validation Acc:  0.8277\n",
      "Iteration  190 : Train Loss =  0.83744067   Train Acc:  0.83783334   Validation Loss =  0.8640306   Validation Acc:  0.8273\n",
      "Iteration  191 : Train Loss =  0.8344985   Train Acc:  0.83933336   Validation Loss =  0.8609194   Validation Acc:  0.8296\n",
      "Iteration  192 : Train Loss =  0.8330262   Train Acc:  0.8392   Validation Loss =  0.8594457   Validation Acc:  0.8296\n",
      "Iteration  193 : Train Loss =  0.8323833   Train Acc:  0.8387   Validation Loss =  0.8590622   Validation Acc:  0.828\n",
      "Iteration  194 : Train Loss =  0.8309736   Train Acc:  0.83921665   Validation Loss =  0.85734487   Validation Acc:  0.8292\n",
      "Iteration  195 : Train Loss =  0.82841676   Train Acc:  0.83955   Validation Loss =  0.85505223   Validation Acc:  0.8289\n",
      "Iteration  196 : Train Loss =  0.8266253   Train Acc:  0.83973336   Validation Loss =  0.853243   Validation Acc:  0.829\n",
      "Iteration  197 : Train Loss =  0.8257388   Train Acc:  0.8397   Validation Loss =  0.85224104   Validation Acc:  0.8301\n",
      "Iteration  198 : Train Loss =  0.8244567   Train Acc:  0.8394167   Validation Loss =  0.85122395   Validation Acc:  0.8282\n",
      "Iteration  199 : Train Loss =  0.82254744   Train Acc:  0.8398833   Validation Loss =  0.84911275   Validation Acc:  0.8304\n",
      "Iteration  200 : Train Loss =  0.8206342   Train Acc:  0.8403   Validation Loss =  0.8473477   Validation Acc:  0.8299\n",
      "Iteration  201 : Train Loss =  0.8193302   Train Acc:  0.84036666   Validation Loss =  0.8461002   Validation Acc:  0.8294\n",
      "Iteration  202 : Train Loss =  0.8182405   Train Acc:  0.84001666   Validation Loss =  0.84491986   Validation Acc:  0.8307\n",
      "Iteration  203 : Train Loss =  0.816749   Train Acc:  0.8405167   Validation Loss =  0.84360564   Validation Acc:  0.8284\n",
      "Iteration  204 : Train Loss =  0.8150405   Train Acc:  0.84098333   Validation Loss =  0.8418356   Validation Acc:  0.8313\n",
      "Iteration  205 : Train Loss =  0.81347144   Train Acc:  0.84096664   Validation Loss =  0.8402649   Validation Acc:  0.8304\n",
      "Iteration  206 : Train Loss =  0.81224227   Train Acc:  0.8411833   Validation Loss =  0.8392131   Validation Acc:  0.8294\n",
      "Iteration  207 : Train Loss =  0.81119096   Train Acc:  0.84113336   Validation Loss =  0.8379079   Validation Acc:  0.8307\n",
      "Iteration  208 : Train Loss =  0.810155   Train Acc:  0.84106666   Validation Loss =  0.83727473   Validation Acc:  0.8308\n",
      "Iteration  209 : Train Loss =  0.809422   Train Acc:  0.8412   Validation Loss =  0.8360859   Validation Acc:  0.8302\n",
      "Iteration  210 : Train Loss =  0.80977184   Train Acc:  0.83968335   Validation Loss =  0.8371054   Validation Acc:  0.83\n",
      "Iteration  211 : Train Loss =  0.8114996   Train Acc:  0.83821666   Validation Loss =  0.83808005   Validation Acc:  0.8278\n",
      "Iteration  212 : Train Loss =  0.8134105   Train Acc:  0.83736664   Validation Loss =  0.8410606   Validation Acc:  0.8252\n",
      "Iteration  213 : Train Loss =  0.8114842   Train Acc:  0.83695   Validation Loss =  0.8380885   Validation Acc:  0.8265\n",
      "Iteration  214 : Train Loss =  0.8049587   Train Acc:  0.83996665   Validation Loss =  0.8324312   Validation Acc:  0.8305\n",
      "Iteration  215 : Train Loss =  0.8002998   Train Acc:  0.8423333   Validation Loss =  0.8273667   Validation Acc:  0.8313\n",
      "Iteration  216 : Train Loss =  0.80125874   Train Acc:  0.8412667   Validation Loss =  0.8281673   Validation Acc:  0.8298\n",
      "Iteration  217 : Train Loss =  0.80304754   Train Acc:  0.83965   Validation Loss =  0.83066237   Validation Acc:  0.8295\n",
      "Iteration  218 : Train Loss =  0.8002965   Train Acc:  0.8405833   Validation Loss =  0.82713675   Validation Acc:  0.829\n",
      "Iteration  219 : Train Loss =  0.795761   Train Acc:  0.84281665   Validation Loss =  0.82311726   Validation Acc:  0.8324\n",
      "Iteration  220 : Train Loss =  0.79479694   Train Acc:  0.8424   Validation Loss =  0.82217747   Validation Acc:  0.8337\n",
      "Iteration  221 : Train Loss =  0.79596865   Train Acc:  0.84105   Validation Loss =  0.8229923   Validation Acc:  0.8294\n",
      "Iteration  222 : Train Loss =  0.7944881   Train Acc:  0.8412667   Validation Loss =  0.8220965   Validation Acc:  0.8317\n",
      "Iteration  223 : Train Loss =  0.79097116   Train Acc:  0.84316665   Validation Loss =  0.81811714   Validation Acc:  0.8323\n",
      "Iteration  224 : Train Loss =  0.78952533   Train Acc:  0.84278333   Validation Loss =  0.816806   Validation Acc:  0.8324\n",
      "Iteration  225 : Train Loss =  0.7898989   Train Acc:  0.84215   Validation Loss =  0.81749046   Validation Acc:  0.8327\n",
      "Iteration  226 : Train Loss =  0.7888063   Train Acc:  0.8427333   Validation Loss =  0.815956   Validation Acc:  0.8312\n",
      "Iteration  227 : Train Loss =  0.7860998   Train Acc:  0.8434833   Validation Loss =  0.8135879   Validation Acc:  0.8337\n",
      "Iteration  228 : Train Loss =  0.78465635   Train Acc:  0.84385   Validation Loss =  0.81211305   Validation Acc:  0.8334\n",
      "Iteration  229 : Train Loss =  0.7845125   Train Acc:  0.84316665   Validation Loss =  0.8118208   Validation Acc:  0.8325\n",
      "Iteration  230 : Train Loss =  0.78344905   Train Acc:  0.8433833   Validation Loss =  0.8110932   Validation Acc:  0.8329\n",
      "Iteration  231 : Train Loss =  0.7813464   Train Acc:  0.84393334   Validation Loss =  0.8087473   Validation Acc:  0.8333\n",
      "Iteration  232 : Train Loss =  0.7799032   Train Acc:  0.84415   Validation Loss =  0.8073908   Validation Acc:  0.8331\n",
      "Iteration  233 : Train Loss =  0.77935964   Train Acc:  0.8440167   Validation Loss =  0.8070253   Validation Acc:  0.8339\n",
      "Iteration  234 : Train Loss =  0.77844644   Train Acc:  0.84395   Validation Loss =  0.80587137   Validation Acc:  0.8329\n",
      "Iteration  235 : Train Loss =  0.7767421   Train Acc:  0.8445333   Validation Loss =  0.804392   Validation Acc:  0.8341\n",
      "Iteration  236 : Train Loss =  0.77527785   Train Acc:  0.8448667   Validation Loss =  0.8028805   Validation Acc:  0.8324\n",
      "Iteration  237 : Train Loss =  0.7744531   Train Acc:  0.84465   Validation Loss =  0.8019751   Validation Acc:  0.8329\n",
      "Iteration  238 : Train Loss =  0.7736039   Train Acc:  0.84473336   Validation Loss =  0.80136716   Validation Acc:  0.8345\n",
      "Iteration  239 : Train Loss =  0.7722436   Train Acc:  0.84503335   Validation Loss =  0.79983324   Validation Acc:  0.8332\n",
      "Iteration  240 : Train Loss =  0.7708078   Train Acc:  0.84566665   Validation Loss =  0.7985054   Validation Acc:  0.8334\n",
      "Iteration  241 : Train Loss =  0.76974905   Train Acc:  0.8456333   Validation Loss =  0.7975186   Validation Acc:  0.8336\n",
      "Iteration  242 : Train Loss =  0.7688558   Train Acc:  0.8455667   Validation Loss =  0.79651284   Validation Acc:  0.8336\n",
      "Iteration  243 : Train Loss =  0.7677642   Train Acc:  0.84548336   Validation Loss =  0.7956127   Validation Acc:  0.8348\n",
      "Iteration  244 : Train Loss =  0.76646936   Train Acc:  0.84596664   Validation Loss =  0.7942072   Validation Acc:  0.8339\n",
      "Iteration  245 : Train Loss =  0.76528096   Train Acc:  0.84605   Validation Loss =  0.79310286   Validation Acc:  0.8339\n",
      "Iteration  246 : Train Loss =  0.76427066   Train Acc:  0.8465833   Validation Loss =  0.7921672   Validation Acc:  0.8343\n",
      "Iteration  247 : Train Loss =  0.7632952   Train Acc:  0.8463   Validation Loss =  0.7911005   Validation Acc:  0.8342\n",
      "Iteration  248 : Train Loss =  0.7621808   Train Acc:  0.8464   Validation Loss =  0.79013944   Validation Acc:  0.8353\n",
      "Iteration  249 : Train Loss =  0.7610167   Train Acc:  0.84651667   Validation Loss =  0.7888931   Validation Acc:  0.8351\n",
      "Iteration  250 : Train Loss =  0.759907   Train Acc:  0.84675   Validation Loss =  0.78785753   Validation Acc:  0.8346\n",
      "Iteration  251 : Train Loss =  0.75888366   Train Acc:  0.847   Validation Loss =  0.7868861   Validation Acc:  0.8351\n",
      "Iteration  252 : Train Loss =  0.7578565   Train Acc:  0.8471   Validation Loss =  0.78581613   Validation Acc:  0.8352\n",
      "Iteration  253 : Train Loss =  0.75681025   Train Acc:  0.84695   Validation Loss =  0.7848685   Validation Acc:  0.8356\n",
      "Iteration  254 : Train Loss =  0.75572574   Train Acc:  0.84746665   Validation Loss =  0.7837406   Validation Acc:  0.8354\n",
      "Iteration  255 : Train Loss =  0.75463974   Train Acc:  0.84746665   Validation Loss =  0.7827169   Validation Acc:  0.8352\n",
      "Iteration  256 : Train Loss =  0.75359434   Train Acc:  0.84755   Validation Loss =  0.7816871   Validation Acc:  0.835\n",
      "Iteration  257 : Train Loss =  0.75256395   Train Acc:  0.848   Validation Loss =  0.780638   Validation Acc:  0.8357\n",
      "Iteration  258 : Train Loss =  0.7515563   Train Acc:  0.84763336   Validation Loss =  0.7797123   Validation Acc:  0.8357\n",
      "Iteration  259 : Train Loss =  0.7505288   Train Acc:  0.8483667   Validation Loss =  0.7786287   Validation Acc:  0.8362\n",
      "Iteration  260 : Train Loss =  0.7494925   Train Acc:  0.84805   Validation Loss =  0.7776885   Validation Acc:  0.8354\n",
      "Iteration  261 : Train Loss =  0.74844587   Train Acc:  0.84858334   Validation Loss =  0.7766142   Validation Acc:  0.836\n",
      "Iteration  262 : Train Loss =  0.747397   Train Acc:  0.84835   Validation Loss =  0.77562046   Validation Acc:  0.8357\n",
      "Iteration  263 : Train Loss =  0.7463916   Train Acc:  0.8487333   Validation Loss =  0.77462196   Validation Acc:  0.8367\n",
      "Iteration  264 : Train Loss =  0.74540067   Train Acc:  0.84875   Validation Loss =  0.7736523   Validation Acc:  0.8366\n",
      "Iteration  265 : Train Loss =  0.74443036   Train Acc:  0.84885   Validation Loss =  0.77271616   Validation Acc:  0.8367\n",
      "Iteration  266 : Train Loss =  0.743443   Train Acc:  0.84886664   Validation Loss =  0.7717409   Validation Acc:  0.8364\n",
      "Iteration  267 : Train Loss =  0.74246895   Train Acc:  0.84926665   Validation Loss =  0.77077794   Validation Acc:  0.8366\n",
      "Iteration  268 : Train Loss =  0.7415324   Train Acc:  0.84901667   Validation Loss =  0.7698931   Validation Acc:  0.836\n",
      "Iteration  269 : Train Loss =  0.74067223   Train Acc:  0.84921664   Validation Loss =  0.7690141   Validation Acc:  0.8367\n",
      "Iteration  270 : Train Loss =  0.73992586   Train Acc:  0.8488333   Validation Loss =  0.76837444   Validation Acc:  0.835\n",
      "Iteration  271 : Train Loss =  0.7393916   Train Acc:  0.84933335   Validation Loss =  0.767738   Validation Acc:  0.8364\n",
      "Iteration  272 : Train Loss =  0.73897964   Train Acc:  0.84866667   Validation Loss =  0.7675245   Validation Acc:  0.8356\n",
      "Iteration  273 : Train Loss =  0.7389841   Train Acc:  0.84863335   Validation Loss =  0.76734906   Validation Acc:  0.8356\n",
      "Iteration  274 : Train Loss =  0.7384267   Train Acc:  0.84863335   Validation Loss =  0.7670859   Validation Acc:  0.8351\n",
      "Iteration  275 : Train Loss =  0.7379699   Train Acc:  0.8484833   Validation Loss =  0.766402   Validation Acc:  0.8347\n",
      "Iteration  276 : Train Loss =  0.73592603   Train Acc:  0.8488   Validation Loss =  0.76460576   Validation Acc:  0.8355\n",
      "Iteration  277 : Train Loss =  0.73389745   Train Acc:  0.8498167   Validation Loss =  0.7624323   Validation Acc:  0.8369\n",
      "Iteration  278 : Train Loss =  0.7320218   Train Acc:  0.8502833   Validation Loss =  0.76065624   Validation Acc:  0.8367\n",
      "Iteration  279 : Train Loss =  0.73097754   Train Acc:  0.85046667   Validation Loss =  0.7596423   Validation Acc:  0.8372\n",
      "Iteration  280 : Train Loss =  0.7305783   Train Acc:  0.8505667   Validation Loss =  0.759205   Validation Acc:  0.8378\n",
      "Iteration  281 : Train Loss =  0.7303448   Train Acc:  0.8495333   Validation Loss =  0.75914145   Validation Acc:  0.8354\n",
      "Iteration  282 : Train Loss =  0.7300875   Train Acc:  0.85005   Validation Loss =  0.7587499   Validation Acc:  0.8355\n",
      "Iteration  283 : Train Loss =  0.72890246   Train Acc:  0.84985   Validation Loss =  0.75774646   Validation Acc:  0.8359\n",
      "Iteration  284 : Train Loss =  0.7274595   Train Acc:  0.85035   Validation Loss =  0.7561623   Validation Acc:  0.837\n",
      "Iteration  285 : Train Loss =  0.7257856   Train Acc:  0.85085   Validation Loss =  0.7545765   Validation Acc:  0.8375\n",
      "Iteration  286 : Train Loss =  0.72452724   Train Acc:  0.8513333   Validation Loss =  0.7533588   Validation Acc:  0.8387\n",
      "Iteration  287 : Train Loss =  0.72370756   Train Acc:  0.85185   Validation Loss =  0.752543   Validation Acc:  0.8382\n",
      "Iteration  288 : Train Loss =  0.72315013   Train Acc:  0.85115   Validation Loss =  0.7520921   Validation Acc:  0.8369\n",
      "Iteration  289 : Train Loss =  0.7226398   Train Acc:  0.8515667   Validation Loss =  0.7515204   Validation Acc:  0.8377\n",
      "Iteration  290 : Train Loss =  0.72183836   Train Acc:  0.85106665   Validation Loss =  0.75084853   Validation Acc:  0.8363\n",
      "Iteration  291 : Train Loss =  0.7208425   Train Acc:  0.85181665   Validation Loss =  0.74978447   Validation Acc:  0.8375\n",
      "Iteration  292 : Train Loss =  0.71955603   Train Acc:  0.8518   Validation Loss =  0.74859357   Validation Acc:  0.8378\n",
      "Iteration  293 : Train Loss =  0.71837866   Train Acc:  0.8527   Validation Loss =  0.747395   Validation Acc:  0.8397\n",
      "Iteration  294 : Train Loss =  0.7173853   Train Acc:  0.8523   Validation Loss =  0.74644816   Validation Acc:  0.839\n",
      "Iteration  295 : Train Loss =  0.716552   Train Acc:  0.85211664   Validation Loss =  0.74568915   Validation Acc:  0.8396\n",
      "Iteration  296 : Train Loss =  0.71582174   Train Acc:  0.85281664   Validation Loss =  0.74492997   Validation Acc:  0.8384\n",
      "Iteration  297 : Train Loss =  0.71511924   Train Acc:  0.85216665   Validation Loss =  0.74433714   Validation Acc:  0.8381\n",
      "Iteration  298 : Train Loss =  0.71440625   Train Acc:  0.8527   Validation Loss =  0.74357843   Validation Acc:  0.8385\n",
      "Iteration  299 : Train Loss =  0.7135562   Train Acc:  0.8526   Validation Loss =  0.74284554   Validation Acc:  0.8384\n",
      "Iteration  300 : Train Loss =  0.71265507   Train Acc:  0.85291666   Validation Loss =  0.7419078   Validation Acc:  0.8386\n",
      "Iteration  301 : Train Loss =  0.71166956   Train Acc:  0.8527667   Validation Loss =  0.7410362   Validation Acc:  0.8384\n",
      "Iteration  302 : Train Loss =  0.71068656   Train Acc:  0.8533667   Validation Loss =  0.7400279   Validation Acc:  0.8391\n",
      "Iteration  303 : Train Loss =  0.7097132   Train Acc:  0.85323334   Validation Loss =  0.7391412   Validation Acc:  0.8391\n",
      "Iteration  304 : Train Loss =  0.70879376   Train Acc:  0.8538   Validation Loss =  0.7382263   Validation Acc:  0.8398\n",
      "Iteration  305 : Train Loss =  0.7079238   Train Acc:  0.8537833   Validation Loss =  0.737397   Validation Acc:  0.8397\n",
      "Iteration  306 : Train Loss =  0.70712554   Train Acc:  0.85385   Validation Loss =  0.73664355   Validation Acc:  0.8398\n",
      "Iteration  307 : Train Loss =  0.7063362   Train Acc:  0.85395   Validation Loss =  0.73586816   Validation Acc:  0.8399\n",
      "Iteration  308 : Train Loss =  0.70555985   Train Acc:  0.85368335   Validation Loss =  0.73515624   Validation Acc:  0.8391\n",
      "Iteration  309 : Train Loss =  0.70482075   Train Acc:  0.8538833   Validation Loss =  0.7344202   Validation Acc:  0.8402\n",
      "Iteration  310 : Train Loss =  0.70409226   Train Acc:  0.85396665   Validation Loss =  0.7337671   Validation Acc:  0.8392\n",
      "Iteration  311 : Train Loss =  0.70338726   Train Acc:  0.85391665   Validation Loss =  0.73307   Validation Acc:  0.8398\n",
      "Iteration  312 : Train Loss =  0.7026982   Train Acc:  0.85436666   Validation Loss =  0.732447   Validation Acc:  0.8391\n",
      "Iteration  313 : Train Loss =  0.702031   Train Acc:  0.8541833   Validation Loss =  0.73177373   Validation Acc:  0.8396\n",
      "Iteration  314 : Train Loss =  0.7012768   Train Acc:  0.85408336   Validation Loss =  0.73111093   Validation Acc:  0.8386\n",
      "Iteration  315 : Train Loss =  0.7005848   Train Acc:  0.85431665   Validation Loss =  0.73038757   Validation Acc:  0.8394\n",
      "Iteration  316 : Train Loss =  0.69979334   Train Acc:  0.85428333   Validation Loss =  0.7296759   Validation Acc:  0.8386\n",
      "Iteration  317 : Train Loss =  0.69899285   Train Acc:  0.8544667   Validation Loss =  0.7288798   Validation Acc:  0.8397\n",
      "Iteration  318 : Train Loss =  0.6980814   Train Acc:  0.85466665   Validation Loss =  0.7280311   Validation Acc:  0.8392\n",
      "Iteration  319 : Train Loss =  0.69719917   Train Acc:  0.8548667   Validation Loss =  0.72715676   Validation Acc:  0.8401\n",
      "Iteration  320 : Train Loss =  0.69629484   Train Acc:  0.85503334   Validation Loss =  0.7263261   Validation Acc:  0.8393\n",
      "Iteration  321 : Train Loss =  0.69546103   Train Acc:  0.85508335   Validation Loss =  0.72548556   Validation Acc:  0.8401\n",
      "Iteration  322 : Train Loss =  0.69461596   Train Acc:  0.85538334   Validation Loss =  0.7247064   Validation Acc:  0.8401\n",
      "Iteration  323 : Train Loss =  0.69380623   Train Acc:  0.85543334   Validation Loss =  0.72390044   Validation Acc:  0.8404\n",
      "Iteration  324 : Train Loss =  0.69307995   Train Acc:  0.85548335   Validation Loss =  0.7232503   Validation Acc:  0.8402\n",
      "Iteration  325 : Train Loss =  0.69244576   Train Acc:  0.85553336   Validation Loss =  0.72260934   Validation Acc:  0.8399\n",
      "Iteration  326 : Train Loss =  0.69194615   Train Acc:  0.85508335   Validation Loss =  0.7222339   Validation Acc:  0.8397\n",
      "Iteration  327 : Train Loss =  0.69167054   Train Acc:  0.8551667   Validation Loss =  0.72189283   Validation Acc:  0.8391\n",
      "Iteration  328 : Train Loss =  0.69162226   Train Acc:  0.85536665   Validation Loss =  0.72204167   Validation Acc:  0.8397\n",
      "Iteration  329 : Train Loss =  0.69180584   Train Acc:  0.85428333   Validation Loss =  0.7221023   Validation Acc:  0.8395\n",
      "Iteration  330 : Train Loss =  0.6919096   Train Acc:  0.85436666   Validation Loss =  0.7224758   Validation Acc:  0.8395\n",
      "Iteration  331 : Train Loss =  0.6914928   Train Acc:  0.85396665   Validation Loss =  0.72186595   Validation Acc:  0.838\n",
      "Iteration  332 : Train Loss =  0.6899841   Train Acc:  0.85511667   Validation Loss =  0.72065264   Validation Acc:  0.8403\n",
      "Iteration  333 : Train Loss =  0.68805194   Train Acc:  0.8555667   Validation Loss =  0.71850455   Validation Acc:  0.8391\n",
      "Iteration  334 : Train Loss =  0.68642724   Train Acc:  0.8559667   Validation Loss =  0.7170435   Validation Acc:  0.8409\n",
      "Iteration  335 : Train Loss =  0.6857053   Train Acc:  0.856   Validation Loss =  0.716278   Validation Acc:  0.8402\n",
      "Iteration  336 : Train Loss =  0.68581396   Train Acc:  0.85565   Validation Loss =  0.71638846   Validation Acc:  0.8402\n",
      "Iteration  337 : Train Loss =  0.6858925   Train Acc:  0.85615   Validation Loss =  0.7166124   Validation Acc:  0.8402\n",
      "Iteration  338 : Train Loss =  0.68506825   Train Acc:  0.8552833   Validation Loss =  0.7157146   Validation Acc:  0.8405\n",
      "Iteration  339 : Train Loss =  0.6834876   Train Acc:  0.8563   Validation Loss =  0.71425974   Validation Acc:  0.8401\n",
      "Iteration  340 : Train Loss =  0.6816913   Train Acc:  0.85683334   Validation Loss =  0.7124427   Validation Acc:  0.8412\n",
      "Iteration  341 : Train Loss =  0.6805515   Train Acc:  0.8572   Validation Loss =  0.7113431   Validation Acc:  0.841\n",
      "Iteration  342 : Train Loss =  0.68021667   Train Acc:  0.8570167   Validation Loss =  0.7110608   Validation Acc:  0.8407\n",
      "Iteration  343 : Train Loss =  0.6801895   Train Acc:  0.85635   Validation Loss =  0.7110523   Validation Acc:  0.8412\n",
      "Iteration  344 : Train Loss =  0.67989373   Train Acc:  0.85681665   Validation Loss =  0.7108347   Validation Acc:  0.8404\n",
      "Iteration  345 : Train Loss =  0.67901826   Train Acc:  0.85651666   Validation Loss =  0.7099626   Validation Acc:  0.8408\n",
      "Iteration  346 : Train Loss =  0.6778723   Train Acc:  0.8567167   Validation Loss =  0.7088734   Validation Acc:  0.8411\n",
      "Iteration  347 : Train Loss =  0.6769289   Train Acc:  0.8571   Validation Loss =  0.70799863   Validation Acc:  0.8427\n",
      "Iteration  348 : Train Loss =  0.67634296   Train Acc:  0.8576667   Validation Loss =  0.7073743   Validation Acc:  0.8404\n",
      "Iteration  349 : Train Loss =  0.675977   Train Acc:  0.8570833   Validation Loss =  0.7071711   Validation Acc:  0.8417\n",
      "Iteration  350 : Train Loss =  0.67548263   Train Acc:  0.85753334   Validation Loss =  0.70656526   Validation Acc:  0.8402\n",
      "Iteration  351 : Train Loss =  0.6746272   Train Acc:  0.85733336   Validation Loss =  0.7058866   Validation Acc:  0.8426\n",
      "Iteration  352 : Train Loss =  0.67358464   Train Acc:  0.8578   Validation Loss =  0.7047819   Validation Acc:  0.8406\n",
      "Iteration  353 : Train Loss =  0.67258656   Train Acc:  0.8580667   Validation Loss =  0.7038778   Validation Acc:  0.8428\n",
      "Iteration  354 : Train Loss =  0.6718608   Train Acc:  0.85828334   Validation Loss =  0.7031526   Validation Acc:  0.8418\n",
      "Iteration  355 : Train Loss =  0.67134607   Train Acc:  0.85786664   Validation Loss =  0.70269305   Validation Acc:  0.8422\n",
      "Iteration  356 : Train Loss =  0.6708486   Train Acc:  0.85801667   Validation Loss =  0.7022296   Validation Acc:  0.842\n",
      "Iteration  357 : Train Loss =  0.6702287   Train Acc:  0.8577667   Validation Loss =  0.7016581   Validation Acc:  0.8424\n",
      "Iteration  358 : Train Loss =  0.6694697   Train Acc:  0.85838336   Validation Loss =  0.7009338   Validation Acc:  0.8417\n",
      "Iteration  359 : Train Loss =  0.66866124   Train Acc:  0.85826665   Validation Loss =  0.70018387   Validation Acc:  0.843\n",
      "Iteration  360 : Train Loss =  0.66793275   Train Acc:  0.8587   Validation Loss =  0.69946814   Validation Acc:  0.8417\n",
      "Iteration  361 : Train Loss =  0.66731536   Train Acc:  0.8584333   Validation Loss =  0.6989368   Validation Acc:  0.8431\n",
      "Iteration  362 : Train Loss =  0.66677773   Train Acc:  0.85856664   Validation Loss =  0.69838953   Validation Acc:  0.8409\n",
      "Iteration  363 : Train Loss =  0.66632533   Train Acc:  0.8581333   Validation Loss =  0.6980705   Validation Acc:  0.843\n",
      "Iteration  364 : Train Loss =  0.6659175   Train Acc:  0.8587667   Validation Loss =  0.69761205   Validation Acc:  0.8417\n",
      "Iteration  365 : Train Loss =  0.6655141   Train Acc:  0.85795   Validation Loss =  0.69737417   Validation Acc:  0.8429\n",
      "Iteration  366 : Train Loss =  0.6650903   Train Acc:  0.8588167   Validation Loss =  0.69682103   Validation Acc:  0.8416\n",
      "Iteration  367 : Train Loss =  0.6648042   Train Acc:  0.85775   Validation Loss =  0.6967405   Validation Acc:  0.8424\n",
      "Iteration  368 : Train Loss =  0.66431963   Train Acc:  0.85836667   Validation Loss =  0.696109   Validation Acc:  0.8416\n",
      "Iteration  369 : Train Loss =  0.66401803   Train Acc:  0.85803336   Validation Loss =  0.69604135   Validation Acc:  0.8422\n",
      "Iteration  370 : Train Loss =  0.6631731   Train Acc:  0.85838336   Validation Loss =  0.6950544   Validation Acc:  0.8424\n",
      "Iteration  371 : Train Loss =  0.6622758   Train Acc:  0.8583   Validation Loss =  0.6943315   Validation Acc:  0.8434\n",
      "Iteration  372 : Train Loss =  0.66105056   Train Acc:  0.8591   Validation Loss =  0.69303054   Validation Acc:  0.842\n",
      "Iteration  373 : Train Loss =  0.6599611   Train Acc:  0.8591   Validation Loss =  0.6920374   Validation Acc:  0.843\n",
      "Iteration  374 : Train Loss =  0.65912455   Train Acc:  0.85941666   Validation Loss =  0.6912246   Validation Acc:  0.8429\n",
      "Iteration  375 : Train Loss =  0.6585939   Train Acc:  0.85955   Validation Loss =  0.69071794   Validation Acc:  0.8434\n",
      "Iteration  376 : Train Loss =  0.6582564   Train Acc:  0.85935   Validation Loss =  0.6904696   Validation Acc:  0.8432\n",
      "Iteration  377 : Train Loss =  0.6579902   Train Acc:  0.85936666   Validation Loss =  0.6901387   Validation Acc:  0.8427\n",
      "Iteration  378 : Train Loss =  0.6578089   Train Acc:  0.85905   Validation Loss =  0.690141   Validation Acc:  0.843\n",
      "Iteration  379 : Train Loss =  0.65736043   Train Acc:  0.85955   Validation Loss =  0.6895696   Validation Acc:  0.8426\n",
      "Iteration  380 : Train Loss =  0.6568247   Train Acc:  0.85898334   Validation Loss =  0.68926275   Validation Acc:  0.8433\n",
      "Iteration  381 : Train Loss =  0.65589875   Train Acc:  0.85955   Validation Loss =  0.6882252   Validation Acc:  0.8432\n",
      "Iteration  382 : Train Loss =  0.654935   Train Acc:  0.8595333   Validation Loss =  0.6874159   Validation Acc:  0.8431\n",
      "Iteration  383 : Train Loss =  0.6540053   Train Acc:  0.86018336   Validation Loss =  0.6864388   Validation Acc:  0.8436\n",
      "Iteration  384 : Train Loss =  0.6532786   Train Acc:  0.8595833   Validation Loss =  0.68577665   Validation Acc:  0.8435\n",
      "Iteration  385 : Train Loss =  0.6527423   Train Acc:  0.86006665   Validation Loss =  0.6852738   Validation Acc:  0.843\n",
      "Iteration  386 : Train Loss =  0.65227437   Train Acc:  0.8603333   Validation Loss =  0.6848093   Validation Acc:  0.8434\n",
      "Iteration  387 : Train Loss =  0.65188795   Train Acc:  0.86005   Validation Loss =  0.68454283   Validation Acc:  0.8433\n",
      "Iteration  388 : Train Loss =  0.65148246   Train Acc:  0.86015   Validation Loss =  0.6840749   Validation Acc:  0.8436\n",
      "Iteration  389 : Train Loss =  0.65110004   Train Acc:  0.8598167   Validation Loss =  0.68388236   Validation Acc:  0.8431\n",
      "Iteration  390 : Train Loss =  0.65058833   Train Acc:  0.8605   Validation Loss =  0.6832617   Validation Acc:  0.8435\n",
      "Iteration  391 : Train Loss =  0.65002435   Train Acc:  0.86   Validation Loss =  0.6828988   Validation Acc:  0.8433\n",
      "Iteration  392 : Train Loss =  0.6492877   Train Acc:  0.8606833   Validation Loss =  0.6820526   Validation Acc:  0.8439\n",
      "Iteration  393 : Train Loss =  0.64858115   Train Acc:  0.86036664   Validation Loss =  0.6815143   Validation Acc:  0.844\n",
      "Iteration  394 : Train Loss =  0.64786404   Train Acc:  0.8609167   Validation Loss =  0.6807479   Validation Acc:  0.8437\n",
      "Iteration  395 : Train Loss =  0.64722735   Train Acc:  0.86041665   Validation Loss =  0.68022287   Validation Acc:  0.8443\n",
      "Iteration  396 : Train Loss =  0.6466303   Train Acc:  0.86081666   Validation Loss =  0.67962325   Validation Acc:  0.8434\n",
      "Iteration  397 : Train Loss =  0.6460772   Train Acc:  0.86071664   Validation Loss =  0.6791146   Validation Acc:  0.8439\n",
      "Iteration  398 : Train Loss =  0.6455746   Train Acc:  0.8610167   Validation Loss =  0.67867094   Validation Acc:  0.844\n",
      "Iteration  399 : Train Loss =  0.64512   Train Acc:  0.8610333   Validation Loss =  0.6782223   Validation Acc:  0.8443\n",
      "Iteration  400 : Train Loss =  0.644685   Train Acc:  0.86095   Validation Loss =  0.67788476   Validation Acc:  0.8438\n",
      "Iteration  401 : Train Loss =  0.64422333   Train Acc:  0.86145   Validation Loss =  0.6773644   Validation Acc:  0.844\n",
      "Iteration  402 : Train Loss =  0.6437905   Train Acc:  0.8609167   Validation Loss =  0.6770824   Validation Acc:  0.8441\n",
      "Iteration  403 : Train Loss =  0.64333683   Train Acc:  0.86121666   Validation Loss =  0.6765443   Validation Acc:  0.8443\n",
      "Iteration  404 : Train Loss =  0.642864   Train Acc:  0.86088336   Validation Loss =  0.67627317   Validation Acc:  0.8444\n",
      "Iteration  405 : Train Loss =  0.64237237   Train Acc:  0.86156666   Validation Loss =  0.6756495   Validation Acc:  0.8444\n",
      "Iteration  406 : Train Loss =  0.6418531   Train Acc:  0.8611   Validation Loss =  0.67534727   Validation Acc:  0.8446\n",
      "Iteration  407 : Train Loss =  0.64130986   Train Acc:  0.86151665   Validation Loss =  0.6746635   Validation Acc:  0.8443\n",
      "Iteration  408 : Train Loss =  0.6407953   Train Acc:  0.86115   Validation Loss =  0.6743836   Validation Acc:  0.8442\n",
      "Iteration  409 : Train Loss =  0.6402368   Train Acc:  0.86186665   Validation Loss =  0.67368674   Validation Acc:  0.8449\n",
      "Iteration  410 : Train Loss =  0.63971215   Train Acc:  0.86141664   Validation Loss =  0.6733703   Validation Acc:  0.8439\n",
      "Iteration  411 : Train Loss =  0.639166   Train Acc:  0.86226666   Validation Loss =  0.67270285   Validation Acc:  0.8448\n",
      "Iteration  412 : Train Loss =  0.63864744   Train Acc:  0.86145   Validation Loss =  0.67238826   Validation Acc:  0.8445\n",
      "Iteration  413 : Train Loss =  0.63810724   Train Acc:  0.86215   Validation Loss =  0.67174333   Validation Acc:  0.8445\n",
      "Iteration  414 : Train Loss =  0.6376104   Train Acc:  0.86158335   Validation Loss =  0.6714252   Validation Acc:  0.8444\n",
      "Iteration  415 : Train Loss =  0.6370912   Train Acc:  0.86231667   Validation Loss =  0.67081606   Validation Acc:  0.8447\n",
      "Iteration  416 : Train Loss =  0.63658285   Train Acc:  0.86195   Validation Loss =  0.6704593   Validation Acc:  0.844\n",
      "Iteration  417 : Train Loss =  0.6360675   Train Acc:  0.8624333   Validation Loss =  0.6699058   Validation Acc:  0.8449\n",
      "Iteration  418 : Train Loss =  0.6355418   Train Acc:  0.8621167   Validation Loss =  0.66947854   Validation Acc:  0.8447\n",
      "Iteration  419 : Train Loss =  0.6350145   Train Acc:  0.86268336   Validation Loss =  0.66894364   Validation Acc:  0.8451\n",
      "Iteration  420 : Train Loss =  0.6344791   Train Acc:  0.86235   Validation Loss =  0.6684877   Validation Acc:  0.8443\n",
      "Iteration  421 : Train Loss =  0.6339559   Train Acc:  0.8629   Validation Loss =  0.6679511   Validation Acc:  0.8453\n",
      "Iteration  422 : Train Loss =  0.6334298   Train Acc:  0.86253333   Validation Loss =  0.66751564   Validation Acc:  0.8448\n",
      "Iteration  423 : Train Loss =  0.63290024   Train Acc:  0.86293334   Validation Loss =  0.66696596   Validation Acc:  0.8454\n",
      "Iteration  424 : Train Loss =  0.6324027   Train Acc:  0.86261666   Validation Loss =  0.66657287   Validation Acc:  0.8449\n",
      "Iteration  425 : Train Loss =  0.6319268   Train Acc:  0.8628   Validation Loss =  0.6660663   Validation Acc:  0.8458\n",
      "Iteration  426 : Train Loss =  0.6314614   Train Acc:  0.8627   Validation Loss =  0.66571337   Validation Acc:  0.8449\n",
      "Iteration  427 : Train Loss =  0.63103485   Train Acc:  0.8628333   Validation Loss =  0.66521454   Validation Acc:  0.8463\n",
      "Iteration  428 : Train Loss =  0.630632   Train Acc:  0.86266667   Validation Loss =  0.66498154   Validation Acc:  0.8441\n",
      "Iteration  429 : Train Loss =  0.6302524   Train Acc:  0.8632   Validation Loss =  0.66445756   Validation Acc:  0.8459\n",
      "Iteration  430 : Train Loss =  0.62998354   Train Acc:  0.86256665   Validation Loss =  0.6644558   Validation Acc:  0.8444\n",
      "Iteration  431 : Train Loss =  0.62976575   Train Acc:  0.86298335   Validation Loss =  0.6640054   Validation Acc:  0.8458\n",
      "Iteration  432 : Train Loss =  0.62972564   Train Acc:  0.86231667   Validation Loss =  0.66432816   Validation Acc:  0.8447\n",
      "Iteration  433 : Train Loss =  0.6295877   Train Acc:  0.8628333   Validation Loss =  0.6638633   Validation Acc:  0.8451\n",
      "Iteration  434 : Train Loss =  0.62955153   Train Acc:  0.8620167   Validation Loss =  0.6642442   Validation Acc:  0.8449\n",
      "Iteration  435 : Train Loss =  0.62922454   Train Acc:  0.86255   Validation Loss =  0.66354525   Validation Acc:  0.8453\n",
      "Iteration  436 : Train Loss =  0.6286584   Train Acc:  0.8620833   Validation Loss =  0.66339815   Validation Acc:  0.8457\n",
      "Iteration  437 : Train Loss =  0.6278274   Train Acc:  0.863   Validation Loss =  0.66227204   Validation Acc:  0.8457\n",
      "Iteration  438 : Train Loss =  0.6268085   Train Acc:  0.86261666   Validation Loss =  0.6615534   Validation Acc:  0.8456\n",
      "Iteration  439 : Train Loss =  0.6259431   Train Acc:  0.86295   Validation Loss =  0.660566   Validation Acc:  0.8463\n",
      "Iteration  440 : Train Loss =  0.62529933   Train Acc:  0.86343336   Validation Loss =  0.66001165   Validation Acc:  0.8473\n",
      "Iteration  441 : Train Loss =  0.6248716   Train Acc:  0.86361665   Validation Loss =  0.65968084   Validation Acc:  0.8448\n",
      "Iteration  442 : Train Loss =  0.6246127   Train Acc:  0.8635167   Validation Loss =  0.6592908   Validation Acc:  0.8459\n",
      "Iteration  443 : Train Loss =  0.624501   Train Acc:  0.8631667   Validation Loss =  0.6594789   Validation Acc:  0.8446\n",
      "Iteration  444 : Train Loss =  0.6242454   Train Acc:  0.86373335   Validation Loss =  0.65893364   Validation Acc:  0.8461\n",
      "Iteration  445 : Train Loss =  0.6239346   Train Acc:  0.86303335   Validation Loss =  0.6590165   Validation Acc:  0.8461\n",
      "Iteration  446 : Train Loss =  0.6233247   Train Acc:  0.8635   Validation Loss =  0.6580896   Validation Acc:  0.847\n",
      "Iteration  447 : Train Loss =  0.622589   Train Acc:  0.86326665   Validation Loss =  0.657679   Validation Acc:  0.8459\n",
      "Iteration  448 : Train Loss =  0.6218734   Train Acc:  0.86368334   Validation Loss =  0.656785   Validation Acc:  0.847\n",
      "Iteration  449 : Train Loss =  0.6212745   Train Acc:  0.86373335   Validation Loss =  0.6563433   Validation Acc:  0.8476\n",
      "Iteration  450 : Train Loss =  0.6208154   Train Acc:  0.8642   Validation Loss =  0.65589684   Validation Acc:  0.8456\n",
      "Iteration  451 : Train Loss =  0.62046695   Train Acc:  0.86371666   Validation Loss =  0.6555059   Validation Acc:  0.8472\n",
      "Iteration  452 : Train Loss =  0.62020266   Train Acc:  0.86413336   Validation Loss =  0.65542233   Validation Acc:  0.8443\n",
      "Iteration  453 : Train Loss =  0.6198684   Train Acc:  0.86405   Validation Loss =  0.654913   Validation Acc:  0.8462\n",
      "Iteration  454 : Train Loss =  0.6195177   Train Acc:  0.8638333   Validation Loss =  0.6548548   Validation Acc:  0.8448\n",
      "Iteration  455 : Train Loss =  0.6190084   Train Acc:  0.8642333   Validation Loss =  0.6541088   Validation Acc:  0.8468\n",
      "Iteration  456 : Train Loss =  0.61847395   Train Acc:  0.8639333   Validation Loss =  0.6538567   Validation Acc:  0.8461\n",
      "Iteration  457 : Train Loss =  0.6178746   Train Acc:  0.86445   Validation Loss =  0.65306413   Validation Acc:  0.8466\n",
      "Iteration  458 : Train Loss =  0.61729795   Train Acc:  0.8642   Validation Loss =  0.652709   Validation Acc:  0.8475\n",
      "Iteration  459 : Train Loss =  0.6168232   Train Acc:  0.86443335   Validation Loss =  0.6521354   Validation Acc:  0.8473\n",
      "Iteration  460 : Train Loss =  0.6164509   Train Acc:  0.8641833   Validation Loss =  0.65188825   Validation Acc:  0.8481\n",
      "Iteration  461 : Train Loss =  0.61609924   Train Acc:  0.8646333   Validation Loss =  0.6515278   Validation Acc:  0.8462\n",
      "Iteration  462 : Train Loss =  0.6157364   Train Acc:  0.86441666   Validation Loss =  0.65119743   Validation Acc:  0.8478\n",
      "Iteration  463 : Train Loss =  0.61537063   Train Acc:  0.86475   Validation Loss =  0.6509427   Validation Acc:  0.8463\n",
      "Iteration  464 : Train Loss =  0.61497325   Train Acc:  0.8644   Validation Loss =  0.6504695   Validation Acc:  0.8479\n",
      "Iteration  465 : Train Loss =  0.6145664   Train Acc:  0.8649   Validation Loss =  0.6502638   Validation Acc:  0.8454\n",
      "Iteration  466 : Train Loss =  0.6140983   Train Acc:  0.8646   Validation Loss =  0.6496218   Validation Acc:  0.848\n",
      "Iteration  467 : Train Loss =  0.61365366   Train Acc:  0.8647   Validation Loss =  0.64940006   Validation Acc:  0.8467\n",
      "Iteration  468 : Train Loss =  0.6131828   Train Acc:  0.8650333   Validation Loss =  0.6487897   Validation Acc:  0.8466\n",
      "Iteration  469 : Train Loss =  0.6127525   Train Acc:  0.86465   Validation Loss =  0.64855254   Validation Acc:  0.8473\n",
      "Iteration  470 : Train Loss =  0.61235833   Train Acc:  0.8650333   Validation Loss =  0.648046   Validation Acc:  0.8477\n",
      "Iteration  471 : Train Loss =  0.6119938   Train Acc:  0.8645833   Validation Loss =  0.6478534   Validation Acc:  0.8476\n",
      "Iteration  472 : Train Loss =  0.6116595   Train Acc:  0.86473334   Validation Loss =  0.6474048   Validation Acc:  0.847\n",
      "Iteration  473 : Train Loss =  0.6113504   Train Acc:  0.8645167   Validation Loss =  0.6473172   Validation Acc:  0.8476\n",
      "Iteration  474 : Train Loss =  0.6110536   Train Acc:  0.86495   Validation Loss =  0.6468285   Validation Acc:  0.8471\n",
      "Iteration  475 : Train Loss =  0.6107603   Train Acc:  0.86475   Validation Loss =  0.6468399   Validation Acc:  0.8475\n",
      "Iteration  476 : Train Loss =  0.61043334   Train Acc:  0.86501664   Validation Loss =  0.64623857   Validation Acc:  0.8472\n",
      "Iteration  477 : Train Loss =  0.61002135   Train Acc:  0.86475   Validation Loss =  0.64617693   Validation Acc:  0.8475\n",
      "Iteration  478 : Train Loss =  0.6095551   Train Acc:  0.86515   Validation Loss =  0.6454251   Validation Acc:  0.847\n",
      "Iteration  479 : Train Loss =  0.60903966   Train Acc:  0.8648667   Validation Loss =  0.64526284   Validation Acc:  0.8476\n",
      "Iteration  480 : Train Loss =  0.60856646   Train Acc:  0.8652667   Validation Loss =  0.64452785   Validation Acc:  0.8476\n",
      "Iteration  481 : Train Loss =  0.6081141   Train Acc:  0.8650333   Validation Loss =  0.64441204   Validation Acc:  0.8473\n",
      "Iteration  482 : Train Loss =  0.6077343   Train Acc:  0.8657   Validation Loss =  0.64377713   Validation Acc:  0.8478\n",
      "Iteration  483 : Train Loss =  0.6074073   Train Acc:  0.86515   Validation Loss =  0.64374405   Validation Acc:  0.847\n",
      "Iteration  484 : Train Loss =  0.60710156   Train Acc:  0.8657333   Validation Loss =  0.64321136   Validation Acc:  0.8483\n",
      "Iteration  485 : Train Loss =  0.6068874   Train Acc:  0.86525   Validation Loss =  0.6432642   Validation Acc:  0.8464\n",
      "Iteration  486 : Train Loss =  0.6066567   Train Acc:  0.8656667   Validation Loss =  0.64287925   Validation Acc:  0.8482\n",
      "Iteration  487 : Train Loss =  0.6064782   Train Acc:  0.8652167   Validation Loss =  0.6428987   Validation Acc:  0.8455\n",
      "Iteration  488 : Train Loss =  0.6061075   Train Acc:  0.8656833   Validation Loss =  0.6424373   Validation Acc:  0.8485\n",
      "Iteration  489 : Train Loss =  0.6057088   Train Acc:  0.86535   Validation Loss =  0.642178   Validation Acc:  0.8459\n",
      "Iteration  490 : Train Loss =  0.60510385   Train Acc:  0.86593336   Validation Loss =  0.64151144   Validation Acc:  0.8482\n",
      "Iteration  491 : Train Loss =  0.6045245   Train Acc:  0.8656833   Validation Loss =  0.64105606   Validation Acc:  0.8467\n",
      "Iteration  492 : Train Loss =  0.6039413   Train Acc:  0.86628336   Validation Loss =  0.64039505   Validation Acc:  0.8487\n",
      "Iteration  493 : Train Loss =  0.6034456   Train Acc:  0.86543334   Validation Loss =  0.6400724   Validation Acc:  0.8476\n",
      "Iteration  494 : Train Loss =  0.6030198   Train Acc:  0.8661   Validation Loss =  0.63952416   Validation Acc:  0.8485\n",
      "Iteration  495 : Train Loss =  0.60269195   Train Acc:  0.86581665   Validation Loss =  0.63939655   Validation Acc:  0.8486\n",
      "Iteration  496 : Train Loss =  0.6024262   Train Acc:  0.86628336   Validation Loss =  0.63895357   Validation Acc:  0.8478\n",
      "Iteration  497 : Train Loss =  0.6021878   Train Acc:  0.86578333   Validation Loss =  0.6389783   Validation Acc:  0.8489\n",
      "Iteration  498 : Train Loss =  0.6019714   Train Acc:  0.86586666   Validation Loss =  0.63854134   Validation Acc:  0.8468\n",
      "Iteration  499 : Train Loss =  0.60174996   Train Acc:  0.86593336   Validation Loss =  0.63865125   Validation Acc:  0.8484\n",
      "Iteration  500 : Train Loss =  0.60149646   Train Acc:  0.86615   Validation Loss =  0.6381231   Validation Acc:  0.8467\n",
      "Iteration  501 : Train Loss =  0.6012038   Train Acc:  0.8657   Validation Loss =  0.6381774   Validation Acc:  0.8485\n",
      "Iteration  502 : Train Loss =  0.6007754   Train Acc:  0.86628336   Validation Loss =  0.6374781   Validation Acc:  0.8469\n",
      "Iteration  503 : Train Loss =  0.60028327   Train Acc:  0.8659   Validation Loss =  0.6372916   Validation Acc:  0.8486\n",
      "Iteration  504 : Train Loss =  0.5997793   Train Acc:  0.8663667   Validation Loss =  0.6365862   Validation Acc:  0.8482\n",
      "Iteration  505 : Train Loss =  0.59927976   Train Acc:  0.8663167   Validation Loss =  0.63631034   Validation Acc:  0.8494\n",
      "Iteration  506 : Train Loss =  0.5988389   Train Acc:  0.8667   Validation Loss =  0.63573956   Validation Acc:  0.8488\n",
      "Iteration  507 : Train Loss =  0.5984888   Train Acc:  0.8660333   Validation Loss =  0.635566   Validation Acc:  0.8482\n",
      "Iteration  508 : Train Loss =  0.5981979   Train Acc:  0.86695   Validation Loss =  0.6351476   Validation Acc:  0.849\n",
      "Iteration  509 : Train Loss =  0.5980191   Train Acc:  0.86625   Validation Loss =  0.63519543   Validation Acc:  0.8477\n",
      "Iteration  510 : Train Loss =  0.5979923   Train Acc:  0.86695   Validation Loss =  0.63493955   Validation Acc:  0.8497\n",
      "Iteration  511 : Train Loss =  0.59820515   Train Acc:  0.8659   Validation Loss =  0.63553476   Validation Acc:  0.8476\n",
      "Iteration  512 : Train Loss =  0.5984372   Train Acc:  0.86645   Validation Loss =  0.6353548   Validation Acc:  0.849\n",
      "Iteration  513 : Train Loss =  0.5988609   Train Acc:  0.8656833   Validation Loss =  0.6363858   Validation Acc:  0.8465\n",
      "Iteration  514 : Train Loss =  0.59870505   Train Acc:  0.8664   Validation Loss =  0.6355847   Validation Acc:  0.8484\n",
      "Iteration  515 : Train Loss =  0.59825826   Train Acc:  0.86536664   Validation Loss =  0.6358838   Validation Acc:  0.8479\n",
      "Iteration  516 : Train Loss =  0.5971763   Train Acc:  0.8667667   Validation Loss =  0.63412803   Validation Acc:  0.8496\n",
      "Iteration  517 : Train Loss =  0.59606564   Train Acc:  0.8659833   Validation Loss =  0.6336247   Validation Acc:  0.848\n",
      "Iteration  518 : Train Loss =  0.5954101   Train Acc:  0.86723334   Validation Loss =  0.63260657   Validation Acc:  0.8486\n",
      "Iteration  519 : Train Loss =  0.595193   Train Acc:  0.86685   Validation Loss =  0.6326375   Validation Acc:  0.8485\n",
      "Iteration  520 : Train Loss =  0.5951822   Train Acc:  0.86686665   Validation Loss =  0.6326295   Validation Acc:  0.847\n",
      "Iteration  521 : Train Loss =  0.5949982   Train Acc:  0.86728334   Validation Loss =  0.63232636   Validation Acc:  0.8493\n",
      "Iteration  522 : Train Loss =  0.59480584   Train Acc:  0.86621666   Validation Loss =  0.6324007   Validation Acc:  0.847\n",
      "Iteration  523 : Train Loss =  0.59425867   Train Acc:  0.8674167   Validation Loss =  0.63156056   Validation Acc:  0.8495\n",
      "Iteration  524 : Train Loss =  0.5937159   Train Acc:  0.86651665   Validation Loss =  0.63138807   Validation Acc:  0.8487\n",
      "Iteration  525 : Train Loss =  0.5932937   Train Acc:  0.8674667   Validation Loss =  0.63068485   Validation Acc:  0.8493\n",
      "Iteration  526 : Train Loss =  0.592985   Train Acc:  0.8664   Validation Loss =  0.63073415   Validation Acc:  0.8497\n",
      "Iteration  527 : Train Loss =  0.59271187   Train Acc:  0.86716664   Validation Loss =  0.6302022   Validation Acc:  0.8482\n",
      "Iteration  528 : Train Loss =  0.5923537   Train Acc:  0.86726665   Validation Loss =  0.63013035   Validation Acc:  0.8491\n",
      "Iteration  529 : Train Loss =  0.5918767   Train Acc:  0.86725   Validation Loss =  0.62943125   Validation Acc:  0.8482\n",
      "Iteration  530 : Train Loss =  0.5913681   Train Acc:  0.8678   Validation Loss =  0.62911165   Validation Acc:  0.8497\n",
      "Iteration  531 : Train Loss =  0.5909387   Train Acc:  0.86716664   Validation Loss =  0.6286168   Validation Acc:  0.8483\n",
      "Iteration  532 : Train Loss =  0.590613   Train Acc:  0.8677   Validation Loss =  0.6283097   Validation Acc:  0.8496\n",
      "Iteration  533 : Train Loss =  0.5904009   Train Acc:  0.8673   Validation Loss =  0.6282532   Validation Acc:  0.8488\n",
      "Iteration  534 : Train Loss =  0.590204   Train Acc:  0.86775   Validation Loss =  0.62785995   Validation Acc:  0.8499\n",
      "Iteration  535 : Train Loss =  0.58991903   Train Acc:  0.8671333   Validation Loss =  0.6279481   Validation Acc:  0.85\n",
      "Iteration  536 : Train Loss =  0.5895359   Train Acc:  0.86801666   Validation Loss =  0.6271589   Validation Acc:  0.8496\n",
      "Iteration  537 : Train Loss =  0.58908707   Train Acc:  0.86761665   Validation Loss =  0.6272072   Validation Acc:  0.8504\n",
      "Iteration  538 : Train Loss =  0.58863074   Train Acc:  0.8677667   Validation Loss =  0.6263202   Validation Acc:  0.8504\n",
      "Iteration  539 : Train Loss =  0.5882128   Train Acc:  0.86785   Validation Loss =  0.6263199   Validation Acc:  0.8505\n",
      "Iteration  540 : Train Loss =  0.5878627   Train Acc:  0.86756665   Validation Loss =  0.6257808   Validation Acc:  0.8499\n",
      "Iteration  541 : Train Loss =  0.58758396   Train Acc:  0.8681   Validation Loss =  0.62558866   Validation Acc:  0.8506\n",
      "Iteration  542 : Train Loss =  0.58735216   Train Acc:  0.86775   Validation Loss =  0.62551284   Validation Acc:  0.8497\n",
      "Iteration  543 : Train Loss =  0.5871651   Train Acc:  0.8681333   Validation Loss =  0.62504584   Validation Acc:  0.8503\n",
      "Iteration  544 : Train Loss =  0.586946   Train Acc:  0.86796665   Validation Loss =  0.6252779   Validation Acc:  0.8491\n",
      "Iteration  545 : Train Loss =  0.5867169   Train Acc:  0.8681167   Validation Loss =  0.624561   Validation Acc:  0.8501\n",
      "Iteration  546 : Train Loss =  0.58639795   Train Acc:  0.8679   Validation Loss =  0.6248076   Validation Acc:  0.8489\n",
      "Iteration  547 : Train Loss =  0.58602625   Train Acc:  0.8685   Validation Loss =  0.6239993   Validation Acc:  0.8507\n",
      "Iteration  548 : Train Loss =  0.5856149   Train Acc:  0.8678833   Validation Loss =  0.6239968   Validation Acc:  0.8496\n",
      "Iteration  549 : Train Loss =  0.5852758   Train Acc:  0.86876667   Validation Loss =  0.62345546   Validation Acc:  0.8512\n",
      "Iteration  550 : Train Loss =  0.58498687   Train Acc:  0.86791664   Validation Loss =  0.62328684   Validation Acc:  0.8494\n",
      "Iteration  551 : Train Loss =  0.58470285   Train Acc:  0.8688   Validation Loss =  0.6230648   Validation Acc:  0.8506\n",
      "Iteration  552 : Train Loss =  0.5844814   Train Acc:  0.86803335   Validation Loss =  0.62274885   Validation Acc:  0.8489\n",
      "Iteration  553 : Train Loss =  0.5842326   Train Acc:  0.8686   Validation Loss =  0.62271327   Validation Acc:  0.851\n",
      "Iteration  554 : Train Loss =  0.583966   Train Acc:  0.8682167   Validation Loss =  0.622272   Validation Acc:  0.8495\n",
      "Iteration  555 : Train Loss =  0.58362615   Train Acc:  0.8685833   Validation Loss =  0.6221634   Validation Acc:  0.8505\n",
      "Iteration  556 : Train Loss =  0.58327633   Train Acc:  0.8681833   Validation Loss =  0.62170166   Validation Acc:  0.8492\n",
      "Iteration  557 : Train Loss =  0.58289844   Train Acc:  0.8689   Validation Loss =  0.6214443   Validation Acc:  0.8514\n",
      "Iteration  558 : Train Loss =  0.5825649   Train Acc:  0.86841667   Validation Loss =  0.621126   Validation Acc:  0.8499\n",
      "Iteration  559 : Train Loss =  0.5822331   Train Acc:  0.8689833   Validation Loss =  0.62074447   Validation Acc:  0.8519\n",
      "Iteration  560 : Train Loss =  0.58195335   Train Acc:  0.86836666   Validation Loss =  0.62063146   Validation Acc:  0.85\n",
      "Iteration  561 : Train Loss =  0.58168644   Train Acc:  0.8688   Validation Loss =  0.6202084   Validation Acc:  0.8516\n",
      "Iteration  562 : Train Loss =  0.5814366   Train Acc:  0.8685167   Validation Loss =  0.6202467   Validation Acc:  0.8503\n",
      "Iteration  563 : Train Loss =  0.58124024   Train Acc:  0.8692667   Validation Loss =  0.61978495   Validation Acc:  0.8517\n",
      "Iteration  564 : Train Loss =  0.58107334   Train Acc:  0.86836666   Validation Loss =  0.6199917   Validation Acc:  0.8501\n",
      "Iteration  565 : Train Loss =  0.58096355   Train Acc:  0.8693333   Validation Loss =  0.61952084   Validation Acc:  0.8511\n",
      "Iteration  566 : Train Loss =  0.58085555   Train Acc:  0.86831665   Validation Loss =  0.61988676   Validation Acc:  0.85\n",
      "Iteration  567 : Train Loss =  0.58074987   Train Acc:  0.86941665   Validation Loss =  0.61932415   Validation Acc:  0.8503\n",
      "Iteration  568 : Train Loss =  0.5805968   Train Acc:  0.86801666   Validation Loss =  0.61970294   Validation Acc:  0.8503\n",
      "Iteration  569 : Train Loss =  0.5803827   Train Acc:  0.8692167   Validation Loss =  0.61900353   Validation Acc:  0.8508\n",
      "Iteration  570 : Train Loss =  0.5800717   Train Acc:  0.8678833   Validation Loss =  0.61920595   Validation Acc:  0.8502\n",
      "Iteration  571 : Train Loss =  0.57963216   Train Acc:  0.8692167   Validation Loss =  0.6183784   Validation Acc:  0.851\n",
      "Iteration  572 : Train Loss =  0.579156   Train Acc:  0.86826664   Validation Loss =  0.61824775   Validation Acc:  0.8499\n",
      "Iteration  573 : Train Loss =  0.578621   Train Acc:  0.86955   Validation Loss =  0.6175501   Validation Acc:  0.8518\n",
      "Iteration  574 : Train Loss =  0.57817584   Train Acc:  0.8689333   Validation Loss =  0.6172142   Validation Acc:  0.8504\n",
      "Iteration  575 : Train Loss =  0.5777935   Train Acc:  0.86948335   Validation Loss =  0.61691225   Validation Acc:  0.8521\n",
      "Iteration  576 : Train Loss =  0.57749057   Train Acc:  0.86938334   Validation Loss =  0.61647713   Validation Acc:  0.8512\n",
      "Iteration  577 : Train Loss =  0.57725334   Train Acc:  0.86906666   Validation Loss =  0.6165354   Validation Acc:  0.8513\n",
      "Iteration  578 : Train Loss =  0.5770579   Train Acc:  0.86905   Validation Loss =  0.61599636   Validation Acc:  0.852\n",
      "Iteration  579 : Train Loss =  0.5769011   Train Acc:  0.86865   Validation Loss =  0.6163278   Validation Acc:  0.8513\n",
      "Iteration  580 : Train Loss =  0.57674897   Train Acc:  0.8693   Validation Loss =  0.6156795   Validation Acc:  0.8518\n",
      "Iteration  581 : Train Loss =  0.57654905   Train Acc:  0.8688167   Validation Loss =  0.6160864   Validation Acc:  0.8514\n",
      "Iteration  582 : Train Loss =  0.5763089   Train Acc:  0.8696333   Validation Loss =  0.61530805   Validation Acc:  0.8522\n",
      "Iteration  583 : Train Loss =  0.5759924   Train Acc:  0.86883336   Validation Loss =  0.6155541   Validation Acc:  0.8514\n",
      "Iteration  584 : Train Loss =  0.575663   Train Acc:  0.86978334   Validation Loss =  0.61481863   Validation Acc:  0.8523\n",
      "Iteration  585 : Train Loss =  0.57532185   Train Acc:  0.8688667   Validation Loss =  0.61485195   Validation Acc:  0.851\n",
      "Iteration  586 : Train Loss =  0.5750525   Train Acc:  0.86953336   Validation Loss =  0.61438817   Validation Acc:  0.8525\n",
      "Iteration  587 : Train Loss =  0.5748887   Train Acc:  0.8692333   Validation Loss =  0.6143593   Validation Acc:  0.8508\n",
      "Iteration  588 : Train Loss =  0.5747642   Train Acc:  0.86971664   Validation Loss =  0.6142534   Validation Acc:  0.8513\n",
      "Iteration  589 : Train Loss =  0.5747565   Train Acc:  0.86906666   Validation Loss =  0.6141917   Validation Acc:  0.8509\n",
      "Iteration  590 : Train Loss =  0.5746453   Train Acc:  0.86918336   Validation Loss =  0.6142807   Validation Acc:  0.8509\n",
      "Iteration  591 : Train Loss =  0.57460463   Train Acc:  0.8693   Validation Loss =  0.61403114   Validation Acc:  0.8509\n",
      "Iteration  592 : Train Loss =  0.57425106   Train Acc:  0.86901665   Validation Loss =  0.6139943   Validation Acc:  0.8504\n",
      "Iteration  593 : Train Loss =  0.5738266   Train Acc:  0.86953336   Validation Loss =  0.613308   Validation Acc:  0.8513\n",
      "Iteration  594 : Train Loss =  0.57321286   Train Acc:  0.86948335   Validation Loss =  0.61298907   Validation Acc:  0.852\n",
      "Iteration  595 : Train Loss =  0.5726445   Train Acc:  0.87   Validation Loss =  0.61225116   Validation Acc:  0.8509\n",
      "Iteration  596 : Train Loss =  0.57216275   Train Acc:  0.8698   Validation Loss =  0.61193377   Validation Acc:  0.8531\n",
      "Iteration  597 : Train Loss =  0.57182837   Train Acc:  0.87021667   Validation Loss =  0.6115732   Validation Acc:  0.8522\n",
      "Iteration  598 : Train Loss =  0.57158345   Train Acc:  0.87011665   Validation Loss =  0.6113237   Validation Acc:  0.8526\n",
      "Iteration  599 : Train Loss =  0.5714275   Train Acc:  0.87011665   Validation Loss =  0.61129403   Validation Acc:  0.8531\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEWCAYAAABPON1ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxpgYZ4naGGNinCVqY4yJcf8fv6Ok37oB51YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    learning_rate = 0.003\n",
    "    num_Iterations = 600\n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    validation_accuracy = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    # load and prepare the training and test data\n",
    "    tr_x, tr_y, te_x, te_y = load_Prepare_Data()\n",
    "    \n",
    "    # To avoid problems related to type conversion, all data is converted to  float32 data types\n",
    "    tr_x = tf.cast(tr_x, tf.float32)\n",
    "    te_x = tf.cast(te_x, tf.float32)\n",
    "    tr_y = tf.cast(tr_y, tf.float32)\n",
    "    te_y = tf.cast(te_y, tf.float32)\n",
    "    \n",
    "    alpha = tf.Variable(0.0003, tf.float32)\n",
    "    #Initialize the values of the weights and bias for the 1st hidden layer\n",
    "    w1 = tf.Variable(tf.random.normal([ 300, tr_x.shape[0]], mean=0.0, stddev=0.05))\n",
    "    b1 = tf.Variable(tf.zeros([300, 1]))\n",
    "    #Initialize the values of the weights and bias for the 2nd hidden layer\n",
    "    w2 = tf.Variable(tf.random.normal([100, 300], mean=0.0, stddev=0.05))\n",
    "    b2 = tf.Variable(tf.zeros([100, 1]))\n",
    "    #Initialize the values of the weights and bias for the SoftMax layer\n",
    "    w3 = tf.Variable(tf.random.normal([ tr_y.shape[0], 100], mean=0.0, stddev=0.05))\n",
    "    b3 = tf.Variable(tf.zeros([tr_y.shape[0], 1]))\n",
    "    \n",
    "    # Iterate the training loop\n",
    "    for i in range(num_Iterations):\n",
    "        \n",
    "        # Create an instance of Gradient Tape to monitor the forward pass to calcualte the gradients based on the training data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = forward_pass(tr_x, w1, b1, w2, b2, w3, b3)\n",
    "            currentLoss = cross_entropy_L1Reg(tr_y, y_pred, w1, w2, w3, alpha)\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        gradients = tape.gradient(currentLoss, [w1, b1, w2, b2, w3, b3])\n",
    "        # Determine the prediction accuracy for the training data\n",
    "        accuracy = calculate_accuracy(tr_y, y_pred)\n",
    "\n",
    "        train_accuracy.append(accuracy)\n",
    "        train_loss.append(currentLoss)\n",
    "\n",
    "        # Calculate forward pass, loss and accuracy for the valdation data\n",
    "        te_y_pred = forward_pass(te_x, w1, b1, w2, b2, w3, b3) \n",
    "        te_currentLoss = cross_entropy_L1Reg(te_y, te_y_pred, w1, w2, w3, alpha)\n",
    "        te_accuracy = calculate_accuracy(te_y, te_y_pred) \n",
    "        validation_accuracy.append(te_accuracy)\n",
    "        validation_loss.append(te_currentLoss)\n",
    "\n",
    "        print (\"Iteration \", i, \": Train Loss = \",currentLoss.numpy(), \"  Train Acc: \", accuracy.numpy(), \"  Validation Loss = \", te_currentLoss.numpy(), \"  Validation Acc: \", te_accuracy.numpy())\n",
    "        # Update the trainable parameters using Adam Optimizer\n",
    "        adam_optimizer.apply_gradients(zip(gradients, [w1, b1, w2, b2, w3, b3]))\n",
    "\n",
    "    # Plot the training and the validation accuracy and loss        \n",
    "    plt.plot(train_accuracy, label=\"Train Acc\")\n",
    "    plt.plot(train_loss, label=\"Train Loss\")\n",
    "    plt.plot(validation_accuracy, label=\"Validation Acc\")\n",
    "    plt.plot(validation_loss, label=\"Validation Loss\")\n",
    "    plt.title('NN with L1 Regularization')\n",
    "    plt.xlabel(\"No. of Iterations\")\n",
    "    plt.text(200, 2, r'$L1\\ Regularization\\ rate=%.4f$' % (alpha.numpy()))\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show();\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Question1_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
