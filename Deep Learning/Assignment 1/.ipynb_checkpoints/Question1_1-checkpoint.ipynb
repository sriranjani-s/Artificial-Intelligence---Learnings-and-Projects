{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dc6E-k4jZ3vg"
   },
   "source": [
    "# R00182510 - Assignment1 PART A - Task 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eq_e-228il-R",
    "outputId": "0edbcb7a-82c5-424c-b4ed-5c786d1cc106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "# To enable TF2\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1BYdw6FiDAw"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "random.seed(182510)\n",
    "\n",
    "# load and prepare the training and test data\n",
    "def load_Prepare_Data():\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "    # load the training and test data    \n",
    "    (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
    "\n",
    "    # reshape the feature data\n",
    "    tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
    "    te_x = te_x.reshape(te_x.shape[0], 784)\n",
    "\n",
    "    # noramlise feature data\n",
    "    tr_x = tr_x / 255.0\n",
    "    te_x = te_x / 255.0\n",
    "\n",
    "    print( \"Shape of training features \", tr_x.shape)\n",
    "    print( \"Shape of test features \", te_x.shape)\n",
    "\n",
    "\n",
    "    # one hot encode the training labels and get the transpose\n",
    "    tr_y = np_utils.to_categorical(tr_y,10)\n",
    "    tr_y = tr_y.T\n",
    "    print (\"Shape of training labels \", tr_y.shape)\n",
    "\n",
    "    # one hot encode the test labels and get the transpose\n",
    "    te_y = np_utils.to_categorical(te_y,10)\n",
    "    te_y = te_y.T\n",
    "    print (\"Shape of testing labels \", te_y.shape)\n",
    "    \n",
    "    # Reshape the training data and test data so \n",
    "    # that the features becomes the rows of the matrix\n",
    "    tr_x = tr_x.T\n",
    "    te_x = te_x.T\n",
    "\n",
    "    print(\"Reshaped training data \", tr_x.shape)\n",
    "    print(\"Reshaped test data \",te_x.shape)\n",
    "    \n",
    "    return(tr_x, tr_y, te_x, te_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGtxlql5iI6i"
   },
   "outputs": [],
   "source": [
    "# push all training data through the softmax layer\n",
    "def forward_pass(x, w_T, b):\n",
    "\n",
    "    # Multiply each training example by the weights and add the bias\n",
    "    y_pred = tf.matmul(w_T, x) + b\n",
    "\n",
    "    # Pipe the results through the Softmax activation in the following steps\n",
    "    t = tf.math.exp(y_pred)\n",
    "    t_sum = tf.reduce_sum(t, 0)\n",
    "    # Reshape the data to apply the normalization to t\n",
    "    t_sum = tf.reshape(t_sum,[1, -1])\n",
    "\n",
    "    #output of the Softmax is just t normalized by dividing by the sum of t. \n",
    "    y_pred_softmax = tf.divide(t, t_sum)\n",
    "\n",
    "    return y_pred_softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLrDkTR7iMyt"
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, y_pred):\n",
    "\n",
    "    # Calculate the cross entropy error for all training data \n",
    "    cross_entropy_loss = -(tf.reduce_sum(tf.multiply(y, tf.math.log(y_pred)), 0))\n",
    "\n",
    "    # Calculate cost which is the mean cross entropy error/ average loss across all training instances\n",
    "    cost = tf.reduce_mean(cross_entropy_loss)\n",
    "\n",
    "    # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y, axis=0))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Re-LAj-YiPcL"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y, y_pred_softmax):\n",
    "\n",
    "    # Calculate the predictions in the form of a boolean array, by considering only the class with the highest probability\n",
    "    # 1 if True (correct prediction), 0 if False (incorrect prediction)\n",
    "    predictions_bool = tf.equal(tf.argmax(y_pred_softmax, 0), tf.argmax(y, 0))\n",
    "    predictions_correct = tf.cast(predictions_bool, tf.float32)\n",
    "\n",
    "    # Finally, we just determine the mean value of the correct predictions\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "  \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "VXXBrZIAiSZi",
    "outputId": "bca8c677-8699-4617-fd92-8c556815924d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training features  (60000, 784)\n",
      "Shape of test features  (10000, 784)\n",
      "Shape of training labels  (10, 60000)\n",
      "Shape of testing labels  (10, 10000)\n",
      "Reshaped training data  (784, 60000)\n",
      "Reshaped test data  (784, 10000)\n",
      "Iteration  0 : Train Loss =  2.597725   Train Acc:  0.06256667   Validation Loss =  2.5906568   Validation Acc:  0.0622\n",
      "Iteration  1 : Train Loss =  2.4564667   Train Acc:  0.047733333   Validation Loss =  2.4498906   Validation Acc:  0.0437\n",
      "Iteration  2 : Train Loss =  2.3418713   Train Acc:  0.06763333   Validation Loss =  2.3359427   Validation Acc:  0.0675\n",
      "Iteration  3 : Train Loss =  2.2446566   Train Acc:  0.1216   Validation Loss =  2.2393558   Validation Acc:  0.1239\n",
      "Iteration  4 : Train Loss =  2.1580234   Train Acc:  0.17175   Validation Loss =  2.153284   Validation Acc:  0.1783\n",
      "Iteration  5 : Train Loss =  2.0792289   Train Acc:  0.21608333   Validation Loss =  2.074991   Validation Acc:  0.222\n",
      "Iteration  6 : Train Loss =  2.0071876   Train Acc:  0.26465   Validation Loss =  2.0034108   Validation Acc:  0.2702\n",
      "Iteration  7 : Train Loss =  1.9408982   Train Acc:  0.31613332   Validation Loss =  1.937558   Validation Acc:  0.3215\n",
      "Iteration  8 : Train Loss =  1.8790897   Train Acc:  0.366   Validation Loss =  1.8761764   Validation Acc:  0.3742\n",
      "Iteration  9 : Train Loss =  1.8205143   Train Acc:  0.40706667   Validation Loss =  1.8180269   Validation Acc:  0.4099\n",
      "Iteration  10 : Train Loss =  1.7642943   Train Acc:  0.44036666   Validation Loss =  1.7622383   Validation Acc:  0.4403\n",
      "Iteration  11 : Train Loss =  1.7100585   Train Acc:  0.46851668   Validation Loss =  1.7084483   Validation Acc:  0.4659\n",
      "Iteration  12 : Train Loss =  1.6578587   Train Acc:  0.4932   Validation Loss =  1.6567129   Validation Acc:  0.4906\n",
      "Iteration  13 : Train Loss =  1.6079755   Train Acc:  0.5136   Validation Loss =  1.6073136   Validation Acc:  0.5096\n",
      "Iteration  14 : Train Loss =  1.5607038   Train Acc:  0.5295   Validation Loss =  1.5605422   Validation Acc:  0.5229\n",
      "Iteration  15 : Train Loss =  1.5161973   Train Acc:  0.54   Validation Loss =  1.516541   Validation Acc:  0.5316\n",
      "Iteration  16 : Train Loss =  1.4744146   Train Acc:  0.5467167   Validation Loss =  1.4752556   Validation Acc:  0.5377\n",
      "Iteration  17 : Train Loss =  1.4351808   Train Acc:  0.5519   Validation Loss =  1.4364957   Validation Acc:  0.5403\n",
      "Iteration  18 : Train Loss =  1.3982992   Train Acc:  0.5569   Validation Loss =  1.4000524   Validation Acc:  0.5469\n",
      "Iteration  19 : Train Loss =  1.3636267   Train Acc:  0.5621667   Validation Loss =  1.3657774   Validation Acc:  0.5521\n",
      "Iteration  20 : Train Loss =  1.3310841   Train Acc:  0.5707667   Validation Loss =  1.33359   Validation Acc:  0.5605\n",
      "Iteration  21 : Train Loss =  1.3006221   Train Acc:  0.58455   Validation Loss =  1.3034486   Validation Acc:  0.5734\n",
      "Iteration  22 : Train Loss =  1.2721902   Train Acc:  0.6034167   Validation Loss =  1.2753114   Validation Acc:  0.5934\n",
      "Iteration  23 : Train Loss =  1.2457212   Train Acc:  0.6214667   Validation Loss =  1.2491201   Validation Acc:  0.6126\n",
      "Iteration  24 : Train Loss =  1.2211195   Train Acc:  0.6358   Validation Loss =  1.2247896   Validation Acc:  0.6256\n",
      "Iteration  25 : Train Loss =  1.1982453   Train Acc:  0.6458667   Validation Loss =  1.2021858   Validation Acc:  0.6395\n",
      "Iteration  26 : Train Loss =  1.1769027   Train Acc:  0.65423334   Validation Loss =  1.1811159   Validation Acc:  0.647\n",
      "Iteration  27 : Train Loss =  1.1568534   Train Acc:  0.661   Validation Loss =  1.1613423   Validation Acc:  0.6523\n",
      "Iteration  28 : Train Loss =  1.1378615   Train Acc:  0.6656167   Validation Loss =  1.1426268   Validation Acc:  0.6579\n",
      "Iteration  29 : Train Loss =  1.1197428   Train Acc:  0.6702667   Validation Loss =  1.1247842   Validation Acc:  0.6601\n",
      "Iteration  30 : Train Loss =  1.1023953   Train Acc:  0.67478335   Validation Loss =  1.1077114   Validation Acc:  0.6651\n",
      "Iteration  31 : Train Loss =  1.0857894   Train Acc:  0.6791   Validation Loss =  1.0913795   Validation Acc:  0.6685\n",
      "Iteration  32 : Train Loss =  1.0699376   Train Acc:  0.6832833   Validation Loss =  1.0758011   Validation Acc:  0.6732\n",
      "Iteration  33 : Train Loss =  1.054861   Train Acc:  0.6867333   Validation Loss =  1.0609987   Validation Acc:  0.6767\n",
      "Iteration  34 : Train Loss =  1.0405712   Train Acc:  0.68981665   Validation Loss =  1.046983   Validation Acc:  0.6795\n",
      "Iteration  35 : Train Loss =  1.0270627   Train Acc:  0.69336665   Validation Loss =  1.0337473   Validation Acc:  0.6825\n",
      "Iteration  36 : Train Loss =  1.0143113   Train Acc:  0.69556665   Validation Loss =  1.0212649   Validation Acc:  0.6847\n",
      "Iteration  37 : Train Loss =  1.0022733   Train Acc:  0.6979   Validation Loss =  1.0094879   Validation Acc:  0.6877\n",
      "Iteration  38 : Train Loss =  0.9908858   Train Acc:  0.7009   Validation Loss =  0.9983501   Validation Acc:  0.6895\n",
      "Iteration  39 : Train Loss =  0.9800759   Train Acc:  0.70278335   Validation Loss =  0.987775   Validation Acc:  0.6916\n",
      "Iteration  40 : Train Loss =  0.96977216   Train Acc:  0.70505   Validation Loss =  0.97768986   Validation Acc:  0.6945\n",
      "Iteration  41 : Train Loss =  0.9599147   Train Acc:  0.7075   Validation Loss =  0.96803594   Validation Acc:  0.6966\n",
      "Iteration  42 : Train Loss =  0.95045924   Train Acc:  0.7097833   Validation Loss =  0.9587707   Validation Acc:  0.699\n",
      "Iteration  43 : Train Loss =  0.9413743   Train Acc:  0.7118   Validation Loss =  0.9498666   Validation Acc:  0.7014\n",
      "Iteration  44 : Train Loss =  0.9326374   Train Acc:  0.7140167   Validation Loss =  0.9413039   Validation Acc:  0.7033\n",
      "Iteration  45 : Train Loss =  0.92423356   Train Acc:  0.7163333   Validation Loss =  0.93306935   Validation Acc:  0.7061\n",
      "Iteration  46 : Train Loss =  0.9161532   Train Acc:  0.71826667   Validation Loss =  0.9251545   Validation Acc:  0.7083\n",
      "Iteration  47 : Train Loss =  0.9083888   Train Acc:  0.72045   Validation Loss =  0.91755056   Validation Acc:  0.7108\n",
      "Iteration  48 : Train Loss =  0.90093094   Train Acc:  0.72263336   Validation Loss =  0.91024685   Validation Acc:  0.7125\n",
      "Iteration  49 : Train Loss =  0.8937661   Train Acc:  0.72503334   Validation Loss =  0.9032281   Validation Acc:  0.7144\n",
      "Iteration  50 : Train Loss =  0.886877   Train Acc:  0.72716665   Validation Loss =  0.8964758   Validation Acc:  0.7153\n",
      "Iteration  51 : Train Loss =  0.8802463   Train Acc:  0.7288833   Validation Loss =  0.88997287   Validation Acc:  0.7174\n",
      "Iteration  52 : Train Loss =  0.8738563   Train Acc:  0.73073334   Validation Loss =  0.88370353   Validation Acc:  0.7193\n",
      "Iteration  53 : Train Loss =  0.86768985   Train Acc:  0.73275   Validation Loss =  0.87765247   Validation Acc:  0.7216\n",
      "Iteration  54 : Train Loss =  0.8617297   Train Acc:  0.73425   Validation Loss =  0.8718055   Validation Acc:  0.7236\n",
      "Iteration  55 : Train Loss =  0.85596   Train Acc:  0.7357   Validation Loss =  0.86614925   Validation Acc:  0.7255\n",
      "Iteration  56 : Train Loss =  0.8503683   Train Acc:  0.7374667   Validation Loss =  0.8606725   Validation Acc:  0.7273\n",
      "Iteration  57 : Train Loss =  0.8449448   Train Acc:  0.7384833   Validation Loss =  0.8553664   Validation Acc:  0.7283\n",
      "Iteration  58 : Train Loss =  0.83968204   Train Acc:  0.73975   Validation Loss =  0.85022366   Validation Acc:  0.7293\n",
      "Iteration  59 : Train Loss =  0.8345729   Train Acc:  0.74153334   Validation Loss =  0.84523606   Validation Acc:  0.7305\n",
      "Iteration  60 : Train Loss =  0.8296112   Train Acc:  0.74285   Validation Loss =  0.8403967   Validation Acc:  0.7324\n",
      "Iteration  61 : Train Loss =  0.8247908   Train Acc:  0.74401665   Validation Loss =  0.8356993   Validation Acc:  0.7334\n",
      "Iteration  62 : Train Loss =  0.8201066   Train Acc:  0.74508333   Validation Loss =  0.8311391   Validation Acc:  0.7346\n",
      "Iteration  63 : Train Loss =  0.81555325   Train Acc:  0.747   Validation Loss =  0.82671   Validation Acc:  0.7356\n",
      "Iteration  64 : Train Loss =  0.8111243   Train Acc:  0.74833333   Validation Loss =  0.8224065   Validation Acc:  0.7367\n",
      "Iteration  65 : Train Loss =  0.80681294   Train Acc:  0.7495   Validation Loss =  0.81822145   Validation Acc:  0.7381\n",
      "Iteration  66 : Train Loss =  0.8026132   Train Acc:  0.75091666   Validation Loss =  0.81414884   Validation Acc:  0.7391\n",
      "Iteration  67 : Train Loss =  0.79851955   Train Acc:  0.7523   Validation Loss =  0.8101823   Validation Acc:  0.7405\n",
      "Iteration  68 : Train Loss =  0.79452735   Train Acc:  0.7535833   Validation Loss =  0.80631655   Validation Acc:  0.7426\n",
      "Iteration  69 : Train Loss =  0.7906324   Train Acc:  0.75481665   Validation Loss =  0.80254674   Validation Acc:  0.7439\n",
      "Iteration  70 : Train Loss =  0.7868305   Train Acc:  0.75585   Validation Loss =  0.79886794   Validation Acc:  0.7461\n",
      "Iteration  71 : Train Loss =  0.78311807   Train Acc:  0.75735   Validation Loss =  0.7952766   Validation Acc:  0.747\n",
      "Iteration  72 : Train Loss =  0.7794918   Train Acc:  0.7586167   Validation Loss =  0.79176915   Validation Acc:  0.7484\n",
      "Iteration  73 : Train Loss =  0.7759488   Train Acc:  0.7596667   Validation Loss =  0.78834337   Validation Acc:  0.7493\n",
      "Iteration  74 : Train Loss =  0.7724857   Train Acc:  0.7610667   Validation Loss =  0.7849957   Validation Acc:  0.7507\n",
      "Iteration  75 : Train Loss =  0.76909894   Train Acc:  0.7621167   Validation Loss =  0.7817227   Validation Acc:  0.752\n",
      "Iteration  76 : Train Loss =  0.7657856   Train Acc:  0.76316667   Validation Loss =  0.77852064   Validation Acc:  0.7529\n",
      "Iteration  77 : Train Loss =  0.7625422   Train Acc:  0.76415   Validation Loss =  0.77538645   Validation Acc:  0.7537\n",
      "Iteration  78 : Train Loss =  0.7593664   Train Acc:  0.76515   Validation Loss =  0.7723163   Validation Acc:  0.7544\n",
      "Iteration  79 : Train Loss =  0.7562557   Train Acc:  0.76575   Validation Loss =  0.7693081   Validation Acc:  0.7551\n",
      "Iteration  80 : Train Loss =  0.75320745   Train Acc:  0.76641667   Validation Loss =  0.7663591   Validation Acc:  0.7553\n",
      "Iteration  81 : Train Loss =  0.7502194   Train Acc:  0.7672   Validation Loss =  0.76346785   Validation Acc:  0.7562\n",
      "Iteration  82 : Train Loss =  0.7472902   Train Acc:  0.76823336   Validation Loss =  0.76063347   Validation Acc:  0.7572\n",
      "Iteration  83 : Train Loss =  0.7444177   Train Acc:  0.76928335   Validation Loss =  0.7578547   Validation Acc:  0.7584\n",
      "Iteration  84 : Train Loss =  0.7416004   Train Acc:  0.77025   Validation Loss =  0.7551307   Validation Acc:  0.759\n",
      "Iteration  85 : Train Loss =  0.73883665   Train Acc:  0.77096665   Validation Loss =  0.7524602   Validation Acc:  0.7594\n",
      "Iteration  86 : Train Loss =  0.73612475   Train Acc:  0.7716333   Validation Loss =  0.7498413   Validation Acc:  0.7597\n",
      "Iteration  87 : Train Loss =  0.73346317   Train Acc:  0.77238333   Validation Loss =  0.74727255   Validation Acc:  0.7598\n",
      "Iteration  88 : Train Loss =  0.7308501   Train Acc:  0.7733   Validation Loss =  0.74475217   Validation Acc:  0.7607\n",
      "Iteration  89 : Train Loss =  0.7282839   Train Acc:  0.77423334   Validation Loss =  0.7422788   Validation Acc:  0.7618\n",
      "Iteration  90 : Train Loss =  0.725763   Train Acc:  0.77485   Validation Loss =  0.7398511   Validation Acc:  0.7625\n",
      "Iteration  91 : Train Loss =  0.723286   Train Acc:  0.77535   Validation Loss =  0.7374682   Validation Acc:  0.7631\n",
      "Iteration  92 : Train Loss =  0.72085154   Train Acc:  0.7761667   Validation Loss =  0.73512834   Validation Acc:  0.764\n",
      "Iteration  93 : Train Loss =  0.7184582   Train Acc:  0.7769167   Validation Loss =  0.73283106   Validation Acc:  0.7648\n",
      "Iteration  94 : Train Loss =  0.7161052   Train Acc:  0.7776167   Validation Loss =  0.7305745   Validation Acc:  0.7654\n",
      "Iteration  95 : Train Loss =  0.713791   Train Acc:  0.7783   Validation Loss =  0.72835743   Validation Acc:  0.7659\n",
      "Iteration  96 : Train Loss =  0.71151483   Train Acc:  0.7789   Validation Loss =  0.72617805   Validation Acc:  0.7664\n",
      "Iteration  97 : Train Loss =  0.70927554   Train Acc:  0.7794667   Validation Loss =  0.7240343   Validation Acc:  0.7672\n",
      "Iteration  98 : Train Loss =  0.7070722   Train Acc:  0.77996665   Validation Loss =  0.72192574   Validation Acc:  0.7674\n",
      "Iteration  99 : Train Loss =  0.7049038   Train Acc:  0.78073335   Validation Loss =  0.7198508   Validation Acc:  0.7681\n",
      "Iteration  100 : Train Loss =  0.7027694   Train Acc:  0.78115   Validation Loss =  0.7178089   Validation Acc:  0.768\n",
      "Iteration  101 : Train Loss =  0.7006682   Train Acc:  0.7817   Validation Loss =  0.7157992   Validation Acc:  0.7689\n",
      "Iteration  102 : Train Loss =  0.6985993   Train Acc:  0.78218335   Validation Loss =  0.71382135   Validation Acc:  0.7692\n",
      "Iteration  103 : Train Loss =  0.6965617   Train Acc:  0.78283334   Validation Loss =  0.71187425   Validation Acc:  0.7698\n",
      "Iteration  104 : Train Loss =  0.69455475   Train Acc:  0.78346664   Validation Loss =  0.70995706   Validation Acc:  0.7703\n",
      "Iteration  105 : Train Loss =  0.6925777   Train Acc:  0.7841667   Validation Loss =  0.70806897   Validation Acc:  0.7707\n",
      "Iteration  106 : Train Loss =  0.6906296   Train Acc:  0.78468335   Validation Loss =  0.70620847   Validation Acc:  0.771\n",
      "Iteration  107 : Train Loss =  0.6887099   Train Acc:  0.78508335   Validation Loss =  0.70437527   Validation Acc:  0.7719\n",
      "Iteration  108 : Train Loss =  0.68681777   Train Acc:  0.78565   Validation Loss =  0.70256823   Validation Acc:  0.7724\n",
      "Iteration  109 : Train Loss =  0.68495274   Train Acc:  0.7862667   Validation Loss =  0.7007872   Validation Acc:  0.7731\n",
      "Iteration  110 : Train Loss =  0.68311393   Train Acc:  0.78686666   Validation Loss =  0.6990315   Validation Acc:  0.7734\n",
      "Iteration  111 : Train Loss =  0.68130076   Train Acc:  0.7873333   Validation Loss =  0.6973008   Validation Acc:  0.7742\n",
      "Iteration  112 : Train Loss =  0.67951274   Train Acc:  0.78783333   Validation Loss =  0.69559497   Validation Acc:  0.7745\n",
      "Iteration  113 : Train Loss =  0.67774934   Train Acc:  0.7882   Validation Loss =  0.6939131   Validation Acc:  0.7752\n",
      "Iteration  114 : Train Loss =  0.6760098   Train Acc:  0.7887667   Validation Loss =  0.6922543   Validation Acc:  0.7755\n",
      "Iteration  115 : Train Loss =  0.6742938   Train Acc:  0.7891667   Validation Loss =  0.69061834   Validation Acc:  0.7757\n",
      "Iteration  116 : Train Loss =  0.6726005   Train Acc:  0.7894167   Validation Loss =  0.6890047   Validation Acc:  0.7759\n",
      "Iteration  117 : Train Loss =  0.6709297   Train Acc:  0.78976667   Validation Loss =  0.6874126   Validation Acc:  0.7764\n",
      "Iteration  118 : Train Loss =  0.6692807   Train Acc:  0.79031664   Validation Loss =  0.6858417   Validation Acc:  0.7771\n",
      "Iteration  119 : Train Loss =  0.66765326   Train Acc:  0.79081666   Validation Loss =  0.6842917   Validation Acc:  0.7777\n",
      "Iteration  120 : Train Loss =  0.66604656   Train Acc:  0.79125   Validation Loss =  0.6827626   Validation Acc:  0.7781\n",
      "Iteration  121 : Train Loss =  0.66446054   Train Acc:  0.79185   Validation Loss =  0.6812536   Validation Acc:  0.7784\n",
      "Iteration  122 : Train Loss =  0.66289455   Train Acc:  0.7925   Validation Loss =  0.67976433   Validation Acc:  0.7792\n",
      "Iteration  123 : Train Loss =  0.6613481   Train Acc:  0.79291666   Validation Loss =  0.6782948   Validation Acc:  0.7798\n",
      "Iteration  124 : Train Loss =  0.65982085   Train Acc:  0.79333335   Validation Loss =  0.6768438   Validation Acc:  0.7801\n",
      "Iteration  125 : Train Loss =  0.65831244   Train Acc:  0.79373336   Validation Loss =  0.6754114   Validation Acc:  0.7803\n",
      "Iteration  126 : Train Loss =  0.6568225   Train Acc:  0.7942167   Validation Loss =  0.6739969   Validation Acc:  0.7808\n",
      "Iteration  127 : Train Loss =  0.6553505   Train Acc:  0.79455   Validation Loss =  0.6726001   Validation Acc:  0.7809\n",
      "Iteration  128 : Train Loss =  0.6538963   Train Acc:  0.79496664   Validation Loss =  0.6712206   Validation Acc:  0.7816\n",
      "Iteration  129 : Train Loss =  0.6524595   Train Acc:  0.79545   Validation Loss =  0.669858   Validation Acc:  0.7819\n",
      "Iteration  130 : Train Loss =  0.6510396   Train Acc:  0.7958   Validation Loss =  0.6685124   Validation Acc:  0.7826\n",
      "Iteration  131 : Train Loss =  0.6496363   Train Acc:  0.7959667   Validation Loss =  0.6671832   Validation Acc:  0.7827\n",
      "Iteration  132 : Train Loss =  0.6482495   Train Acc:  0.7963   Validation Loss =  0.66586995   Validation Acc:  0.7829\n",
      "Iteration  133 : Train Loss =  0.6468786   Train Acc:  0.7967167   Validation Loss =  0.6645725   Validation Acc:  0.7834\n",
      "Iteration  134 : Train Loss =  0.6455233   Train Acc:  0.79705   Validation Loss =  0.66329026   Validation Acc:  0.7836\n",
      "Iteration  135 : Train Loss =  0.64418364   Train Acc:  0.79758334   Validation Loss =  0.6620228   Validation Acc:  0.7837\n",
      "Iteration  136 : Train Loss =  0.642859   Train Acc:  0.798   Validation Loss =  0.6607704   Validation Acc:  0.784\n",
      "Iteration  137 : Train Loss =  0.6415492   Train Acc:  0.7984833   Validation Loss =  0.65953207   Validation Acc:  0.7844\n",
      "Iteration  138 : Train Loss =  0.64025396   Train Acc:  0.79865   Validation Loss =  0.6583081   Validation Acc:  0.7848\n",
      "Iteration  139 : Train Loss =  0.63897306   Train Acc:  0.7991167   Validation Loss =  0.65709805   Validation Acc:  0.7849\n",
      "Iteration  140 : Train Loss =  0.6377061   Train Acc:  0.7994   Validation Loss =  0.6559015   Validation Acc:  0.7856\n",
      "Iteration  141 : Train Loss =  0.636453   Train Acc:  0.79985   Validation Loss =  0.65471804   Validation Acc:  0.7861\n",
      "Iteration  142 : Train Loss =  0.63521343   Train Acc:  0.8002167   Validation Loss =  0.65354794   Validation Acc:  0.787\n",
      "Iteration  143 : Train Loss =  0.6339871   Train Acc:  0.80051666   Validation Loss =  0.65239054   Validation Acc:  0.7876\n",
      "Iteration  144 : Train Loss =  0.63277376   Train Acc:  0.8008   Validation Loss =  0.6512458   Validation Acc:  0.7882\n",
      "Iteration  145 : Train Loss =  0.6315733   Train Acc:  0.8010833   Validation Loss =  0.6501133   Validation Acc:  0.7881\n",
      "Iteration  146 : Train Loss =  0.63038546   Train Acc:  0.8013833   Validation Loss =  0.64899325   Validation Acc:  0.7885\n",
      "Iteration  147 : Train Loss =  0.62921005   Train Acc:  0.8017   Validation Loss =  0.6478851   Validation Acc:  0.7885\n",
      "Iteration  148 : Train Loss =  0.62804663   Train Acc:  0.80186665   Validation Loss =  0.64678866   Validation Acc:  0.7889\n",
      "Iteration  149 : Train Loss =  0.6268953   Train Acc:  0.80216664   Validation Loss =  0.64570403   Validation Acc:  0.7893\n",
      "Iteration  150 : Train Loss =  0.6257557   Train Acc:  0.8024333   Validation Loss =  0.6446307   Validation Acc:  0.79\n",
      "Iteration  151 : Train Loss =  0.6246277   Train Acc:  0.80275   Validation Loss =  0.6435685   Validation Acc:  0.7903\n",
      "Iteration  152 : Train Loss =  0.6235111   Train Acc:  0.80296665   Validation Loss =  0.6425175   Validation Acc:  0.7912\n",
      "Iteration  153 : Train Loss =  0.6224056   Train Acc:  0.80333334   Validation Loss =  0.64147735   Validation Acc:  0.7915\n",
      "Iteration  154 : Train Loss =  0.62131107   Train Acc:  0.80355   Validation Loss =  0.6404477   Validation Acc:  0.7921\n",
      "Iteration  155 : Train Loss =  0.6202273   Train Acc:  0.8038333   Validation Loss =  0.63942873   Validation Acc:  0.7924\n",
      "Iteration  156 : Train Loss =  0.61915416   Train Acc:  0.8042   Validation Loss =  0.63842005   Validation Acc:  0.7926\n",
      "Iteration  157 : Train Loss =  0.6180916   Train Acc:  0.80436665   Validation Loss =  0.63742137   Validation Acc:  0.7933\n",
      "Iteration  158 : Train Loss =  0.61703926   Train Acc:  0.8048   Validation Loss =  0.6364328   Validation Acc:  0.7935\n",
      "Iteration  159 : Train Loss =  0.615997   Train Acc:  0.80508333   Validation Loss =  0.635454   Validation Acc:  0.7941\n",
      "Iteration  160 : Train Loss =  0.61496484   Train Acc:  0.8053167   Validation Loss =  0.6344848   Validation Acc:  0.7939\n",
      "Iteration  161 : Train Loss =  0.6139424   Train Acc:  0.8053833   Validation Loss =  0.6335252   Validation Acc:  0.794\n",
      "Iteration  162 : Train Loss =  0.61292964   Train Acc:  0.80558336   Validation Loss =  0.63257504   Validation Acc:  0.7945\n",
      "Iteration  163 : Train Loss =  0.6119263   Train Acc:  0.8059   Validation Loss =  0.6316342   Validation Acc:  0.795\n",
      "Iteration  164 : Train Loss =  0.6109324   Train Acc:  0.80621666   Validation Loss =  0.6307022   Validation Acc:  0.7956\n",
      "Iteration  165 : Train Loss =  0.6099478   Train Acc:  0.8064833   Validation Loss =  0.6297795   Validation Acc:  0.7958\n",
      "Iteration  166 : Train Loss =  0.6089721   Train Acc:  0.80685   Validation Loss =  0.62886536   Validation Acc:  0.7956\n",
      "Iteration  167 : Train Loss =  0.6080054   Train Acc:  0.80705   Validation Loss =  0.62795985   Validation Acc:  0.7958\n",
      "Iteration  168 : Train Loss =  0.6070475   Train Acc:  0.8073   Validation Loss =  0.62706304   Validation Acc:  0.7959\n",
      "Iteration  169 : Train Loss =  0.6060984   Train Acc:  0.8075333   Validation Loss =  0.6261747   Validation Acc:  0.7964\n",
      "Iteration  170 : Train Loss =  0.6051577   Train Acc:  0.80765   Validation Loss =  0.6252945   Validation Acc:  0.7963\n",
      "Iteration  171 : Train Loss =  0.6042254   Train Acc:  0.8078667   Validation Loss =  0.62442267   Validation Acc:  0.7961\n",
      "Iteration  172 : Train Loss =  0.6033014   Train Acc:  0.8081   Validation Loss =  0.62355876   Validation Acc:  0.7964\n",
      "Iteration  173 : Train Loss =  0.6023855   Train Acc:  0.80833334   Validation Loss =  0.6227028   Validation Acc:  0.7964\n",
      "Iteration  174 : Train Loss =  0.60147786   Train Acc:  0.8085333   Validation Loss =  0.62185466   Validation Acc:  0.7968\n",
      "Iteration  175 : Train Loss =  0.60057807   Train Acc:  0.80865   Validation Loss =  0.6210141   Validation Acc:  0.7968\n",
      "Iteration  176 : Train Loss =  0.5996861   Train Acc:  0.80885   Validation Loss =  0.6201813   Validation Acc:  0.7969\n",
      "Iteration  177 : Train Loss =  0.59880173   Train Acc:  0.80913335   Validation Loss =  0.61935604   Validation Acc:  0.7972\n",
      "Iteration  178 : Train Loss =  0.597925   Train Acc:  0.80935   Validation Loss =  0.61853796   Validation Acc:  0.7972\n",
      "Iteration  179 : Train Loss =  0.59705585   Train Acc:  0.80953336   Validation Loss =  0.6177273   Validation Acc:  0.7972\n",
      "Iteration  180 : Train Loss =  0.596194   Train Acc:  0.80981666   Validation Loss =  0.6169237   Validation Acc:  0.7973\n",
      "Iteration  181 : Train Loss =  0.5953394   Train Acc:  0.81011665   Validation Loss =  0.6161272   Validation Acc:  0.7975\n",
      "Iteration  182 : Train Loss =  0.5944922   Train Acc:  0.8103667   Validation Loss =  0.61533767   Validation Acc:  0.7978\n",
      "Iteration  183 : Train Loss =  0.59365183   Train Acc:  0.8106   Validation Loss =  0.614555   Validation Acc:  0.7982\n",
      "Iteration  184 : Train Loss =  0.5928186   Train Acc:  0.8108   Validation Loss =  0.6137791   Validation Acc:  0.7985\n",
      "Iteration  185 : Train Loss =  0.59199214   Train Acc:  0.8109   Validation Loss =  0.6130097   Validation Acc:  0.799\n",
      "Iteration  186 : Train Loss =  0.5911725   Train Acc:  0.8111167   Validation Loss =  0.6122472   Validation Acc:  0.7988\n",
      "Iteration  187 : Train Loss =  0.5903596   Train Acc:  0.81125   Validation Loss =  0.6114909   Validation Acc:  0.7988\n",
      "Iteration  188 : Train Loss =  0.58955324   Train Acc:  0.8116   Validation Loss =  0.61074114   Validation Acc:  0.7991\n",
      "Iteration  189 : Train Loss =  0.5887535   Train Acc:  0.81186664   Validation Loss =  0.60999775   Validation Acc:  0.7989\n",
      "Iteration  190 : Train Loss =  0.58796024   Train Acc:  0.81198335   Validation Loss =  0.60926056   Validation Acc:  0.7991\n",
      "Iteration  191 : Train Loss =  0.58717316   Train Acc:  0.81236666   Validation Loss =  0.6085295   Validation Acc:  0.7991\n",
      "Iteration  192 : Train Loss =  0.5863926   Train Acc:  0.8127   Validation Loss =  0.6078044   Validation Acc:  0.7992\n",
      "Iteration  193 : Train Loss =  0.58561796   Train Acc:  0.81295   Validation Loss =  0.60708565   Validation Acc:  0.7993\n",
      "Iteration  194 : Train Loss =  0.58484954   Train Acc:  0.81308335   Validation Loss =  0.60637254   Validation Acc:  0.7996\n",
      "Iteration  195 : Train Loss =  0.58408713   Train Acc:  0.81333333   Validation Loss =  0.6056653   Validation Acc:  0.7996\n",
      "Iteration  196 : Train Loss =  0.5833307   Train Acc:  0.81346667   Validation Loss =  0.6049638   Validation Acc:  0.7999\n",
      "Iteration  197 : Train Loss =  0.58258015   Train Acc:  0.8136333   Validation Loss =  0.6042682   Validation Acc:  0.8\n",
      "Iteration  198 : Train Loss =  0.5818353   Train Acc:  0.81378335   Validation Loss =  0.60357803   Validation Acc:  0.8003\n",
      "Iteration  199 : Train Loss =  0.58109635   Train Acc:  0.814   Validation Loss =  0.60289353   Validation Acc:  0.8004\n",
      "Iteration  200 : Train Loss =  0.5803629   Train Acc:  0.81405   Validation Loss =  0.60221434   Validation Acc:  0.8005\n",
      "Iteration  201 : Train Loss =  0.5796351   Train Acc:  0.8143333   Validation Loss =  0.6015406   Validation Acc:  0.8006\n",
      "Iteration  202 : Train Loss =  0.57891273   Train Acc:  0.81441665   Validation Loss =  0.6008723   Validation Acc:  0.8011\n",
      "Iteration  203 : Train Loss =  0.5781958   Train Acc:  0.81458336   Validation Loss =  0.6002092   Validation Acc:  0.8012\n",
      "Iteration  204 : Train Loss =  0.5774845   Train Acc:  0.81486666   Validation Loss =  0.5995514   Validation Acc:  0.8017\n",
      "Iteration  205 : Train Loss =  0.57677823   Train Acc:  0.81515   Validation Loss =  0.59889865   Validation Acc:  0.8018\n",
      "Iteration  206 : Train Loss =  0.5760772   Train Acc:  0.8153833   Validation Loss =  0.59825087   Validation Acc:  0.8024\n",
      "Iteration  207 : Train Loss =  0.5753815   Train Acc:  0.81555   Validation Loss =  0.5976084   Validation Acc:  0.8026\n",
      "Iteration  208 : Train Loss =  0.5746909   Train Acc:  0.8157667   Validation Loss =  0.5969707   Validation Acc:  0.8028\n",
      "Iteration  209 : Train Loss =  0.57400537   Train Acc:  0.81598336   Validation Loss =  0.5963377   Validation Acc:  0.8028\n",
      "Iteration  210 : Train Loss =  0.57332474   Train Acc:  0.8161833   Validation Loss =  0.59571   Validation Acc:  0.8033\n",
      "Iteration  211 : Train Loss =  0.572649   Train Acc:  0.8163   Validation Loss =  0.5950867   Validation Acc:  0.8037\n",
      "Iteration  212 : Train Loss =  0.5719783   Train Acc:  0.8165333   Validation Loss =  0.5944682   Validation Acc:  0.8041\n",
      "Iteration  213 : Train Loss =  0.57131225   Train Acc:  0.81665   Validation Loss =  0.5938545   Validation Acc:  0.8046\n",
      "Iteration  214 : Train Loss =  0.5706511   Train Acc:  0.8167   Validation Loss =  0.5932452   Validation Acc:  0.8053\n",
      "Iteration  215 : Train Loss =  0.56999457   Train Acc:  0.8169   Validation Loss =  0.5926405   Validation Acc:  0.805\n",
      "Iteration  216 : Train Loss =  0.56934285   Train Acc:  0.81696665   Validation Loss =  0.59204024   Validation Acc:  0.8048\n",
      "Iteration  217 : Train Loss =  0.5686954   Train Acc:  0.8171333   Validation Loss =  0.59144455   Validation Acc:  0.8049\n",
      "Iteration  218 : Train Loss =  0.5680527   Train Acc:  0.81731665   Validation Loss =  0.5908532   Validation Acc:  0.8057\n",
      "Iteration  219 : Train Loss =  0.5674146   Train Acc:  0.81738335   Validation Loss =  0.5902663   Validation Acc:  0.8063\n",
      "Iteration  220 : Train Loss =  0.5667808   Train Acc:  0.81755   Validation Loss =  0.5896837   Validation Acc:  0.8068\n",
      "Iteration  221 : Train Loss =  0.56615144   Train Acc:  0.81766665   Validation Loss =  0.58910507   Validation Acc:  0.8073\n",
      "Iteration  222 : Train Loss =  0.5655263   Train Acc:  0.8178167   Validation Loss =  0.58853084   Validation Acc:  0.8074\n",
      "Iteration  223 : Train Loss =  0.5649056   Train Acc:  0.81785   Validation Loss =  0.58796066   Validation Acc:  0.8075\n",
      "Iteration  224 : Train Loss =  0.56428915   Train Acc:  0.81803334   Validation Loss =  0.5873947   Validation Acc:  0.8077\n",
      "Iteration  225 : Train Loss =  0.5636767   Train Acc:  0.81811666   Validation Loss =  0.58683264   Validation Acc:  0.8078\n",
      "Iteration  226 : Train Loss =  0.5630686   Train Acc:  0.81825   Validation Loss =  0.5862747   Validation Acc:  0.8078\n",
      "Iteration  227 : Train Loss =  0.5624645   Train Acc:  0.8185   Validation Loss =  0.5857207   Validation Acc:  0.8082\n",
      "Iteration  228 : Train Loss =  0.56186444   Train Acc:  0.8186833   Validation Loss =  0.5851705   Validation Acc:  0.8083\n",
      "Iteration  229 : Train Loss =  0.56126845   Train Acc:  0.8188   Validation Loss =  0.5846244   Validation Acc:  0.8083\n",
      "Iteration  230 : Train Loss =  0.56067646   Train Acc:  0.8190167   Validation Loss =  0.58408195   Validation Acc:  0.8085\n",
      "Iteration  231 : Train Loss =  0.5600883   Train Acc:  0.81916666   Validation Loss =  0.58354336   Validation Acc:  0.8088\n",
      "Iteration  232 : Train Loss =  0.55950403   Train Acc:  0.8193   Validation Loss =  0.5830084   Validation Acc:  0.8088\n",
      "Iteration  233 : Train Loss =  0.55892354   Train Acc:  0.81945   Validation Loss =  0.58247733   Validation Acc:  0.8088\n",
      "Iteration  234 : Train Loss =  0.558347   Train Acc:  0.8196167   Validation Loss =  0.58194983   Validation Acc:  0.8092\n",
      "Iteration  235 : Train Loss =  0.55777407   Train Acc:  0.81981665   Validation Loss =  0.581426   Validation Acc:  0.8093\n",
      "Iteration  236 : Train Loss =  0.55720496   Train Acc:  0.8200333   Validation Loss =  0.5809057   Validation Acc:  0.8092\n",
      "Iteration  237 : Train Loss =  0.5566396   Train Acc:  0.8200667   Validation Loss =  0.58038896   Validation Acc:  0.8094\n",
      "Iteration  238 : Train Loss =  0.55607766   Train Acc:  0.8201333   Validation Loss =  0.5798757   Validation Acc:  0.8095\n",
      "Iteration  239 : Train Loss =  0.5555194   Train Acc:  0.82026666   Validation Loss =  0.5793659   Validation Acc:  0.8098\n",
      "Iteration  240 : Train Loss =  0.5549648   Train Acc:  0.8204   Validation Loss =  0.57885957   Validation Acc:  0.81\n",
      "Iteration  241 : Train Loss =  0.5544137   Train Acc:  0.82055   Validation Loss =  0.5783566   Validation Acc:  0.8103\n",
      "Iteration  242 : Train Loss =  0.5538659   Train Acc:  0.82063335   Validation Loss =  0.577857   Validation Acc:  0.8103\n",
      "Iteration  243 : Train Loss =  0.55332166   Train Acc:  0.8207667   Validation Loss =  0.57736075   Validation Acc:  0.8105\n",
      "Iteration  244 : Train Loss =  0.552781   Train Acc:  0.82098335   Validation Loss =  0.5768679   Validation Acc:  0.8105\n",
      "Iteration  245 : Train Loss =  0.55224353   Train Acc:  0.82123333   Validation Loss =  0.5763781   Validation Acc:  0.8105\n",
      "Iteration  246 : Train Loss =  0.5517094   Train Acc:  0.8214333   Validation Loss =  0.5758917   Validation Acc:  0.8104\n",
      "Iteration  247 : Train Loss =  0.5511787   Train Acc:  0.82161665   Validation Loss =  0.5754083   Validation Acc:  0.8104\n",
      "Iteration  248 : Train Loss =  0.55065125   Train Acc:  0.82175   Validation Loss =  0.57492816   Validation Acc:  0.8108\n",
      "Iteration  249 : Train Loss =  0.55012697   Train Acc:  0.82195   Validation Loss =  0.57445115   Validation Acc:  0.8112\n",
      "Iteration  250 : Train Loss =  0.54960597   Train Acc:  0.82196665   Validation Loss =  0.5739772   Validation Acc:  0.8115\n",
      "Iteration  251 : Train Loss =  0.54908806   Train Acc:  0.82196665   Validation Loss =  0.57350636   Validation Acc:  0.8118\n",
      "Iteration  252 : Train Loss =  0.54857343   Train Acc:  0.82203335   Validation Loss =  0.57303846   Validation Acc:  0.8122\n",
      "Iteration  253 : Train Loss =  0.54806185   Train Acc:  0.8221167   Validation Loss =  0.57257366   Validation Acc:  0.8123\n",
      "Iteration  254 : Train Loss =  0.54755336   Train Acc:  0.8222833   Validation Loss =  0.5721117   Validation Acc:  0.8124\n",
      "Iteration  255 : Train Loss =  0.5470479   Train Acc:  0.82238334   Validation Loss =  0.5716529   Validation Acc:  0.8127\n",
      "Iteration  256 : Train Loss =  0.5465455   Train Acc:  0.8225833   Validation Loss =  0.57119685   Validation Acc:  0.8128\n",
      "Iteration  257 : Train Loss =  0.546046   Train Acc:  0.82273334   Validation Loss =  0.57074374   Validation Acc:  0.813\n",
      "Iteration  258 : Train Loss =  0.5455496   Train Acc:  0.8229167   Validation Loss =  0.57029325   Validation Acc:  0.8133\n",
      "Iteration  259 : Train Loss =  0.54505605   Train Acc:  0.82308334   Validation Loss =  0.5698458   Validation Acc:  0.8133\n",
      "Iteration  260 : Train Loss =  0.54456544   Train Acc:  0.82325   Validation Loss =  0.56940114   Validation Acc:  0.8135\n",
      "Iteration  261 : Train Loss =  0.54407775   Train Acc:  0.82346666   Validation Loss =  0.5689592   Validation Acc:  0.8136\n",
      "Iteration  262 : Train Loss =  0.5435928   Train Acc:  0.8236833   Validation Loss =  0.56852007   Validation Acc:  0.8138\n",
      "Iteration  263 : Train Loss =  0.5431108   Train Acc:  0.8237333   Validation Loss =  0.56808364   Validation Acc:  0.8141\n",
      "Iteration  264 : Train Loss =  0.5426315   Train Acc:  0.82386667   Validation Loss =  0.5676498   Validation Acc:  0.8141\n",
      "Iteration  265 : Train Loss =  0.542155   Train Acc:  0.8240167   Validation Loss =  0.56721866   Validation Acc:  0.8146\n",
      "Iteration  266 : Train Loss =  0.5416813   Train Acc:  0.82416666   Validation Loss =  0.5667902   Validation Acc:  0.8147\n",
      "Iteration  267 : Train Loss =  0.5412103   Train Acc:  0.8243   Validation Loss =  0.56636435   Validation Acc:  0.815\n",
      "Iteration  268 : Train Loss =  0.5407419   Train Acc:  0.8244333   Validation Loss =  0.5659411   Validation Acc:  0.8151\n",
      "Iteration  269 : Train Loss =  0.5402763   Train Acc:  0.82458335   Validation Loss =  0.5655204   Validation Acc:  0.8152\n",
      "Iteration  270 : Train Loss =  0.5398132   Train Acc:  0.8247833   Validation Loss =  0.5651022   Validation Acc:  0.8153\n",
      "Iteration  271 : Train Loss =  0.53935295   Train Acc:  0.8249   Validation Loss =  0.56468636   Validation Acc:  0.8154\n",
      "Iteration  272 : Train Loss =  0.5388951   Train Acc:  0.82505   Validation Loss =  0.56427324   Validation Acc:  0.8154\n",
      "Iteration  273 : Train Loss =  0.5384399   Train Acc:  0.82515   Validation Loss =  0.5638625   Validation Acc:  0.8154\n",
      "Iteration  274 : Train Loss =  0.53798723   Train Acc:  0.82521665   Validation Loss =  0.5634543   Validation Acc:  0.8153\n",
      "Iteration  275 : Train Loss =  0.53753704   Train Acc:  0.8254333   Validation Loss =  0.56304836   Validation Acc:  0.8153\n",
      "Iteration  276 : Train Loss =  0.5370894   Train Acc:  0.82563335   Validation Loss =  0.56264484   Validation Acc:  0.8155\n",
      "Iteration  277 : Train Loss =  0.53664416   Train Acc:  0.8258333   Validation Loss =  0.5622438   Validation Acc:  0.8155\n",
      "Iteration  278 : Train Loss =  0.5362015   Train Acc:  0.82605   Validation Loss =  0.561845   Validation Acc:  0.8154\n",
      "Iteration  279 : Train Loss =  0.5357612   Train Acc:  0.82608336   Validation Loss =  0.56144863   Validation Acc:  0.8154\n",
      "Iteration  280 : Train Loss =  0.53532326   Train Acc:  0.8262   Validation Loss =  0.5610545   Validation Acc:  0.8157\n",
      "Iteration  281 : Train Loss =  0.53488785   Train Acc:  0.82628334   Validation Loss =  0.5606627   Validation Acc:  0.8158\n",
      "Iteration  282 : Train Loss =  0.53445476   Train Acc:  0.82631665   Validation Loss =  0.5602732   Validation Acc:  0.8156\n",
      "Iteration  283 : Train Loss =  0.5340239   Train Acc:  0.82636666   Validation Loss =  0.55988586   Validation Acc:  0.8156\n",
      "Iteration  284 : Train Loss =  0.5335955   Train Acc:  0.8264167   Validation Loss =  0.55950075   Validation Acc:  0.8157\n",
      "Iteration  285 : Train Loss =  0.5331693   Train Acc:  0.8265333   Validation Loss =  0.55911785   Validation Acc:  0.8158\n",
      "Iteration  286 : Train Loss =  0.5327454   Train Acc:  0.82665   Validation Loss =  0.55873716   Validation Acc:  0.816\n",
      "Iteration  287 : Train Loss =  0.5323239   Train Acc:  0.82678336   Validation Loss =  0.5583587   Validation Acc:  0.816\n",
      "Iteration  288 : Train Loss =  0.5319046   Train Acc:  0.8268667   Validation Loss =  0.5579823   Validation Acc:  0.8161\n",
      "Iteration  289 : Train Loss =  0.5314874   Train Acc:  0.82706666   Validation Loss =  0.557608   Validation Acc:  0.8161\n",
      "Iteration  290 : Train Loss =  0.5310725   Train Acc:  0.82725   Validation Loss =  0.557236   Validation Acc:  0.8163\n",
      "Iteration  291 : Train Loss =  0.5306598   Train Acc:  0.82748336   Validation Loss =  0.5568659   Validation Acc:  0.8163\n",
      "Iteration  292 : Train Loss =  0.5302493   Train Acc:  0.8275   Validation Loss =  0.55649793   Validation Acc:  0.8163\n",
      "Iteration  293 : Train Loss =  0.5298409   Train Acc:  0.82755   Validation Loss =  0.55613214   Validation Acc:  0.8162\n",
      "Iteration  294 : Train Loss =  0.5294346   Train Acc:  0.82773334   Validation Loss =  0.5557681   Validation Acc:  0.8163\n",
      "Iteration  295 : Train Loss =  0.5290304   Train Acc:  0.82788336   Validation Loss =  0.5554063   Validation Acc:  0.8164\n",
      "Iteration  296 : Train Loss =  0.5286284   Train Acc:  0.82795   Validation Loss =  0.55504644   Validation Acc:  0.8164\n",
      "Iteration  297 : Train Loss =  0.5282284   Train Acc:  0.8279   Validation Loss =  0.55468845   Validation Acc:  0.8164\n",
      "Iteration  298 : Train Loss =  0.5278305   Train Acc:  0.828   Validation Loss =  0.5543326   Validation Acc:  0.8166\n",
      "Iteration  299 : Train Loss =  0.52743465   Train Acc:  0.8282   Validation Loss =  0.5539787   Validation Acc:  0.8167\n",
      "Iteration  300 : Train Loss =  0.5270408   Train Acc:  0.8283167   Validation Loss =  0.55362654   Validation Acc:  0.8167\n",
      "Iteration  301 : Train Loss =  0.52664894   Train Acc:  0.8284   Validation Loss =  0.5532764   Validation Acc:  0.8166\n",
      "Iteration  302 : Train Loss =  0.5262591   Train Acc:  0.82841665   Validation Loss =  0.5529282   Validation Acc:  0.8165\n",
      "Iteration  303 : Train Loss =  0.5258712   Train Acc:  0.8286167   Validation Loss =  0.55258185   Validation Acc:  0.8165\n",
      "Iteration  304 : Train Loss =  0.5254853   Train Acc:  0.8288   Validation Loss =  0.5522374   Validation Acc:  0.8167\n",
      "Iteration  305 : Train Loss =  0.52510136   Train Acc:  0.82885   Validation Loss =  0.55189484   Validation Acc:  0.8171\n",
      "Iteration  306 : Train Loss =  0.52471936   Train Acc:  0.82893336   Validation Loss =  0.551554   Validation Acc:  0.8171\n",
      "Iteration  307 : Train Loss =  0.5243392   Train Acc:  0.82916665   Validation Loss =  0.55121505   Validation Acc:  0.817\n",
      "Iteration  308 : Train Loss =  0.5239609   Train Acc:  0.8293   Validation Loss =  0.55087805   Validation Acc:  0.8171\n",
      "Iteration  309 : Train Loss =  0.52358454   Train Acc:  0.82926667   Validation Loss =  0.55054253   Validation Acc:  0.8169\n",
      "Iteration  310 : Train Loss =  0.5232101   Train Acc:  0.8293333   Validation Loss =  0.550209   Validation Acc:  0.817\n",
      "Iteration  311 : Train Loss =  0.52283746   Train Acc:  0.8294167   Validation Loss =  0.54987717   Validation Acc:  0.817\n",
      "Iteration  312 : Train Loss =  0.5224667   Train Acc:  0.82946664   Validation Loss =  0.54954696   Validation Acc:  0.8169\n",
      "Iteration  313 : Train Loss =  0.5220977   Train Acc:  0.82965   Validation Loss =  0.54921865   Validation Acc:  0.817\n",
      "Iteration  314 : Train Loss =  0.52173054   Train Acc:  0.8298333   Validation Loss =  0.548892   Validation Acc:  0.8172\n",
      "Iteration  315 : Train Loss =  0.52136517   Train Acc:  0.82995   Validation Loss =  0.5485672   Validation Acc:  0.8172\n",
      "Iteration  316 : Train Loss =  0.5210015   Train Acc:  0.8300167   Validation Loss =  0.54824394   Validation Acc:  0.8174\n",
      "Iteration  317 : Train Loss =  0.5206397   Train Acc:  0.83005   Validation Loss =  0.5479223   Validation Acc:  0.8174\n",
      "Iteration  318 : Train Loss =  0.5202796   Train Acc:  0.83015   Validation Loss =  0.5476024   Validation Acc:  0.8173\n",
      "Iteration  319 : Train Loss =  0.51992124   Train Acc:  0.83021665   Validation Loss =  0.5472843   Validation Acc:  0.8174\n",
      "Iteration  320 : Train Loss =  0.5195646   Train Acc:  0.83028334   Validation Loss =  0.5469677   Validation Acc:  0.8176\n",
      "Iteration  321 : Train Loss =  0.5192097   Train Acc:  0.8303667   Validation Loss =  0.54665256   Validation Acc:  0.8177\n",
      "Iteration  322 : Train Loss =  0.51885647   Train Acc:  0.83045   Validation Loss =  0.5463393   Validation Acc:  0.8177\n",
      "Iteration  323 : Train Loss =  0.5185049   Train Acc:  0.83045   Validation Loss =  0.5460274   Validation Acc:  0.8178\n",
      "Iteration  324 : Train Loss =  0.51815504   Train Acc:  0.8305333   Validation Loss =  0.54571724   Validation Acc:  0.8179\n",
      "Iteration  325 : Train Loss =  0.51780677   Train Acc:  0.83065   Validation Loss =  0.5454086   Validation Acc:  0.8181\n",
      "Iteration  326 : Train Loss =  0.5174602   Train Acc:  0.8308   Validation Loss =  0.5451014   Validation Acc:  0.8181\n",
      "Iteration  327 : Train Loss =  0.5171152   Train Acc:  0.8308833   Validation Loss =  0.5447959   Validation Acc:  0.8182\n",
      "Iteration  328 : Train Loss =  0.51677185   Train Acc:  0.83103335   Validation Loss =  0.544492   Validation Acc:  0.8183\n",
      "Iteration  329 : Train Loss =  0.5164302   Train Acc:  0.83115   Validation Loss =  0.5441896   Validation Acc:  0.8183\n",
      "Iteration  330 : Train Loss =  0.51609004   Train Acc:  0.83125   Validation Loss =  0.54388845   Validation Acc:  0.8183\n",
      "Iteration  331 : Train Loss =  0.5157515   Train Acc:  0.8314   Validation Loss =  0.543589   Validation Acc:  0.8185\n",
      "Iteration  332 : Train Loss =  0.5154145   Train Acc:  0.8314833   Validation Loss =  0.54329115   Validation Acc:  0.8186\n",
      "Iteration  333 : Train Loss =  0.515079   Train Acc:  0.8315333   Validation Loss =  0.54299456   Validation Acc:  0.8185\n",
      "Iteration  334 : Train Loss =  0.51474506   Train Acc:  0.83166665   Validation Loss =  0.5426995   Validation Acc:  0.8184\n",
      "Iteration  335 : Train Loss =  0.5144128   Train Acc:  0.83176666   Validation Loss =  0.54240596   Validation Acc:  0.8185\n",
      "Iteration  336 : Train Loss =  0.51408195   Train Acc:  0.8319   Validation Loss =  0.54211384   Validation Acc:  0.8187\n",
      "Iteration  337 : Train Loss =  0.5137525   Train Acc:  0.83205   Validation Loss =  0.5418231   Validation Acc:  0.8185\n",
      "Iteration  338 : Train Loss =  0.5134248   Train Acc:  0.8322   Validation Loss =  0.54153377   Validation Acc:  0.8186\n",
      "Iteration  339 : Train Loss =  0.51309836   Train Acc:  0.83225   Validation Loss =  0.5412459   Validation Acc:  0.8185\n",
      "Iteration  340 : Train Loss =  0.5127736   Train Acc:  0.8322833   Validation Loss =  0.5409595   Validation Acc:  0.8186\n",
      "Iteration  341 : Train Loss =  0.5124502   Train Acc:  0.83243334   Validation Loss =  0.5406744   Validation Acc:  0.8188\n",
      "Iteration  342 : Train Loss =  0.5121283   Train Acc:  0.83245   Validation Loss =  0.54039055   Validation Acc:  0.8192\n",
      "Iteration  343 : Train Loss =  0.51180774   Train Acc:  0.8326167   Validation Loss =  0.5401082   Validation Acc:  0.8192\n",
      "Iteration  344 : Train Loss =  0.5114886   Train Acc:  0.8326   Validation Loss =  0.5398272   Validation Acc:  0.8192\n",
      "Iteration  345 : Train Loss =  0.5111711   Train Acc:  0.83273333   Validation Loss =  0.5395475   Validation Acc:  0.8192\n",
      "Iteration  346 : Train Loss =  0.51085484   Train Acc:  0.83278334   Validation Loss =  0.5392692   Validation Acc:  0.819\n",
      "Iteration  347 : Train Loss =  0.51054007   Train Acc:  0.83276665   Validation Loss =  0.5389923   Validation Acc:  0.8191\n",
      "Iteration  348 : Train Loss =  0.51022655   Train Acc:  0.8329167   Validation Loss =  0.5387166   Validation Acc:  0.8191\n",
      "Iteration  349 : Train Loss =  0.5099145   Train Acc:  0.83305   Validation Loss =  0.53844225   Validation Acc:  0.8192\n",
      "Iteration  350 : Train Loss =  0.50960386   Train Acc:  0.83323336   Validation Loss =  0.53816915   Validation Acc:  0.8193\n",
      "Iteration  351 : Train Loss =  0.5092945   Train Acc:  0.8333333   Validation Loss =  0.5378973   Validation Acc:  0.8194\n",
      "Iteration  352 : Train Loss =  0.5089866   Train Acc:  0.8333   Validation Loss =  0.53762674   Validation Acc:  0.8193\n",
      "Iteration  353 : Train Loss =  0.5086799   Train Acc:  0.8333167   Validation Loss =  0.5373575   Validation Acc:  0.8193\n",
      "Iteration  354 : Train Loss =  0.5083747   Train Acc:  0.8335   Validation Loss =  0.5370896   Validation Acc:  0.8191\n",
      "Iteration  355 : Train Loss =  0.50807077   Train Acc:  0.83351666   Validation Loss =  0.53682286   Validation Acc:  0.8192\n",
      "Iteration  356 : Train Loss =  0.5077681   Train Acc:  0.8336667   Validation Loss =  0.5365574   Validation Acc:  0.8193\n",
      "Iteration  357 : Train Loss =  0.50746673   Train Acc:  0.83381665   Validation Loss =  0.536293   Validation Acc:  0.8193\n",
      "Iteration  358 : Train Loss =  0.5071668   Train Acc:  0.83388335   Validation Loss =  0.53603005   Validation Acc:  0.8193\n",
      "Iteration  359 : Train Loss =  0.50686795   Train Acc:  0.8340333   Validation Loss =  0.53576815   Validation Acc:  0.8194\n",
      "Iteration  360 : Train Loss =  0.50657046   Train Acc:  0.8340667   Validation Loss =  0.53550756   Validation Acc:  0.8194\n",
      "Iteration  361 : Train Loss =  0.5062743   Train Acc:  0.83426666   Validation Loss =  0.53524816   Validation Acc:  0.8194\n",
      "Iteration  362 : Train Loss =  0.50597936   Train Acc:  0.83433336   Validation Loss =  0.5349898   Validation Acc:  0.8196\n",
      "Iteration  363 : Train Loss =  0.5056856   Train Acc:  0.8344667   Validation Loss =  0.5347328   Validation Acc:  0.8197\n",
      "Iteration  364 : Train Loss =  0.50539315   Train Acc:  0.8344833   Validation Loss =  0.5344769   Validation Acc:  0.8199\n",
      "Iteration  365 : Train Loss =  0.505102   Train Acc:  0.8344833   Validation Loss =  0.5342221   Validation Acc:  0.8201\n",
      "Iteration  366 : Train Loss =  0.5048119   Train Acc:  0.83451664   Validation Loss =  0.5339686   Validation Acc:  0.8206\n",
      "Iteration  367 : Train Loss =  0.50452316   Train Acc:  0.83455   Validation Loss =  0.5337162   Validation Acc:  0.8207\n",
      "Iteration  368 : Train Loss =  0.5042356   Train Acc:  0.83466667   Validation Loss =  0.53346497   Validation Acc:  0.8207\n",
      "Iteration  369 : Train Loss =  0.5039492   Train Acc:  0.8347167   Validation Loss =  0.53321475   Validation Acc:  0.8207\n",
      "Iteration  370 : Train Loss =  0.503664   Train Acc:  0.8347833   Validation Loss =  0.5329657   Validation Acc:  0.8207\n",
      "Iteration  371 : Train Loss =  0.5033801   Train Acc:  0.8348   Validation Loss =  0.5327179   Validation Acc:  0.8208\n",
      "Iteration  372 : Train Loss =  0.50309724   Train Acc:  0.83486664   Validation Loss =  0.5324709   Validation Acc:  0.821\n",
      "Iteration  373 : Train Loss =  0.50281554   Train Acc:  0.83496666   Validation Loss =  0.5322252   Validation Acc:  0.8209\n",
      "Iteration  374 : Train Loss =  0.5025351   Train Acc:  0.83505   Validation Loss =  0.5319805   Validation Acc:  0.821\n",
      "Iteration  375 : Train Loss =  0.5022558   Train Acc:  0.83515   Validation Loss =  0.531737   Validation Acc:  0.8212\n",
      "Iteration  376 : Train Loss =  0.50197756   Train Acc:  0.8351833   Validation Loss =  0.5314946   Validation Acc:  0.821\n",
      "Iteration  377 : Train Loss =  0.5017005   Train Acc:  0.83523333   Validation Loss =  0.53125316   Validation Acc:  0.8209\n",
      "Iteration  378 : Train Loss =  0.50142455   Train Acc:  0.83538336   Validation Loss =  0.5310128   Validation Acc:  0.821\n",
      "Iteration  379 : Train Loss =  0.5011498   Train Acc:  0.8354167   Validation Loss =  0.53077364   Validation Acc:  0.8212\n",
      "Iteration  380 : Train Loss =  0.5008761   Train Acc:  0.83563334   Validation Loss =  0.53053534   Validation Acc:  0.8212\n",
      "Iteration  381 : Train Loss =  0.5006035   Train Acc:  0.83565   Validation Loss =  0.5302981   Validation Acc:  0.8212\n",
      "Iteration  382 : Train Loss =  0.500332   Train Acc:  0.83585   Validation Loss =  0.53006196   Validation Acc:  0.8211\n",
      "Iteration  383 : Train Loss =  0.5000616   Train Acc:  0.8359   Validation Loss =  0.52982676   Validation Acc:  0.8212\n",
      "Iteration  384 : Train Loss =  0.4997923   Train Acc:  0.836   Validation Loss =  0.5295927   Validation Acc:  0.8212\n",
      "Iteration  385 : Train Loss =  0.49952406   Train Acc:  0.83608335   Validation Loss =  0.52935964   Validation Acc:  0.8211\n",
      "Iteration  386 : Train Loss =  0.4992569   Train Acc:  0.8362   Validation Loss =  0.5291274   Validation Acc:  0.8211\n",
      "Iteration  387 : Train Loss =  0.49899083   Train Acc:  0.83631665   Validation Loss =  0.5288964   Validation Acc:  0.8212\n",
      "Iteration  388 : Train Loss =  0.4987257   Train Acc:  0.83635   Validation Loss =  0.5286663   Validation Acc:  0.821\n",
      "Iteration  389 : Train Loss =  0.49846175   Train Acc:  0.83643335   Validation Loss =  0.52843714   Validation Acc:  0.8212\n",
      "Iteration  390 : Train Loss =  0.49819884   Train Acc:  0.8366167   Validation Loss =  0.5282089   Validation Acc:  0.8213\n",
      "Iteration  391 : Train Loss =  0.4979369   Train Acc:  0.8367   Validation Loss =  0.52798176   Validation Acc:  0.8213\n",
      "Iteration  392 : Train Loss =  0.497676   Train Acc:  0.83676666   Validation Loss =  0.52775544   Validation Acc:  0.8215\n",
      "Iteration  393 : Train Loss =  0.49741614   Train Acc:  0.83683336   Validation Loss =  0.52753025   Validation Acc:  0.8215\n",
      "Iteration  394 : Train Loss =  0.4971573   Train Acc:  0.8369333   Validation Loss =  0.5273061   Validation Acc:  0.8215\n",
      "Iteration  395 : Train Loss =  0.49689946   Train Acc:  0.83701664   Validation Loss =  0.5270825   Validation Acc:  0.8218\n",
      "Iteration  396 : Train Loss =  0.49664265   Train Acc:  0.8371   Validation Loss =  0.52686024   Validation Acc:  0.8221\n",
      "Iteration  397 : Train Loss =  0.4963868   Train Acc:  0.83713335   Validation Loss =  0.5266387   Validation Acc:  0.8223\n",
      "Iteration  398 : Train Loss =  0.49613187   Train Acc:  0.8372167   Validation Loss =  0.5264181   Validation Acc:  0.8219\n",
      "Iteration  399 : Train Loss =  0.49587807   Train Acc:  0.83725   Validation Loss =  0.52619845   Validation Acc:  0.8218\n",
      "Iteration  400 : Train Loss =  0.49562514   Train Acc:  0.83725   Validation Loss =  0.52597976   Validation Acc:  0.8218\n",
      "Iteration  401 : Train Loss =  0.49537325   Train Acc:  0.8373167   Validation Loss =  0.525762   Validation Acc:  0.8219\n",
      "Iteration  402 : Train Loss =  0.49512225   Train Acc:  0.8374   Validation Loss =  0.52554494   Validation Acc:  0.8221\n",
      "Iteration  403 : Train Loss =  0.49487233   Train Acc:  0.83745   Validation Loss =  0.52532905   Validation Acc:  0.8222\n",
      "Iteration  404 : Train Loss =  0.49462324   Train Acc:  0.83758336   Validation Loss =  0.5251138   Validation Acc:  0.8221\n",
      "Iteration  405 : Train Loss =  0.4943752   Train Acc:  0.8376833   Validation Loss =  0.52489966   Validation Acc:  0.8221\n",
      "Iteration  406 : Train Loss =  0.494128   Train Acc:  0.8378   Validation Loss =  0.5246862   Validation Acc:  0.8221\n",
      "Iteration  407 : Train Loss =  0.4938818   Train Acc:  0.83783334   Validation Loss =  0.5244738   Validation Acc:  0.8221\n",
      "Iteration  408 : Train Loss =  0.49363652   Train Acc:  0.83783334   Validation Loss =  0.52426213   Validation Acc:  0.8221\n",
      "Iteration  409 : Train Loss =  0.4933921   Train Acc:  0.8379167   Validation Loss =  0.5240515   Validation Acc:  0.8223\n",
      "Iteration  410 : Train Loss =  0.49314862   Train Acc:  0.8380833   Validation Loss =  0.5238415   Validation Acc:  0.8223\n",
      "Iteration  411 : Train Loss =  0.49290612   Train Acc:  0.83818334   Validation Loss =  0.5236324   Validation Acc:  0.8225\n",
      "Iteration  412 : Train Loss =  0.49266458   Train Acc:  0.83818334   Validation Loss =  0.5234243   Validation Acc:  0.8224\n",
      "Iteration  413 : Train Loss =  0.49242377   Train Acc:  0.83835   Validation Loss =  0.52321696   Validation Acc:  0.8224\n",
      "Iteration  414 : Train Loss =  0.49218395   Train Acc:  0.8384   Validation Loss =  0.5230104   Validation Acc:  0.8227\n",
      "Iteration  415 : Train Loss =  0.4919449   Train Acc:  0.83848333   Validation Loss =  0.5228048   Validation Acc:  0.8227\n",
      "Iteration  416 : Train Loss =  0.49170694   Train Acc:  0.83851665   Validation Loss =  0.5225999   Validation Acc:  0.8227\n",
      "Iteration  417 : Train Loss =  0.49146974   Train Acc:  0.8386   Validation Loss =  0.5223958   Validation Acc:  0.8228\n",
      "Iteration  418 : Train Loss =  0.49123347   Train Acc:  0.8387   Validation Loss =  0.5221926   Validation Acc:  0.8229\n",
      "Iteration  419 : Train Loss =  0.49099806   Train Acc:  0.83875   Validation Loss =  0.5219901   Validation Acc:  0.823\n",
      "Iteration  420 : Train Loss =  0.4907635   Train Acc:  0.8388   Validation Loss =  0.52178866   Validation Acc:  0.823\n",
      "Iteration  421 : Train Loss =  0.49052975   Train Acc:  0.8389   Validation Loss =  0.5215878   Validation Acc:  0.8231\n",
      "Iteration  422 : Train Loss =  0.49029678   Train Acc:  0.83891666   Validation Loss =  0.5213878   Validation Acc:  0.8232\n",
      "Iteration  423 : Train Loss =  0.49006483   Train Acc:  0.8390167   Validation Loss =  0.5211886   Validation Acc:  0.8231\n",
      "Iteration  424 : Train Loss =  0.4898336   Train Acc:  0.8390333   Validation Loss =  0.5209901   Validation Acc:  0.8231\n",
      "Iteration  425 : Train Loss =  0.48960328   Train Acc:  0.8391   Validation Loss =  0.5207925   Validation Acc:  0.8231\n",
      "Iteration  426 : Train Loss =  0.48937374   Train Acc:  0.8391333   Validation Loss =  0.5205956   Validation Acc:  0.8233\n",
      "Iteration  427 : Train Loss =  0.48914504   Train Acc:  0.83925   Validation Loss =  0.52039945   Validation Acc:  0.8233\n",
      "Iteration  428 : Train Loss =  0.48891723   Train Acc:  0.83935   Validation Loss =  0.5202042   Validation Acc:  0.8233\n",
      "Iteration  429 : Train Loss =  0.48869014   Train Acc:  0.8393833   Validation Loss =  0.5200096   Validation Acc:  0.8234\n",
      "Iteration  430 : Train Loss =  0.48846394   Train Acc:  0.83956665   Validation Loss =  0.5198158   Validation Acc:  0.8235\n",
      "Iteration  431 : Train Loss =  0.4882385   Train Acc:  0.83965   Validation Loss =  0.5196228   Validation Acc:  0.8235\n",
      "Iteration  432 : Train Loss =  0.48801386   Train Acc:  0.8398   Validation Loss =  0.5194304   Validation Acc:  0.8238\n",
      "Iteration  433 : Train Loss =  0.48779   Train Acc:  0.83991665   Validation Loss =  0.5192389   Validation Acc:  0.8239\n",
      "Iteration  434 : Train Loss =  0.48756698   Train Acc:  0.83995   Validation Loss =  0.5190479   Validation Acc:  0.8238\n",
      "Iteration  435 : Train Loss =  0.48734477   Train Acc:  0.83995   Validation Loss =  0.5188579   Validation Acc:  0.8239\n",
      "Iteration  436 : Train Loss =  0.48712334   Train Acc:  0.83996665   Validation Loss =  0.51866853   Validation Acc:  0.824\n",
      "Iteration  437 : Train Loss =  0.48690262   Train Acc:  0.84   Validation Loss =  0.51847994   Validation Acc:  0.8242\n",
      "Iteration  438 : Train Loss =  0.48668268   Train Acc:  0.8402   Validation Loss =  0.518292   Validation Acc:  0.8244\n",
      "Iteration  439 : Train Loss =  0.48646355   Train Acc:  0.84031665   Validation Loss =  0.51810473   Validation Acc:  0.8245\n",
      "Iteration  440 : Train Loss =  0.4862452   Train Acc:  0.84036666   Validation Loss =  0.5179183   Validation Acc:  0.8245\n",
      "Iteration  441 : Train Loss =  0.4860276   Train Acc:  0.8404   Validation Loss =  0.5177325   Validation Acc:  0.8245\n",
      "Iteration  442 : Train Loss =  0.48581076   Train Acc:  0.84043336   Validation Loss =  0.51754737   Validation Acc:  0.8244\n",
      "Iteration  443 : Train Loss =  0.48559466   Train Acc:  0.84043336   Validation Loss =  0.517363   Validation Acc:  0.8244\n",
      "Iteration  444 : Train Loss =  0.4853793   Train Acc:  0.8404667   Validation Loss =  0.5171793   Validation Acc:  0.8245\n",
      "Iteration  445 : Train Loss =  0.48516467   Train Acc:  0.8405833   Validation Loss =  0.5169963   Validation Acc:  0.8245\n",
      "Iteration  446 : Train Loss =  0.4849508   Train Acc:  0.8407   Validation Loss =  0.51681423   Validation Acc:  0.8246\n",
      "Iteration  447 : Train Loss =  0.4847377   Train Acc:  0.84076667   Validation Loss =  0.51663244   Validation Acc:  0.8245\n",
      "Iteration  448 : Train Loss =  0.48452523   Train Acc:  0.8408333   Validation Loss =  0.5164514   Validation Acc:  0.8245\n",
      "Iteration  449 : Train Loss =  0.48431364   Train Acc:  0.8408833   Validation Loss =  0.5162712   Validation Acc:  0.8246\n",
      "Iteration  450 : Train Loss =  0.4841026   Train Acc:  0.84098333   Validation Loss =  0.51609164   Validation Acc:  0.8249\n",
      "Iteration  451 : Train Loss =  0.48389238   Train Acc:  0.84108335   Validation Loss =  0.5159127   Validation Acc:  0.8249\n",
      "Iteration  452 : Train Loss =  0.48368287   Train Acc:  0.84106666   Validation Loss =  0.51573443   Validation Acc:  0.8251\n",
      "Iteration  453 : Train Loss =  0.48347408   Train Acc:  0.84105   Validation Loss =  0.5155568   Validation Acc:  0.8251\n",
      "Iteration  454 : Train Loss =  0.48326594   Train Acc:  0.84113336   Validation Loss =  0.5153799   Validation Acc:  0.8252\n",
      "Iteration  455 : Train Loss =  0.48305854   Train Acc:  0.8411833   Validation Loss =  0.51520365   Validation Acc:  0.8254\n",
      "Iteration  456 : Train Loss =  0.48285183   Train Acc:  0.84125   Validation Loss =  0.515028   Validation Acc:  0.8254\n",
      "Iteration  457 : Train Loss =  0.48264587   Train Acc:  0.8412667   Validation Loss =  0.51485306   Validation Acc:  0.8255\n",
      "Iteration  458 : Train Loss =  0.48244053   Train Acc:  0.84141666   Validation Loss =  0.5146786   Validation Acc:  0.8255\n",
      "Iteration  459 : Train Loss =  0.48223588   Train Acc:  0.8415   Validation Loss =  0.5145051   Validation Acc:  0.8254\n",
      "Iteration  460 : Train Loss =  0.48203197   Train Acc:  0.84148335   Validation Loss =  0.5143318   Validation Acc:  0.8254\n",
      "Iteration  461 : Train Loss =  0.48182866   Train Acc:  0.8415667   Validation Loss =  0.51415926   Validation Acc:  0.8256\n",
      "Iteration  462 : Train Loss =  0.48162603   Train Acc:  0.84165   Validation Loss =  0.5139876   Validation Acc:  0.8258\n",
      "Iteration  463 : Train Loss =  0.48142415   Train Acc:  0.84176666   Validation Loss =  0.5138163   Validation Acc:  0.8258\n",
      "Iteration  464 : Train Loss =  0.48122284   Train Acc:  0.84188336   Validation Loss =  0.5136458   Validation Acc:  0.8258\n",
      "Iteration  465 : Train Loss =  0.48102227   Train Acc:  0.8419167   Validation Loss =  0.5134759   Validation Acc:  0.8259\n",
      "Iteration  466 : Train Loss =  0.48082232   Train Acc:  0.84201664   Validation Loss =  0.51330644   Validation Acc:  0.826\n",
      "Iteration  467 : Train Loss =  0.48062298   Train Acc:  0.84206665   Validation Loss =  0.5131378   Validation Acc:  0.826\n",
      "Iteration  468 : Train Loss =  0.4804245   Train Acc:  0.84211665   Validation Loss =  0.51296973   Validation Acc:  0.8262\n",
      "Iteration  469 : Train Loss =  0.48022643   Train Acc:  0.84218335   Validation Loss =  0.51280206   Validation Acc:  0.8262\n",
      "Iteration  470 : Train Loss =  0.4800291   Train Acc:  0.8422167   Validation Loss =  0.5126352   Validation Acc:  0.8262\n",
      "Iteration  471 : Train Loss =  0.47983244   Train Acc:  0.84225   Validation Loss =  0.51246876   Validation Acc:  0.8263\n",
      "Iteration  472 : Train Loss =  0.47963646   Train Acc:  0.8423   Validation Loss =  0.51230305   Validation Acc:  0.8264\n",
      "Iteration  473 : Train Loss =  0.47944096   Train Acc:  0.8423   Validation Loss =  0.512138   Validation Acc:  0.8263\n",
      "Iteration  474 : Train Loss =  0.47924617   Train Acc:  0.84235   Validation Loss =  0.5119733   Validation Acc:  0.8263\n",
      "Iteration  475 : Train Loss =  0.47905204   Train Acc:  0.84241664   Validation Loss =  0.51180935   Validation Acc:  0.8263\n",
      "Iteration  476 : Train Loss =  0.47885853   Train Acc:  0.84245   Validation Loss =  0.5116461   Validation Acc:  0.8263\n",
      "Iteration  477 : Train Loss =  0.47866562   Train Acc:  0.84251666   Validation Loss =  0.51148313   Validation Acc:  0.8264\n",
      "Iteration  478 : Train Loss =  0.4784733   Train Acc:  0.84253335   Validation Loss =  0.5113209   Validation Acc:  0.8263\n",
      "Iteration  479 : Train Loss =  0.47828165   Train Acc:  0.84256667   Validation Loss =  0.51115924   Validation Acc:  0.8266\n",
      "Iteration  480 : Train Loss =  0.4780906   Train Acc:  0.84265   Validation Loss =  0.51099813   Validation Acc:  0.8268\n",
      "Iteration  481 : Train Loss =  0.47790012   Train Acc:  0.8426667   Validation Loss =  0.5108375   Validation Acc:  0.8269\n",
      "Iteration  482 : Train Loss =  0.4777103   Train Acc:  0.84275   Validation Loss =  0.51067764   Validation Acc:  0.8271\n",
      "Iteration  483 : Train Loss =  0.47752103   Train Acc:  0.84285   Validation Loss =  0.51051813   Validation Acc:  0.8271\n",
      "Iteration  484 : Train Loss =  0.47733235   Train Acc:  0.8429833   Validation Loss =  0.5103593   Validation Acc:  0.8271\n",
      "Iteration  485 : Train Loss =  0.47714433   Train Acc:  0.8430333   Validation Loss =  0.510201   Validation Acc:  0.827\n",
      "Iteration  486 : Train Loss =  0.4769569   Train Acc:  0.84313333   Validation Loss =  0.51004314   Validation Acc:  0.827\n",
      "Iteration  487 : Train Loss =  0.47677004   Train Acc:  0.84315   Validation Loss =  0.50988585   Validation Acc:  0.8271\n",
      "Iteration  488 : Train Loss =  0.47658372   Train Acc:  0.84321666   Validation Loss =  0.5097292   Validation Acc:  0.8272\n",
      "Iteration  489 : Train Loss =  0.47639805   Train Acc:  0.84326667   Validation Loss =  0.5095729   Validation Acc:  0.827\n",
      "Iteration  490 : Train Loss =  0.4762129   Train Acc:  0.8433333   Validation Loss =  0.5094175   Validation Acc:  0.8269\n",
      "Iteration  491 : Train Loss =  0.47602838   Train Acc:  0.8434   Validation Loss =  0.5092623   Validation Acc:  0.8269\n",
      "Iteration  492 : Train Loss =  0.4758444   Train Acc:  0.84356666   Validation Loss =  0.5091079   Validation Acc:  0.8268\n",
      "Iteration  493 : Train Loss =  0.475661   Train Acc:  0.8436   Validation Loss =  0.5089537   Validation Acc:  0.8267\n",
      "Iteration  494 : Train Loss =  0.4754781   Train Acc:  0.84358335   Validation Loss =  0.5088004   Validation Acc:  0.8267\n",
      "Iteration  495 : Train Loss =  0.4752959   Train Acc:  0.84365   Validation Loss =  0.5086474   Validation Acc:  0.8268\n",
      "Iteration  496 : Train Loss =  0.47511423   Train Acc:  0.8437   Validation Loss =  0.50849485   Validation Acc:  0.8268\n",
      "Iteration  497 : Train Loss =  0.47493306   Train Acc:  0.8437333   Validation Loss =  0.5083429   Validation Acc:  0.8269\n",
      "Iteration  498 : Train Loss =  0.4747525   Train Acc:  0.8438333   Validation Loss =  0.5081915   Validation Acc:  0.827\n",
      "Iteration  499 : Train Loss =  0.47457242   Train Acc:  0.84391665   Validation Loss =  0.50804055   Validation Acc:  0.8273\n",
      "Iteration  500 : Train Loss =  0.47439295   Train Acc:  0.84391665   Validation Loss =  0.50789016   Validation Acc:  0.8273\n",
      "Iteration  501 : Train Loss =  0.47421393   Train Acc:  0.84398335   Validation Loss =  0.50774026   Validation Acc:  0.8272\n",
      "Iteration  502 : Train Loss =  0.47403562   Train Acc:  0.84403336   Validation Loss =  0.5075909   Validation Acc:  0.8272\n",
      "Iteration  503 : Train Loss =  0.47385782   Train Acc:  0.8440833   Validation Loss =  0.507442   Validation Acc:  0.8272\n",
      "Iteration  504 : Train Loss =  0.47368044   Train Acc:  0.8441167   Validation Loss =  0.5072936   Validation Acc:  0.8272\n",
      "Iteration  505 : Train Loss =  0.47350365   Train Acc:  0.8441333   Validation Loss =  0.5071458   Validation Acc:  0.8272\n",
      "Iteration  506 : Train Loss =  0.4733274   Train Acc:  0.84425   Validation Loss =  0.5069983   Validation Acc:  0.8274\n",
      "Iteration  507 : Train Loss =  0.47315168   Train Acc:  0.84438336   Validation Loss =  0.5068514   Validation Acc:  0.8276\n",
      "Iteration  508 : Train Loss =  0.4729765   Train Acc:  0.84435   Validation Loss =  0.506705   Validation Acc:  0.8275\n",
      "Iteration  509 : Train Loss =  0.47280192   Train Acc:  0.8444   Validation Loss =  0.50655895   Validation Acc:  0.8276\n",
      "Iteration  510 : Train Loss =  0.47262776   Train Acc:  0.8444167   Validation Loss =  0.5064135   Validation Acc:  0.8277\n",
      "Iteration  511 : Train Loss =  0.4724541   Train Acc:  0.8444833   Validation Loss =  0.5062685   Validation Acc:  0.8277\n",
      "Iteration  512 : Train Loss =  0.47228098   Train Acc:  0.84461665   Validation Loss =  0.5061241   Validation Acc:  0.8278\n",
      "Iteration  513 : Train Loss =  0.47210845   Train Acc:  0.84465   Validation Loss =  0.5059801   Validation Acc:  0.8278\n",
      "Iteration  514 : Train Loss =  0.4719364   Train Acc:  0.84468335   Validation Loss =  0.5058364   Validation Acc:  0.8278\n",
      "Iteration  515 : Train Loss =  0.47176477   Train Acc:  0.84475   Validation Loss =  0.5056934   Validation Acc:  0.8279\n",
      "Iteration  516 : Train Loss =  0.47159374   Train Acc:  0.8448   Validation Loss =  0.5055508   Validation Acc:  0.8279\n",
      "Iteration  517 : Train Loss =  0.4714232   Train Acc:  0.84491664   Validation Loss =  0.5054087   Validation Acc:  0.828\n",
      "Iteration  518 : Train Loss =  0.47125322   Train Acc:  0.84493333   Validation Loss =  0.50526696   Validation Acc:  0.828\n",
      "Iteration  519 : Train Loss =  0.47108367   Train Acc:  0.84498334   Validation Loss =  0.50512576   Validation Acc:  0.828\n",
      "Iteration  520 : Train Loss =  0.4709145   Train Acc:  0.84496665   Validation Loss =  0.504985   Validation Acc:  0.828\n",
      "Iteration  521 : Train Loss =  0.47074604   Train Acc:  0.84498334   Validation Loss =  0.5048446   Validation Acc:  0.828\n",
      "Iteration  522 : Train Loss =  0.47057793   Train Acc:  0.84505   Validation Loss =  0.5047047   Validation Acc:  0.8282\n",
      "Iteration  523 : Train Loss =  0.47041035   Train Acc:  0.84515   Validation Loss =  0.50456536   Validation Acc:  0.8282\n",
      "Iteration  524 : Train Loss =  0.4702433   Train Acc:  0.8451833   Validation Loss =  0.5044264   Validation Acc:  0.8282\n",
      "Iteration  525 : Train Loss =  0.47007665   Train Acc:  0.8452167   Validation Loss =  0.5042879   Validation Acc:  0.8283\n",
      "Iteration  526 : Train Loss =  0.46991053   Train Acc:  0.8453   Validation Loss =  0.5041499   Validation Acc:  0.8283\n",
      "Iteration  527 : Train Loss =  0.46974492   Train Acc:  0.84536666   Validation Loss =  0.5040122   Validation Acc:  0.8283\n",
      "Iteration  528 : Train Loss =  0.46957973   Train Acc:  0.84538335   Validation Loss =  0.5038751   Validation Acc:  0.8282\n",
      "Iteration  529 : Train Loss =  0.4694151   Train Acc:  0.84545   Validation Loss =  0.50373834   Validation Acc:  0.828\n",
      "Iteration  530 : Train Loss =  0.46925092   Train Acc:  0.8455333   Validation Loss =  0.503602   Validation Acc:  0.8281\n",
      "Iteration  531 : Train Loss =  0.46908715   Train Acc:  0.8455833   Validation Loss =  0.5034661   Validation Acc:  0.8282\n",
      "Iteration  532 : Train Loss =  0.4689239   Train Acc:  0.8457   Validation Loss =  0.5033307   Validation Acc:  0.8284\n",
      "Iteration  533 : Train Loss =  0.46876103   Train Acc:  0.84568334   Validation Loss =  0.5031958   Validation Acc:  0.8283\n",
      "Iteration  534 : Train Loss =  0.46859872   Train Acc:  0.84568334   Validation Loss =  0.5030611   Validation Acc:  0.8283\n",
      "Iteration  535 : Train Loss =  0.46843684   Train Acc:  0.84568334   Validation Loss =  0.50292695   Validation Acc:  0.8282\n",
      "Iteration  536 : Train Loss =  0.46827546   Train Acc:  0.84575   Validation Loss =  0.50279325   Validation Acc:  0.8282\n",
      "Iteration  537 : Train Loss =  0.4681144   Train Acc:  0.84578335   Validation Loss =  0.5026599   Validation Acc:  0.8283\n",
      "Iteration  538 : Train Loss =  0.467954   Train Acc:  0.84583336   Validation Loss =  0.502527   Validation Acc:  0.8284\n",
      "Iteration  539 : Train Loss =  0.46779382   Train Acc:  0.8458667   Validation Loss =  0.50239456   Validation Acc:  0.8284\n",
      "Iteration  540 : Train Loss =  0.46763423   Train Acc:  0.8459   Validation Loss =  0.5022624   Validation Acc:  0.8285\n",
      "Iteration  541 : Train Loss =  0.46747512   Train Acc:  0.8459   Validation Loss =  0.50213075   Validation Acc:  0.8286\n",
      "Iteration  542 : Train Loss =  0.46731642   Train Acc:  0.8459167   Validation Loss =  0.5019996   Validation Acc:  0.8286\n",
      "Iteration  543 : Train Loss =  0.46715814   Train Acc:  0.84595   Validation Loss =  0.5018687   Validation Acc:  0.8289\n",
      "Iteration  544 : Train Loss =  0.46700034   Train Acc:  0.84601665   Validation Loss =  0.5017383   Validation Acc:  0.829\n",
      "Iteration  545 : Train Loss =  0.4668429   Train Acc:  0.84605   Validation Loss =  0.50160825   Validation Acc:  0.829\n",
      "Iteration  546 : Train Loss =  0.466686   Train Acc:  0.8461   Validation Loss =  0.50147873   Validation Acc:  0.829\n",
      "Iteration  547 : Train Loss =  0.46652943   Train Acc:  0.84613335   Validation Loss =  0.5013495   Validation Acc:  0.8288\n",
      "Iteration  548 : Train Loss =  0.4663734   Train Acc:  0.8461667   Validation Loss =  0.5012206   Validation Acc:  0.8289\n",
      "Iteration  549 : Train Loss =  0.46621782   Train Acc:  0.8462667   Validation Loss =  0.5010923   Validation Acc:  0.829\n",
      "Iteration  550 : Train Loss =  0.46606255   Train Acc:  0.8462833   Validation Loss =  0.50096416   Validation Acc:  0.829\n",
      "Iteration  551 : Train Loss =  0.46590775   Train Acc:  0.84635   Validation Loss =  0.50083655   Validation Acc:  0.829\n",
      "Iteration  552 : Train Loss =  0.4657534   Train Acc:  0.84636664   Validation Loss =  0.5007093   Validation Acc:  0.8291\n",
      "Iteration  553 : Train Loss =  0.46559954   Train Acc:  0.84638333   Validation Loss =  0.50058246   Validation Acc:  0.829\n",
      "Iteration  554 : Train Loss =  0.4654459   Train Acc:  0.84643334   Validation Loss =  0.50045604   Validation Acc:  0.8291\n",
      "Iteration  555 : Train Loss =  0.46529284   Train Acc:  0.84648335   Validation Loss =  0.5003299   Validation Acc:  0.829\n",
      "Iteration  556 : Train Loss =  0.46514013   Train Acc:  0.84653336   Validation Loss =  0.5002042   Validation Acc:  0.8291\n",
      "Iteration  557 : Train Loss =  0.46498796   Train Acc:  0.8465833   Validation Loss =  0.5000789   Validation Acc:  0.8292\n",
      "Iteration  558 : Train Loss =  0.46483606   Train Acc:  0.8466167   Validation Loss =  0.49995402   Validation Acc:  0.8292\n",
      "Iteration  559 : Train Loss =  0.4646846   Train Acc:  0.8466   Validation Loss =  0.4998296   Validation Acc:  0.8292\n",
      "Iteration  560 : Train Loss =  0.4645336   Train Acc:  0.84665   Validation Loss =  0.4997053   Validation Acc:  0.8292\n",
      "Iteration  561 : Train Loss =  0.464383   Train Acc:  0.8466667   Validation Loss =  0.49958146   Validation Acc:  0.8292\n",
      "Iteration  562 : Train Loss =  0.4642328   Train Acc:  0.8467   Validation Loss =  0.49945816   Validation Acc:  0.8293\n",
      "Iteration  563 : Train Loss =  0.46408302   Train Acc:  0.84676665   Validation Loss =  0.49933505   Validation Acc:  0.8292\n",
      "Iteration  564 : Train Loss =  0.46393353   Train Acc:  0.84678334   Validation Loss =  0.4992123   Validation Acc:  0.8292\n",
      "Iteration  565 : Train Loss =  0.4637846   Train Acc:  0.84683335   Validation Loss =  0.49908996   Validation Acc:  0.8292\n",
      "Iteration  566 : Train Loss =  0.463636   Train Acc:  0.84686667   Validation Loss =  0.49896798   Validation Acc:  0.8292\n",
      "Iteration  567 : Train Loss =  0.4634877   Train Acc:  0.8469833   Validation Loss =  0.49884638   Validation Acc:  0.8292\n",
      "Iteration  568 : Train Loss =  0.46333995   Train Acc:  0.84705   Validation Loss =  0.4987253   Validation Acc:  0.8292\n",
      "Iteration  569 : Train Loss =  0.46319252   Train Acc:  0.84708333   Validation Loss =  0.4986044   Validation Acc:  0.8292\n",
      "Iteration  570 : Train Loss =  0.46304548   Train Acc:  0.84711665   Validation Loss =  0.49848384   Validation Acc:  0.8293\n",
      "Iteration  571 : Train Loss =  0.46289888   Train Acc:  0.84718335   Validation Loss =  0.49836367   Validation Acc:  0.8293\n",
      "Iteration  572 : Train Loss =  0.46275267   Train Acc:  0.8472667   Validation Loss =  0.49824396   Validation Acc:  0.8293\n",
      "Iteration  573 : Train Loss =  0.46260676   Train Acc:  0.8473167   Validation Loss =  0.4981245   Validation Acc:  0.8293\n",
      "Iteration  574 : Train Loss =  0.4624613   Train Acc:  0.8473833   Validation Loss =  0.49800536   Validation Acc:  0.8296\n",
      "Iteration  575 : Train Loss =  0.46231622   Train Acc:  0.8474   Validation Loss =  0.49788672   Validation Acc:  0.8296\n",
      "Iteration  576 : Train Loss =  0.4621715   Train Acc:  0.84748334   Validation Loss =  0.49776822   Validation Acc:  0.8296\n",
      "Iteration  577 : Train Loss =  0.46202716   Train Acc:  0.8475   Validation Loss =  0.4976502   Validation Acc:  0.8297\n",
      "Iteration  578 : Train Loss =  0.46188322   Train Acc:  0.84755   Validation Loss =  0.49753252   Validation Acc:  0.8298\n",
      "Iteration  579 : Train Loss =  0.46173963   Train Acc:  0.8477   Validation Loss =  0.49741524   Validation Acc:  0.8298\n",
      "Iteration  580 : Train Loss =  0.4615964   Train Acc:  0.8477   Validation Loss =  0.4972983   Validation Acc:  0.8298\n",
      "Iteration  581 : Train Loss =  0.46145356   Train Acc:  0.84775   Validation Loss =  0.49718153   Validation Acc:  0.8298\n",
      "Iteration  582 : Train Loss =  0.4613111   Train Acc:  0.84783334   Validation Loss =  0.49706534   Validation Acc:  0.8299\n",
      "Iteration  583 : Train Loss =  0.461169   Train Acc:  0.84795   Validation Loss =  0.49694923   Validation Acc:  0.83\n",
      "Iteration  584 : Train Loss =  0.46102726   Train Acc:  0.8480167   Validation Loss =  0.49683365   Validation Acc:  0.83\n",
      "Iteration  585 : Train Loss =  0.46088588   Train Acc:  0.848   Validation Loss =  0.49671832   Validation Acc:  0.83\n",
      "Iteration  586 : Train Loss =  0.46074492   Train Acc:  0.84798336   Validation Loss =  0.49660328   Validation Acc:  0.83\n",
      "Iteration  587 : Train Loss =  0.46060434   Train Acc:  0.84798336   Validation Loss =  0.49648872   Validation Acc:  0.83\n",
      "Iteration  588 : Train Loss =  0.46046406   Train Acc:  0.8479667   Validation Loss =  0.49637446   Validation Acc:  0.83\n",
      "Iteration  589 : Train Loss =  0.4603241   Train Acc:  0.848   Validation Loss =  0.49626034   Validation Acc:  0.8301\n",
      "Iteration  590 : Train Loss =  0.46018457   Train Acc:  0.8480333   Validation Loss =  0.49614668   Validation Acc:  0.8303\n",
      "Iteration  591 : Train Loss =  0.4600454   Train Acc:  0.8480167   Validation Loss =  0.49603334   Validation Acc:  0.8304\n",
      "Iteration  592 : Train Loss =  0.45990658   Train Acc:  0.8480833   Validation Loss =  0.49592042   Validation Acc:  0.8303\n",
      "Iteration  593 : Train Loss =  0.45976806   Train Acc:  0.84815   Validation Loss =  0.49580777   Validation Acc:  0.8303\n",
      "Iteration  594 : Train Loss =  0.45962995   Train Acc:  0.84816664   Validation Loss =  0.49569526   Validation Acc:  0.8303\n",
      "Iteration  595 : Train Loss =  0.45949212   Train Acc:  0.84821665   Validation Loss =  0.4955833   Validation Acc:  0.8303\n",
      "Iteration  596 : Train Loss =  0.4593547   Train Acc:  0.84823334   Validation Loss =  0.49547163   Validation Acc:  0.8303\n",
      "Iteration  597 : Train Loss =  0.45921764   Train Acc:  0.84831667   Validation Loss =  0.49536017   Validation Acc:  0.8303\n",
      "Iteration  598 : Train Loss =  0.45908093   Train Acc:  0.84833336   Validation Loss =  0.49524912   Validation Acc:  0.8303\n",
      "Iteration  599 : Train Loss =  0.45894447   Train Acc:  0.8484333   Validation Loss =  0.4951383   Validation Acc:  0.8305\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEGCAYAAAB1iW6ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF7AxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    num_Iterations = 600\n",
    "    adam_optimizer = tf.keras.optimizers.Adam()\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    validation_accuracy = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    # load and prepare the training and test data\n",
    "    tr_x, tr_y, te_x, te_y = load_Prepare_Data()\n",
    "    \n",
    "    \n",
    "    # To avoid problems related to type conversion, all data is converted to  float32 data types\n",
    "    tr_x = tf.cast(tr_x, tf.float32)\n",
    "    te_x = tf.cast(te_x, tf.float32)\n",
    "    tr_y = tf.cast(tr_y, tf.float32)\n",
    "    te_y = tf.cast(te_y, tf.float32)\n",
    "        \n",
    "    #Initialize the values of the weights for the softmax layer\n",
    "    w = tf.Variable(tf.random.normal([ tr_y.shape[0], tr_x.shape[0]], mean=0.0, stddev=0.05))\n",
    "    #Initialize the values of the bias for the SoftMax layer\n",
    "    b = tf.Variable(tf.zeros([tr_y.shape[0], 1]))\n",
    "    \n",
    "    # Iterate the training loop\n",
    "    for i in range(num_Iterations):\n",
    "        \n",
    "        # Create an instance of Gradient Tape to monitor the forward pass to calcualte the gradients based on the training data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = forward_pass(tr_x, w, b)\n",
    "            currentLoss = cross_entropy(tr_y, y_pred)\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        gradients = tape.gradient(currentLoss, [w, b])\n",
    "        # Determine the prediction accuracy for the training data\n",
    "        accuracy = calculate_accuracy(tr_y, y_pred)\n",
    "\n",
    "        train_accuracy.append(accuracy)\n",
    "        train_loss.append(currentLoss)\n",
    "        \n",
    "        # Calculate forward pass, loss and accuracy for the valdation data\n",
    "        te_y_pred = forward_pass(te_x, w, b) \n",
    "        te_currentLoss = cross_entropy(te_y, te_y_pred)\n",
    "        te_accuracy = calculate_accuracy(te_y, te_y_pred) \n",
    "        \n",
    "        validation_accuracy.append(te_accuracy)\n",
    "        validation_loss.append(te_currentLoss)\n",
    "\n",
    "        print (\"Iteration \", i, \": Train Loss = \",currentLoss.numpy(), \"  Train Acc: \", accuracy.numpy(), \"  Validation Loss = \", te_currentLoss.numpy(), \"  Validation Acc: \", te_accuracy.numpy())\n",
    "        # Update the trainable parameters using Adam Optimizer\n",
    "        adam_optimizer.apply_gradients(zip(gradients, [w,b]))\n",
    "           \n",
    "    # Plot the training and the validation accuracy and loss\n",
    "    plt.plot(train_accuracy, label=\"Train Acc\")\n",
    "    plt.plot(train_loss, label=\"Train Loss\")\n",
    "    plt.plot(validation_accuracy, label=\"Validation Acc\")\n",
    "    plt.plot(validation_loss, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"No. of Iterations\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show();\n",
    "\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Question1_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
