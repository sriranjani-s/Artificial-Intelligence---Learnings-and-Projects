{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigment I - Part 1\n",
    "Student ID : R00182510 <br>\n",
    "Name : Sriranjani Sridharan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Euclidean distance between the fetures of the test instance and features of the training instances\n",
    "def calculateDistance(train_data, test_instance):\n",
    "\n",
    "    distance = np.sqrt((np.sum(np.square(train_data-test_instance),axis=1)))    \n",
    "    # sort the index based on minimum to maximum distance values\n",
    "    indices = np.argsort(distance)\n",
    "    \n",
    "    return (distance,indices)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any K > 1, check if the K deciding classes of the test instance, have a majority class value or are in a tie\n",
    "# If it is a tie, choose the class of instances that have the minimum average distance to the test instance\n",
    "def check_class_tie_breaker(k_labels,k_distances):\n",
    "    \n",
    "    unique_class, counts = np.unique(k_labels, return_counts=True)\n",
    "    \n",
    "     # if there is a majority class value or only a single class value, return it\n",
    "    if len(np.unique(counts)) > 1 or len(unique_class) == 1:\n",
    "        return np.bincount(k_labels).argmax()\n",
    "    else:\n",
    "        \n",
    "        # create a dictionary with class value as key and the average distance of its instances as its value\n",
    "        k_distances_dict = dict.fromkeys(unique_class,0)\n",
    "        \n",
    "        for x,y in zip(k_labels,k_distances):\n",
    "            k_distances_dict[x] +=  y\n",
    "        \n",
    "        for x,y in k_distances_dict.items():\n",
    "            class_count = np.count_nonzero(k_labels == x)\n",
    "            k_distances_dict[x] /= class_count\n",
    "        \n",
    "        # return the class with minimum average distance\n",
    "        return min(k_distances_dict, key = k_distances_dict.get)         \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every test instance, get the unweighted distances of K nearest neighbour instances and predict its class label\n",
    "\n",
    "def predict_testlabel(k, train_data, test_data, train_label):\n",
    "    \n",
    "    predicted_test_label = []\n",
    "    \n",
    "    for instance in test_data:\n",
    "        \n",
    "        distance,indices = calculateDistance(train_data,instance)\n",
    "        \n",
    "        k_indices = indices[0:k]\n",
    "        k_labels = [int(x) for x in np.take(train_label,k_indices)]\n",
    "        k_distances = np.take(distance,k_indices)        \n",
    "        \n",
    "        # if K = 1, retun the class of the Kth neighbour \n",
    "        # else for K > 1 return the majority class or the class which has minimum average distance based on the tie breaker \n",
    "        if k == 1:\n",
    "            predicted_test_label.append(k_labels[0])\n",
    "        else:\n",
    "            predicted_test_label.append(check_class_tie_breaker(k_labels,k_distances))\n",
    "\n",
    "    return(predicted_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_main_classification(k):\n",
    "    \n",
    "    # read input files\n",
    "    train = np.genfromtxt('.\\\\data\\\\classification\\\\trainingData.csv',delimiter = ',')\n",
    "    test = np.genfromtxt('.\\\\data\\\\classification\\\\testData.csv',delimiter = ',')\n",
    "\n",
    "    k_values = k\n",
    "    \n",
    "    # Segregate the feature data from the class labels in both the training and test datasets\n",
    "    train_data = train[:,0:-1]\n",
    "    train_label = train[:,-1]\n",
    "\n",
    "    test_data = test[:,0:-1]\n",
    "    test_label = test[:,-1]\n",
    "    \n",
    "    # for every K value, determine the KNN algorithm predictions and accuracy for the test data\n",
    "    for k in k_values:\n",
    "        \n",
    "        predicted_labels = np.array(predict_testlabel(k,train_data, test_data, train_label),dtype = float)\n",
    "        \n",
    "        k_accuracy = (np.sum(predicted_labels == test_label) / len(test_label)) * 100\n",
    "        \n",
    "        print(\"K : %d , Accuracy : %3.4f%%\" %(k, k_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K : 1 , Accuracy : 89.5000%\n"
     ]
    }
   ],
   "source": [
    "# Pass the K-value(s) to the KNN algorithm\n",
    "K = [1]\n",
    "\n",
    "KNN_main_classification(K)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
